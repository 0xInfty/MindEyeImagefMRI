{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d93e001f-ae42-4afe-b347-c906c19f721e",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/SauravMaheshkar/medical/blob/main/fMRI/CLIPvoxel_to_CLIPimage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfbf508-76c9-48ec-8292-377fafe7df51",
   "metadata": {},
   "source": [
    "This notebook takes CLIP-voxels (brain voxels that have already been mapped to CLIP space via contrastive learning) and then uses a diffusion model to better align these CLIP-voxels to CLIP-image space (i.e., this is how DallE-2 uses a diffusion prior to go from CLIP-text space to CLIP-image space, as the spaces are still disjointed after contrastive learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f0f4f3",
   "metadata": {},
   "source": [
    "# Import packages & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a31fb86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimgoo/miniconda3/envs/medical-v1/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from datetime import datetime\n",
    "import h5py\n",
    "import webdataset as wds\n",
    "from info_nce import InfoNCE\n",
    "import clip\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from utils import * \n",
    "from models import * \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device:\",device)\n",
    "\n",
    "seed_everything(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4de5170-80d1-4240-8a4a-12115dae15d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT using distributed parallel processing!\n"
     ]
    }
   ],
   "source": [
    "# setup multi-gpu Data Distributed Processing (ddp) if available\n",
    "# if not using ddp, using_ddp should be False and local_rank=0\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import ddp_config\n",
    "\n",
    "using_ddp, local_rank = ddp_config.ddp_test()\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.set_device(local_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af15dc3",
   "metadata": {},
   "source": [
    "## Which pretrained model are you using for voxel alignment to embedding space? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1388385-a32b-434a-8bde-5702c65db6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "modality = \"image\"\n",
    "clip_variant = \"ViT-L/14\"\n",
    "# img_augmentation = True # do image augmentation?\n",
    "# soft_clip = True # use loss_nce + loss_soft? (seems to improve fwd/bwd retrieval)\n",
    "clamp_embs = False # clamp embeddings to (-1.5, 1.5)\n",
    "norm_embs = False # L2 norm embeddings after clamping (must be False for training the prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "728a97ec-5ba6-4237-9f60-841d151ec912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_var = images\n"
     ]
    }
   ],
   "source": [
    "assert clip_variant in (\"RN50\", \"ViT-L/14\", \"ViT-B/32\")\n",
    "if modality == \"text\":\n",
    "    image_var = \"trail\"\n",
    "elif modality == \"image\":\n",
    "    image_var = \"images\"\n",
    "else:\n",
    "    raise Exception(f\"Unknown modality: {modality}\")\n",
    "print(\"image_var =\", image_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c48c8fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT-L/14 cuda\n"
     ]
    }
   ],
   "source": [
    "assert norm_embs == False, 'norm embs must be False for training the prior'\n",
    "\n",
    "clip_extractor = Clipper(clip_variant, clamp_embs=clamp_embs, norm_embs=norm_embs)\n",
    "\n",
    "# # load COCO annotations curated in the same way as the mind_reader (Lin Sprague Singh) preprint\n",
    "# f = h5py.File('/scratch/gpfs/KNORMAN/nsdgeneral_hdf5/COCO_73k_subj_indices.hdf5', 'r')\n",
    "# subj01_order = f['subj01'][:]\n",
    "# f.close()\n",
    "# annots = np.load('/scratch/gpfs/KNORMAN/nsdgeneral_hdf5/COCO_73k_annots_curated.npy',allow_pickle=True)\n",
    "# subj01_annots = annots[subj01_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d507e6a-2b0a-4c39-bcc5-f29406d496ea",
   "metadata": {},
   "source": [
    "# Prep dataloaders and brain_to_clip model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceee8aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 128\n",
      "num_devices 1\n",
      "num_workers 1\n",
      "global_batch_size 128\n",
      "num_worker_batches 195\n",
      "validation: num_worker_batches 4\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "#batch_size = 256\n",
    "num_devices = torch.cuda.device_count()\n",
    "num_workers = num_devices\n",
    "\n",
    "train_dl, val_dl = get_dataloaders(batch_size, image_var, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70a1b5b6-ceed-467f-9302-accf8ad0abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first batches\n",
    "for train_i, (voxel0, image0) in enumerate(train_dl):\n",
    "    break\n",
    "\n",
    "for val_i, (val_voxel0, val_image0) in enumerate(val_dl):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c67c76f7-c333-4d4e-ac03-fa007624b023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.py\n",
    "brain_net = BrainNetwork(768) \n",
    "if using_ddp:\n",
    "    brain_net0 = brain_net.to(local_rank)\n",
    "    brain_net = DDP(brain_net0, device_ids=[local_rank])\n",
    "else:\n",
    "    brain_net = brain_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf48b50b-4a3a-4ac1-8192-307205fca400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load SD image variation pipeline to sample during training\n",
    "sd_pipe, unet, vae, noise_scheduler = load_sd_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab0fd77-08c7-4205-b5e5-a90707852f27",
   "metadata": {},
   "source": [
    "# Train Diffusion Prior\n",
    "\n",
    "This will map the CLIP-voxel embeddings to the CLIP-image embeddings space. Right now they are disjointed embedding spaces despite being aligned to maximize same-pair cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "195927a8-38f2-4395-ad38-bb077a86811c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ckpt_path checkpoints/clip_image_vitL_2stage_mixco_lotemp_125ep_subj01_best.pth\n",
      "nothing to plot\n"
     ]
    }
   ],
   "source": [
    "# Loading checkpoint\n",
    "#ckpt_path = f'checkpoints/{model_name}_subj01_epoch19.pth'\n",
    "#ckpt_path = f'checkpoints/clip_image_vitL_augTrue_softTrue_clampFalse_normFalse_subj01_epoch19.pth'\n",
    "ckpt_path = f'checkpoints/clip_image_vitL_2stage_mixco_lotemp_125ep_subj01_best.pth'\n",
    "#ckpt_path = 'checkpoints/clip_image_vitL_augTrue_softTrue_clampFalse_normTrue_100ep_sumLoss_subj01_epoch44.pth'\n",
    "\n",
    "print(\"ckpt_path\",ckpt_path)\n",
    "\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)    \n",
    "if 'model_state_dict' in checkpoint:\n",
    "    brain_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    plot_brainnet_ckpt(ckpt_path)\n",
    "else:\n",
    "    brain_net.load_state_dict(checkpoint)\n",
    "    print('nothing to plot')\n",
    "    \n",
    "brain_net.eval()\n",
    "brain_net.requires_grad_(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b748f7b-7c5b-4da7-b4fb-5134e32b830b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_voxel2clip():\n",
    "    \"\"\" Run the thing that Voxel_to_CLIP.ipynb runs at the end \"\"\"\n",
    "    \n",
    "    url = \"/scratch/gpfs/KNORMAN/webdataset_nsd/webdataset_split/val/val_subj01_0.tar\"\n",
    "    num_worker_batches = 10\n",
    "    eval_batch_size = 300\n",
    "    val_data = wds.DataPipeline([wds.ResampledShards(url),\n",
    "                        wds.tarfile_to_samples(),\n",
    "                        wds.decode(\"torch\"),\n",
    "                        wds.rename(images=\"jpg;png\", voxels=\"nsdgeneral.npy\", \n",
    "                                    embs=\"sgxl_emb.npy\", trial=\"trial.npy\"),\n",
    "                        wds.to_tuple(\"voxels\", \"images\", \"trial\"),\n",
    "                        wds.batched(eval_batch_size, partial=True),\n",
    "                    ]).with_epoch(num_worker_batches)\n",
    "    val_dl = wds.WebLoader(val_data, num_workers=num_workers,\n",
    "                           batch_size=None, shuffle=False, persistent_workers=True)\n",
    "\n",
    "    for val_i, (voxel, img, trial) in enumerate(val_dl):\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                voxel = voxel.to(device)\n",
    "\n",
    "                if \"image\" in ckpt_path: # using images\n",
    "                    emb0 = clip_extractor.embed_image(img)\n",
    "                else:\n",
    "                    emb0 = clip_extractor.embed_curated_annotations(subj01_annots[trial])\n",
    "\n",
    "                emb_0 = nn.functional.normalize(brain_net(voxel),dim=-1)  # brain embeddings\n",
    "\n",
    "                labels = torch.arange(len(emb0)).to(device)\n",
    "                similarities0_bwd = batchwise_cosine_similarity(emb0,emb_0)  # clip, brain\n",
    "                similarities0_fwd = batchwise_cosine_similarity(emb_0,emb0)  # brain, clip\n",
    "\n",
    "                if val_i==0:\n",
    "                    cnt=1\n",
    "                    percent_correct_fwd = topk(similarities0_fwd, labels,k=1)\n",
    "                    percent_correct_bwd = topk(similarities0_bwd, labels,k=1)\n",
    "                else:\n",
    "                    cnt+=1\n",
    "                    percent_correct_fwd += topk(similarities0_fwd, labels,k=1)\n",
    "                    percent_correct_bwd += topk(similarities0_bwd, labels,k=1)\n",
    "    percent_correct_fwd /= cnt\n",
    "    percent_correct_bwd /= cnt\n",
    "    print(\"fwd percent_correct\", percent_correct_fwd)\n",
    "    print(\"bwd percent_correct\", percent_correct_bwd)\n",
    "\n",
    "    # plot results from one batch\n",
    "    similarities0_fwd = np.array(similarities0_fwd.detach().cpu())\n",
    "    fig, ax = plt.subplots(nrows=4, ncols=6, figsize=(11,12))\n",
    "    for trial in range(4):\n",
    "        ax[trial, 0].imshow(torch_to_Image(img[trial]))\n",
    "        ax[trial, 0].set_title(\"original\\nimage\")\n",
    "        ax[trial, 0].axis(\"off\")\n",
    "        for attempt in range(5):\n",
    "            which = np.flip(np.argsort(similarities0_fwd[trial]))[attempt]\n",
    "            ax[trial, attempt+1].imshow(torch_to_Image(img[which]))\n",
    "            ax[trial, attempt+1].set_title(f\"Top {attempt+1}\")\n",
    "            ax[trial, attempt+1].axis(\"off\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5563b18a-76c6-4198-91b5-36f159179391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_voxel2clip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa3d6d21-ab58-4402-a9af-edc281216b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ckpt_path(tag):\n",
    "    return ckpt_path.replace('.pth', '') + f'_diffusionprior_{model_tag}_{tag}.pth'\n",
    "\n",
    "def save_ckpt(tag):\n",
    "    ckpt_path = get_ckpt_path(tag)\n",
    "    print(f'saving {ckpt_path}')\n",
    "    \n",
    "    if (using_ddp==False) or (using_ddp==True and local_rank==0):\n",
    "        state_dict = brain_net.state_dict()\n",
    "        if using_ddp: # if using DDP, convert DDP state_dict to non-DDP before saving\n",
    "            for key in list(state_dict.keys()):\n",
    "                if 'module.' in key:\n",
    "                    state_dict[key.replace('module.', '')] = state_dict[key]\n",
    "                    del state_dict[key]   \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': diffusion_prior.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_losses': losses,\n",
    "            'val_losses': val_losses,\n",
    "            'lrs': lrs,\n",
    "            'sims': sims,\n",
    "            'val_sims': val_sims,\n",
    "            }, ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff373d37-5e89-4c71-953c-4488e945f440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dalle2_pytorch import DiffusionPriorNetwork, DiffusionPrior\n",
    "\n",
    "# setup prior network, which contains an autoregressive transformer\n",
    "# prior_network = DiffusionPriorNetwork(\n",
    "#     dim = 768,\n",
    "#     depth = 6,\n",
    "#     dim_head = 64,\n",
    "#     heads = 8\n",
    "# ).to(device)\n",
    "\n",
    "# diffusion_prior = DiffusionPrior(\n",
    "#     net = prior_network,\n",
    "#     #clip = clip_model,\n",
    "#     image_embed_dim = 768,\n",
    "#     condition_on_text_encodings = False,\n",
    "#     timesteps = 100,\n",
    "#     cond_drop_prob = 0.2\n",
    "# ).to(device)\n",
    "\n",
    "## justinpinkney/clip2latent\n",
    "# network:\n",
    "#   dim: 512\n",
    "#   num_timesteps: 1000\n",
    "#   depth: 12\n",
    "#   dim_head: 64\n",
    "#   heads: 12\n",
    "# diffusion:\n",
    "#   image_embed_dim: 512\n",
    "#   timesteps: 1000\n",
    "#   cond_drop_prob: 0.2\n",
    "#   image_embed_scale: 1.0\n",
    "#   text_embed_scale: 1.0\n",
    "#   beta_schedule: cosine\n",
    "#   predict_x_start: true\n",
    "\n",
    "# depth = 6\n",
    "# heads = 8\n",
    "\n",
    "## too small\n",
    "# depth = 2\n",
    "# heads = 2\n",
    "\n",
    "## too big\n",
    "# depth = 12\n",
    "# heads = 12\n",
    "\n",
    "depth = 6\n",
    "heads = 12 # this way 12 heads * 64 head dim = 768\n",
    "\n",
    "timesteps = 1000\n",
    "# image_embed_scale = None\n",
    "image_embed_scale = 1.0\n",
    "\n",
    "prior_network = DiffusionPriorNetwork(\n",
    "    dim = 768,\n",
    "    depth = depth,\n",
    "    dim_head = 64,\n",
    "    heads = heads\n",
    ").to(device)\n",
    "\n",
    "# diffusion_prior = DiffusionPrior(\n",
    "#     net = prior_network,\n",
    "#     #clip = clip_model,\n",
    "#     image_embed_dim = 768,\n",
    "#     condition_on_text_encodings = False,\n",
    "#     timesteps = timesteps,\n",
    "#     cond_drop_prob = 0.2\n",
    "# ).to(device)\n",
    "\n",
    "# custom version that can fix seeds\n",
    "diffusion_prior = BrainDiffusionPrior(\n",
    "    net = prior_network,\n",
    "    #clip = clip_model,\n",
    "    image_embed_dim = 768,\n",
    "    condition_on_text_encodings = False,\n",
    "    timesteps = timesteps,\n",
    "    cond_drop_prob = 0.2,\n",
    "    image_embed_scale=1.0,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bf52a9d-26b1-4856-9488-c1377b8293ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "55,661,968 total\n",
      "55,661,952 trainable\n"
     ]
    }
   ],
   "source": [
    "count_params(diffusion_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7a85ea0-b194-42c0-9c01-ef7b4731ca1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch.nn.functional.mse_loss(input: torch.Tensor, target: torch.Tensor, size_average: Optional[bool] = None, reduce: Optional[bool] = None, reduction: str = 'mean') -> torch.Tensor>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# diffusion_prior\n",
    "diffusion_prior.noise_scheduler.loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e316bf96-01a6-4857-bb95-613e2f290828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_time_embeds 1\n",
      "num_image_embeds 1\n",
      "num_text_embeds 1\n",
      "max_text_len 256\n",
      "self_cond False\n"
     ]
    }
   ],
   "source": [
    "for x in ['num_time_embeds', 'num_image_embeds', 'num_text_embeds', 'max_text_len', 'self_cond']:\n",
    "    print(x, getattr(prior_network, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed6a83b4-9b24-4f01-aeee-2eeb873495fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_cond_drop_prob 0.2\n",
      "image_cond_drop_prob 0.2\n",
      "can_classifier_guidance True\n",
      "condition_on_text_encodings False\n",
      "predict_x_start True\n",
      "predict_v False\n",
      "image_embed_scale 1.0\n",
      "sampling_clamp_l2norm False\n",
      "sampling_final_clamp_l2norm False\n",
      "training_clamp_l2norm False\n",
      "init_image_embed_l2norm False\n"
     ]
    }
   ],
   "source": [
    "for x in ['text_cond_drop_prob', 'image_cond_drop_prob', 'can_classifier_guidance', \n",
    "          'condition_on_text_encodings', 'predict_x_start', 'predict_v', 'image_embed_scale',\n",
    "          'sampling_clamp_l2norm', 'sampling_final_clamp_l2norm', 'training_clamp_l2norm', 'init_image_embed_l2norm',\n",
    "         ]:\n",
    "    print(x, getattr(diffusion_prior, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "732a46ec-9d38-4c12-897e-20f02b4fd10d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lr=cycle_headsNsteps'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch = 0\n",
    "num_epochs = 60\n",
    "\n",
    "# lr = 3e-4\n",
    "# optimizer = torch.optim.AdamW(diffusion_prior.parameters(), lr=lr)\n",
    "# lr_scheduler = None\n",
    "\n",
    "initial_learning_rate = 1e-3 #3e-5\n",
    "optimizer = torch.optim.AdamW(diffusion_prior.parameters(), lr=initial_learning_rate)\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=3e-4, \n",
    "                                            total_steps=num_epochs*((24983//batch_size)//num_devices), \n",
    "                                            final_div_factor=1000,\n",
    "                                            last_epoch=-1, pct_start=2/num_epochs)\n",
    "\n",
    "losses, val_losses, lrs = [], [], []\n",
    "sims, val_sims = [], []\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# resume from checkpoint:\n",
    "# prior_checkpoint = torch.load(f'{ckpt_path[:-12]}_epoch{epoch}_diffusionprior.pth', \n",
    "#                               map_location=device)\n",
    "# epoch = prior_checkpoint['epoch']+1\n",
    "# diffusion_prior.load_state_dict(prior_checkpoint['model_state_dict'])\n",
    "# losses = prior_checkpoint['train_losses']\n",
    "# optimizer.load_state_dict(prior_checkpoint['optimizer_state_dict'])\n",
    "# optimizer.param_groups[0]['lr'] = lr\n",
    "\n",
    "model_tag = f'lr=cycle_headsNsteps'\n",
    "model_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f5e7103-621b-47f7-851a-8c740a6ec270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/clip_image_vitL_2stage_mixco_lotemp_125ep_subj01_best_diffusionprior_lr=cycle_headsNsteps_best.pth'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ckpt_path('best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "970d70b3-1c37-4c2d-b343-8217db66980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = OrderedDict(\n",
    "    modality=modality,\n",
    "    clip_variant=clip_variant,\n",
    "    clamp_embs=clamp_embs,\n",
    "    norm_embs=norm_embs,\n",
    "    model_tag=model_tag,\n",
    "    num_epochs=num_epochs,\n",
    "    ckpt_voxel2clip=ckpt_path,\n",
    "    ckpt_prior=get_ckpt_path('best'),\n",
    "    depth=depth,\n",
    "    heads=heads,\n",
    "    timesteps=timesteps,\n",
    "    image_embed_scale=image_embed_scale,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3a8b607-f0c8-4ba1-907a-99bbfd7fe4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_wandb = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27445433-719e-4ccc-b9c5-c3590ddee108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjimgoo\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jimgoo/git/medical/fMRI/wandb/run-20230117_210515-1pybxzfo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jimgoo/diffusion-prior/runs/1pybxzfo\" target=\"_blank\">worldly-salad-2</a></strong> to <a href=\"https://wandb.ai/jimgoo/diffusion-prior\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/jimgoo/diffusion-prior\" target=\"_blank\">https://wandb.ai/jimgoo/diffusion-prior</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/jimgoo/diffusion-prior/runs/1pybxzfo\" target=\"_blank\">https://wandb.ai/jimgoo/diffusion-prior/runs/1pybxzfo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if use_wandb:\n",
    "    import wandb\n",
    "\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"diffusion-prior\",\n",
    "        notes=\"\",\n",
    "        tags=[],\n",
    "        config=config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81565e96-0ddf-4383-aa35-f12b8453b87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fff70042-b8da-49e0-9365-1b7828a85937",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not full_training:\n",
    "    # fake DataLoaders with just the first batches\n",
    "    bs = 5\n",
    "    train_dl = [(voxel0[:bs], image0[:bs])]\n",
    "    val_dl = [(val_voxel0[:bs], val_image0[:bs])]\n",
    "#train_dl, val_dl = get_dataloaders(batch_size, image_var, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18977d5d-983c-4c9e-bca2-ad1e12c2d7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                              | 0/60 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# feed text and images into diffusion prior network\n",
    "progress_bar = tqdm(range(epoch,num_epochs),ncols=250)\n",
    "for epoch in progress_bar:\n",
    "    diffusion_prior.train()\n",
    "    for train_i, (voxel, image) in enumerate(train_dl):\n",
    "        optimizer.zero_grad()\n",
    "        image = image.to(device)\n",
    "\n",
    "        clip_embed = brain_net(voxel.to(device).float())\n",
    "        #clip_embed = nn.functional.normalize(clip_embed,dim=-1)\n",
    "\n",
    "        # clip_embed = clip_extractor.embed_curated_annotations(subj01_annots[voxel])\n",
    "\n",
    "        image_clip = clip_extractor.embed_image(image).float()\n",
    "\n",
    "        loss, pred = diffusion_prior(text_embed=clip_embed, image_embed=image_clip)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step() \n",
    "\n",
    "        losses.append(loss.item())\n",
    "        lrs.append(optimizer.param_groups[0]['lr'])\n",
    "        sims.append(F.cosine_similarity(image_clip, pred).mean().item())\n",
    "        \n",
    "    diffusion_prior.eval()\n",
    "    for val_i, (val_voxel, val_image) in enumerate(val_dl):    \n",
    "        with torch.no_grad(): \n",
    "            val_image = val_image.to(device)\n",
    "\n",
    "            clip_embed = brain_net(val_voxel.to(device).float())\n",
    "            #clip_embed = nn.functional.normalize(clip_embed,dim=-1)\n",
    "\n",
    "            # clip_embed = clip_extractor.embed_curated_annotations(subj01_annots[voxel])\n",
    "\n",
    "            image_clip = clip_extractor.embed_image(val_image).float()\n",
    "\n",
    "            val_loss, val_pred = diffusion_prior(text_embed=clip_embed, image_embed=image_clip)\n",
    "\n",
    "            val_losses.append(val_loss.item())\n",
    "            val_sims.append(F.cosine_similarity(image_clip, val_pred).mean().item())\n",
    "            \n",
    "    if full_training:\n",
    "        val_loss = np.mean(val_losses[-(val_i+1):])\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_ckpt('best')\n",
    "            if using_ddp:\n",
    "                dist.barrier() # this tells the other gpus wait for the first gpu to finish saving the model\n",
    "        else:\n",
    "            print(f'not best - val_loss: {val_loss:.3f}, best_val_loss: {best_val_loss:.3f}')\n",
    "\n",
    "        # Save model checkpoint every 5 epochs if full_training==True\n",
    "        #if (epoch+1) % 10 == 0:\n",
    "        if epoch == num_epochs - 1:\n",
    "            save_ckpt(f'epoch{epoch:03d}')\n",
    "            if using_ddp:\n",
    "                dist.barrier() # this tells the other gpus wait for the first gpu to finish saving the model\n",
    "\n",
    "    logs = OrderedDict(\n",
    "        loss=np.mean(losses[-(train_i+1):]),\n",
    "        val_loss=np.mean(val_losses[-(val_i+1):]),\n",
    "        lr=lrs[-1],\n",
    "        sim=np.mean(sims[-(train_i+1):]),\n",
    "        val_sim=np.mean(val_sims[-(val_i+1):]),\n",
    "        steps=len(losses),\n",
    "    )\n",
    "    progress_bar.set_postfix(**logs)\n",
    "    \n",
    "    if not full_training and epoch % 5 == 0:\n",
    "        clear_output(wait=True)\n",
    "        plot_prior(losses, val_losses, lrs, sims, val_sims)\n",
    "    \n",
    "    if use_wandb:\n",
    "        clear_output(wait=True)\n",
    "        plot_prior(losses, val_losses, lrs, sims, val_sims)\n",
    "        \n",
    "        print('-'*50)\n",
    "        print('train')\n",
    "\n",
    "        figs = sample_images(\n",
    "            clip_extractor, brain_net, unet, vae, noise_scheduler, diffusion_prior,\n",
    "            voxel0[:4], image0[:4], seed=42,\n",
    "        )\n",
    "        \n",
    "        for idx, f in enumerate(figs):\n",
    "            logs['train/sample/%02d' % idx] = wandb.Image(f)\n",
    "        \n",
    "        print('-'*50)\n",
    "        print('val')\n",
    "        \n",
    "        figs = sample_images(\n",
    "            clip_extractor, brain_net, unet, vae, noise_scheduler, diffusion_prior,\n",
    "            val_voxel0[:4], val_image0[:4], seed=42,\n",
    "        )\n",
    "        \n",
    "        for idx, f in enumerate(figs):\n",
    "            logs['val/sample/%02d' % idx] = wandb.Image(f)\n",
    "            \n",
    "        wandb.log(logs)\n",
    "        \n",
    "if use_wandb:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2802f784-f3a7-4110-bd10-75b4bb9966ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1674064022.696008"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9356c427-13f5-401a-89f8-fd1d759bed5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b26ef6a5-5882-4f03-8ace-8eddafcf887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "749ffe4a-f35f-4783-90b7-113bbe8cc023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(F.cosine_similarity(image_clip, val_pred))\n",
    "# for i in range(image_clip.shape[0]):\n",
    "#     print(F.cosine_similarity(image_clip[i,:], val_pred[i,:], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34e4ad2-b526-4203-bbd9-cfdf1dbeb114",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_ckpt(f'epoch{epoch:03d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294a7495-7fd7-4cbe-affb-ccda0260d0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior_ckpt_path = get_ckpt_path(f'epoch{num_epochs-1:03d}')\n",
    "# plot_prior_ckpt(prior_ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c355e8-5c29-41ec-bf49-7ab821acb728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained checkpoint\n",
    "# prior_ckpt_path = f'{ckpt_path[:-12]}_epoch24_diffusionprior.pth'\n",
    "#prior_ckpt_path = '/home/jimgoo/data/neuro/clip_image_vitL_2stage_mixco_lotemp_125ep_subj01_best|epoch004_diffusionprior-jimgoo.pth'\n",
    "#prior_ckpt_path = 'checkpoints/clip_image_vitL_augTrue_softTrue_clampFalse_normFalse_subj01_epoch19|epoch039_diffusionprior-jimgoo.pth'\n",
    "#prior_ckpt_path = '/home/jimgoo/data/neuro/clip_image_vitL_2stage_mixco_lotemp_125ep_subj01_best|epoch034_diffusionprior_lrCycle.pth'\n",
    "prior_ckpt_path = get_ckpt_path('best')\n",
    "print('prior_ckpt_path', prior_ckpt_path)\n",
    "\n",
    "plot_prior_ckpt(prior_ckpt_path)\n",
    "\n",
    "prior_checkpoint = torch.load(prior_ckpt_path, map_location=device)\n",
    "print('epoch', prior_checkpoint['epoch'])\n",
    "print('n_steps', len(prior_checkpoint['train_losses']))\n",
    "\n",
    "diffusion_prior.load_state_dict(prior_checkpoint['model_state_dict'])\n",
    "diffusion_prior = diffusion_prior.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6901479d-b00a-4758-9030-573947078f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_prior.eval()\n",
    "diffusion_prior.requires_grad_(False)\n",
    "\n",
    "prior_network.eval()\n",
    "prior_network.requires_grad_(False)\n",
    "\n",
    "brain_net.eval()\n",
    "brain_net.requires_grad_(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22653b76-241e-464f-a51d-269575de5791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (voxel, image) in enumerate(val_dl):\n",
    "# #for i, (voxel, image) in enumerate(train_dl):\n",
    "#     image_test = image.to(device)\n",
    "#     clip_embed = brain_net(voxel.to(device).float())\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a31892-1e7f-4f5f-8b8a-e82164804747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #22\n",
    "# for idx in range(4):\n",
    "#     plt.imshow(torch_to_Image(image_test[idx]))\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "#     image_clip = clip_extractor.embed_image(image_test[idx][None])\n",
    "#     orig_vox_clip = clip_embed[idx][None]\n",
    "    \n",
    "#     print(\"norm of orig clip\", image_clip.norm().item())\n",
    "#     print(\"norm before prior\", clip_embed[idx][None].norm().item())\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         align_vox_clip = diffusion_prior.p_sample_loop(clip_embed[idx][None].shape, \n",
    "#                                                        text_cond = dict(text_embed = clip_embed[idx][None].long()), \n",
    "#                                                        cond_scale = 1., timesteps = 100)\n",
    "#         print(\"norm after prior\", align_vox_clip.norm().item())\n",
    "    \n",
    "#     print(\"Cosine sim for CLIP-voxel x CLIP-image\", nn.functional.cosine_similarity(orig_vox_clip.float(),image_clip,dim=1))\n",
    "#     print(\"Cosine sim for CLIP-voxel-aligned x CLIP-image\",nn.functional.cosine_similarity(align_vox_clip.float(),image_clip,dim=1))\n",
    "\n",
    "#     plt.plot(image_clip.detach().cpu().numpy().flatten(),c='k')\n",
    "#     plt.plot(orig_vox_clip.detach().cpu().numpy().flatten(),ls='--')\n",
    "#     plt.show()\n",
    "\n",
    "#     plt.plot(image_clip.detach().cpu().numpy().flatten(),c='k')\n",
    "#     plt.plot(align_vox_clip.detach().cpu().numpy().flatten(),c='orange',ls='--')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d44f90-af14-49c5-aa1b-bb837b647c95",
   "metadata": {},
   "source": [
    "# Test out feeding these aligned CLIP-voxel embeddings through Stable Diffusion (Image Variation) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937771a0-dd5a-450c-98ee-8a81397dc4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val\n",
    "n_examples = 8\n",
    "\n",
    "sample_images(\n",
    "    clip_extractor, brain_net, unet, vae, noise_scheduler, diffusion_prior,\n",
    "    val_voxel0[:n_examples], val_image0[:n_examples], \n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605c567f-fb86-4821-88a9-38e1edca2f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "sample_images(\n",
    "    clip_extractor, brain_net, unet, vae, noise_scheduler, diffusion_prior,\n",
    "    voxel0[:n_examples], image0[:n_examples], \n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f835224-e947-4fca-8a93-20ed4f03d707",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:medical-v1]",
   "language": "python",
   "name": "conda-env-medical-v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "62aae01ef0cf7b6af841ab1c8ce59175c4332e693ab3d00bc32ceffb78a35376"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
