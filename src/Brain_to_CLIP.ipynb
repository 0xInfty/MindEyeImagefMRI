{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26ca08a0-e0e9-4cf0-b95c-2c93376f2eb7",
   "metadata": {},
   "source": [
    "This notebook takes brain voxels and maps them to CLIP-space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4d95fac-ac1d-473c-ab96-650f76e6aaf5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Brain_to_CLIP_accel.ipynb to python\n",
      "[NbConvertApp] Writing 18273 bytes to Brain_to_CLIP_accel.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # convert this notebook to .py such that you can then run it via slurm with \"sbatch *.slurm\"\n",
    "# from subprocess import call\n",
    "# command = \"jupyter nbconvert Brain_to_CLIP_accel.ipynb --to python\"\n",
    "# call(command,shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f0f4f3",
   "metadata": {},
   "source": [
    "# Import packages & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bad764b-45c1-45ce-a716-8d055e09821a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "distributed = False num_devices = 1 local rank = 0 world size = 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from info_nce import InfoNCE\n",
    "from dalle2_pytorch import DiffusionPriorNetwork\n",
    "import kornia\n",
    "from kornia.augmentation.container import AugmentationSequential\n",
    "\n",
    "import utils\n",
    "from utils import torch_to_matplotlib, torch_to_Image\n",
    "from models import Clipper, BrainNetwork, BrainDiffusionPrior\n",
    "from model3d import SimpleVoxel3dConvEncoder\n",
    "\n",
    "import torch.distributed as dist\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# uses tf32 data type which is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# Log to weights and biases?\n",
    "wandb_log = False\n",
    "\n",
    "# Resume from ckpt? #\n",
    "resume_from_ckpt = False\n",
    "if resume_from_ckpt:\n",
    "    ckpt_path = '../train_logs/vox2clip_indiv/ckpt-voxel2clip-epoch029.pth'\n",
    "else:\n",
    "    ckpt_path = 'none'\n",
    "\n",
    "# Multi-GPU config #\n",
    "accelerator = Accelerator()\n",
    "print = accelerator.print # only print if local_rank=0\n",
    "\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0: num_devices = 1\n",
    "num_workers = num_devices\n",
    "\n",
    "print(accelerator.state)\n",
    "local_rank = accelerator.state.local_process_index\n",
    "world_size = accelerator.state.num_processes\n",
    "if num_devices<=1 and world_size<=1:\n",
    "    distributed=False\n",
    "else:\n",
    "    distributed=True\n",
    "print(\"distributed =\",distributed,\"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018b82b-c054-4463-9527-4b0c2a75bda6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60cd7f2c-37fd-426b-a0c6-633e51bc4c4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"v2c_avg\"\n",
    "if not wandb_log:\n",
    "    model_name = \"testing\"\n",
    "\n",
    "modality = \"image\"\n",
    "if modality == \"text\":\n",
    "    is_text = True\n",
    "else:\n",
    "    is_text = False\n",
    "clip_variant = \"ViT-L/14\" # (\"RN50\", \"ViT-L/14\", \"ViT-B/32\")\n",
    "clamp_embs = False # clamp embeddings to (-1.5, 1.5)\n",
    "voxel_dims = 1 # 1 for flattened input, 3 for 3d input\n",
    "seed = 42\n",
    "\n",
    "mixup_pct = 0.5\n",
    "use_image_aug = True\n",
    "\n",
    "num_epochs = 120\n",
    "if voxel_dims==1:\n",
    "    batch_size = 300\n",
    "else:\n",
    "    batch_size = 128\n",
    "\n",
    "lr_scheduler = 'cycle'\n",
    "initial_lr = 5e-4 # only used if lr_scheduler is 'fixed'\n",
    "max_lr = 3e-4\n",
    "ckpt_saving = True\n",
    "ckpt_interval = 10\n",
    "save_at_end = True\n",
    "outdir = f'../train_logs/{model_name}'\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir,exist_ok=True)\n",
    "remote_data = False # if True, pull webdatasets from huggingface\n",
    "\n",
    "if use_image_aug:\n",
    "    train_augs = AugmentationSequential(\n",
    "        kornia.augmentation.RandomResizedCrop((224,224), (0.6,1), p=0.3),\n",
    "        kornia.augmentation.Resize((224, 224)),\n",
    "        kornia.augmentation.RandomHorizontalFlip(p=0.5),\n",
    "        kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1, p=0.3),\n",
    "        kornia.augmentation.RandomGrayscale(p=0.3),\n",
    "        data_keys=[\"input\"],\n",
    "    )\n",
    "else:\n",
    "    train_augs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d13c25-1369-4c49-81d4-83d713586096",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prep models and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7966d5a-c8a9-4461-808a-2f89bb00fe9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using cudnn.deterministic\n",
      "Pulling NSD webdataset data...\n",
      "Prepping train and validation dataloaders...\n",
      "Getting dataloaders...\n",
      "\n",
      "num_train 8859\n",
      "global_batch_size 128\n",
      "batch_size 128\n",
      "num_workers 1\n",
      "num_batches 69\n",
      "num_worker_batches 69\n",
      "cache_dir None\n",
      "\n",
      "num_val 982\n",
      "val_batch_size 300\n",
      "val_num_workers 1\n",
      "voxel.shape torch.Size([300, 3, 68, 64, 47])\n"
     ]
    }
   ],
   "source": [
    "# need non-deterministic CuDNN for conv3D to work\n",
    "utils.seed_everything(seed, cudnn_deterministic=False)\n",
    "\n",
    "if modality=='text':\n",
    "    print('Using CLIP-text, preparing COCO annotations...')\n",
    "    import h5py\n",
    "    # load COCO annotations curated in the same way as the mind_reader (Lin Sprague Singh) preprint\n",
    "    f = h5py.File('/scratch/gpfs/KNORMAN/nsdgeneral_hdf5/COCO_73k_subj_indices.hdf5', 'r')\n",
    "    subj01_order = f['subj01'][:]\n",
    "    f.close()\n",
    "    annots = np.load('/scratch/gpfs/KNORMAN/nsdgeneral_hdf5/COCO_73k_annots_curated.npy',allow_pickle=True)\n",
    "    subj01_annots = annots[subj01_order]\n",
    "\n",
    "print('Pulling NSD webdataset data...')\n",
    "if remote_data:\n",
    "    # pull data directly from huggingface\n",
    "    train_url, val_url = utils.get_huggingface_urls(data_commit)\n",
    "    meta_url = None\n",
    "else:\n",
    "    # local paths\n",
    "    # data_commit = '9947586218b6b7c8cab804009ddca5045249a38d'\n",
    "    # train_url = f\"/fsx/proj-medarc/fmri/natural-scenes-dataset/{data_commit}/datasets_pscotti_naturalscenesdataset_resolve_{data_commit}_webdataset_train/train_subj01_{{0..49}}.tar\"\n",
    "    # val_url = f\"/fsx/proj-medarc/fmri/natural-scenes-dataset/{data_commit}/datasets_pscotti_naturalscenesdataset_resolve_{data_commit}_webdataset_val/val_subj01_0.tar\"\n",
    "    # meta_url = None\n",
    "    # num_train = num_val = None # None means use all samples as specified in webdataset metadata.json\n",
    "    \n",
    "    train_url = \"{/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split/train/train_subj01_{0..17}.tar,/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split/val/val_subj01_0.tar}\"\n",
    "    val_url = \"/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split/test/test_subj01_{0..1}.tar\"\n",
    "    meta_url = \"/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split/metadata_subj01.json\"\n",
    "    num_train = 8559 + 300\n",
    "    num_val = 982\n",
    "\n",
    "# which to use for the voxels\n",
    "if voxel_dims == 1:\n",
    "    voxels_key = 'nsdgeneral.npy'\n",
    "elif voxel_dims == 3:\n",
    "    voxels_key = 'wholebrain_3d.npy'\n",
    "else:\n",
    "    raise Exception(f\"voxel_dims must be 1 or 3, not {voxel_dims}\")\n",
    "\n",
    "print('Prepping train and validation dataloaders...')\n",
    "train_dl, val_dl, num_train, num_val = utils.get_dataloaders(\n",
    "    batch_size,'images',\n",
    "    num_devices=num_devices,\n",
    "    num_workers=num_workers,\n",
    "    train_url=train_url,\n",
    "    val_url=val_url,\n",
    "    meta_url=meta_url,\n",
    "    num_train=num_train,\n",
    "    num_val=num_val,\n",
    "    val_batch_size=300,\n",
    "    cache_dir=\"/tmp/wds-cache\",\n",
    "    seed=seed,\n",
    "    voxels_key=voxels_key,\n",
    "    local_rank=local_rank,\n",
    ")\n",
    "\n",
    "if voxel_dims == 3:\n",
    "    import nibabel as nib\n",
    "    noise_ceils_path = '/fsx/proj-medarc/fmri/natural-scenes-dataset/temp_s3/nsddata_betas/ppdata/subj01/func1pt8mm/betas_fithrf_GLMdenoise_RR/ncsnr.nii.gz'\n",
    "    noise_ceils = nib.load(noise_ceils_path).get_fdata()\n",
    "    # plt.plot(np.sort(noise_ceils.flatten()))\n",
    "    # plt.show()\n",
    "    x_inc,y_inc,z_inc = np.where(noise_ceils > .5)\n",
    "\n",
    "    # check that your data loader is working and save voxel shape after excluding low signal voxels\n",
    "    for val_i, (voxel, img_input, key) in enumerate(val_dl):\n",
    "        voxel = voxel[:,:,np.unique(x_inc),:,:]\n",
    "        voxel = voxel[:,:,:,np.unique(y_inc),:]\n",
    "        voxel = voxel[:,:,:,:,np.unique(z_inc)]\n",
    "        print(\"voxel.shape\", voxel.shape) # voxel.shape torch.Size([300, 3, 68, 64, 47])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e14d0482-dc42-43b9-9ce1-953c32f2c9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Clipper...\n",
      "ViT-L/14 cuda\n",
      "Creating voxel2clip...\n",
      "Input shape: torch.Size([68, 64, 47])\n",
      "Conv 0 output: 64 x [68, 64, 47]\n",
      "Conv 1 output: 128 x [34, 32, 24]\n",
      "Conv 2 output: 256 x [12, 11, 8]\n",
      "Conv 3 output: 128 x [4, 4, 3]\n",
      "Projection input features: 6144\n",
      "params of voxel2clip:\n",
      "param counts:\n",
      "6,713,472 total\n",
      "6,713,472 trainable\n",
      "\n",
      "Done with model preparations!\n"
     ]
    }
   ],
   "source": [
    "print('Creating Clipper...')\n",
    "    \n",
    "# Don't L2 norm the extracted CLIP embeddings since we want the prior \n",
    "# to learn un-normed embeddings for usage with the SD image variation pipeline.\n",
    "clip_extractor = Clipper(clip_variant, clamp_embs=False, norm_embs=False, device=device, train_transforms=train_augs)\n",
    "\n",
    "print('Creating voxel2clip...')\n",
    "\n",
    "if voxel_dims == 1: # 1D data\n",
    "    voxel2clip_kwargs = dict(out_dim=768)\n",
    "    voxel2clip = BrainNetwork(**voxel2clip_kwargs)\n",
    "elif voxel_dims == 3: # 3D data\n",
    "    voxel2clip_kwargs = dict(\n",
    "        out_dim=768,\n",
    "        dims=voxel.shape[2:],\n",
    "        channels=[64, 128, 256, 128],\n",
    "        strides=[1, 2, 3, 3],\n",
    "        padding=[1, 1, 1, 1],\n",
    "        dilation=[1, 1, 1, 1],\n",
    "        kernel=[3, 3, 3, 3],\n",
    "    )\n",
    "    voxel2clip = SimpleVoxel3dConvEncoder(**voxel2clip_kwargs)  \n",
    "\n",
    "print(\"params of voxel2clip:\")\n",
    "if local_rank==0:\n",
    "    utils.count_params(voxel2clip)\n",
    "\n",
    "no_decay = ['bias']\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in voxel2clip.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in voxel2clip.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=initial_lr) # lr doesnt get used if lr_scheduler='cycle'\n",
    "\n",
    "if lr_scheduler == 'fixed':\n",
    "    lr_scheduler = None\n",
    "elif lr_scheduler == 'cycle':\n",
    "    total_steps=num_epochs*(num_train//batch_size)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=max_lr,\n",
    "        total_steps=total_steps,\n",
    "        final_div_factor=1000,\n",
    "        last_epoch=-1, pct_start=2/num_epochs\n",
    "    )\n",
    "    \n",
    "def save_ckpt(tag):\n",
    "    ckpt_path = os.path.join(outdir, f'{tag}.pth')\n",
    "    print(f'saving {ckpt_path}',flush=True)\n",
    "    state_dict = voxel2clip.state_dict()\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': voxel2clip.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_losses': losses,\n",
    "        'val_losses': val_losses,\n",
    "        'fwd_percent_correct': fwd_percent_correct,\n",
    "        'bwd_percent_correct': bwd_percent_correct,\n",
    "        'val_fwd_percent_correct': val_fwd_percent_correct,\n",
    "        'val_bwd_percent_correct': val_bwd_percent_correct,\n",
    "        'lrs': lrs,\n",
    "        }, ckpt_path)\n",
    "        \n",
    "print(\"\\nDone with model preparations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983f458b-35b8-49f2-b6db-80296cece730",
   "metadata": {},
   "source": [
    "# Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a25a662-daa8-4de9-9233-8364800fcb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params for wandb\n",
    "if local_rank==0 and wandb_log:\n",
    "    wandb_project = 'stability'\n",
    "    wandb_run = model_name\n",
    "    wandb_notes = ''\n",
    "    \n",
    "    if wandb_log: \n",
    "        import wandb\n",
    "        print(f\"wandb {wandb_project} run {wandb_run}\")\n",
    "        wandb.login(host='https://stability.wandb.io')#, relogin=True)\n",
    "        wandb_config = {\n",
    "          \"model_name\": model_name,\n",
    "          \"modality\": modality,\n",
    "          \"voxel_dims\": voxel_dims,\n",
    "          \"clip_variant\": clip_variant,\n",
    "          \"batch_size\": batch_size,\n",
    "          \"num_epochs\": num_epochs,\n",
    "          \"use_image_aug\": use_image_aug,\n",
    "          \"max_lr\": max_lr,\n",
    "          \"lr_scheduler\": lr_scheduler,\n",
    "          \"clamp_embs\": clamp_embs,\n",
    "          \"mixup_pct\": mixup_pct,\n",
    "          \"num_train\": num_train,\n",
    "          \"num_val\": num_val,\n",
    "          \"seed\": seed,\n",
    "          \"distributed\": distributed,\n",
    "          \"num_devices\": num_devices,\n",
    "          \"world_size\": world_size,\n",
    "          \"resume_from_ckpt\": resume_from_ckpt,\n",
    "          \"ckpt_path\": ckpt_path,\n",
    "          \"train_url\": train_url,\n",
    "          \"val_url\": val_url,\n",
    "        }\n",
    "        print(\"wandb_config:\\n\",wandb_config)\n",
    "        wandb.init(\n",
    "            project=wandb_project,\n",
    "            name=wandb_run,\n",
    "            config=wandb_config,\n",
    "            notes=wandb_notes,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd80e42-a111-4e6a-adbd-9def3a963306",
   "metadata": {},
   "source": [
    "# Huggingface Accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d5ae0ca-02c2-43d1-b20a-d1a33801873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel2clip, optimizer, train_dl, val_dl, lr_scheduler = accelerator.prepare(\n",
    "    voxel2clip, optimizer, train_dl, val_dl, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5690151-2131-4918-b750-e869cbd1a8a8",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4a3368c-e6ce-49cc-b970-ee3dba12dfcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using cudnn.deterministic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | 0/120 [02:23<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 14.63 GiB (GPU 0; 39.56 GiB total capacity; 17.68 GiB already allocated; 11.25 GiB free; 22.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m     clip_voxels \u001b[38;5;241m=\u001b[39m voxel2clip\u001b[38;5;241m.\u001b[39mmodule(voxel)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 124\u001b[0m     clip_voxels \u001b[38;5;241m=\u001b[39m \u001b[43mvoxel2clip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvoxel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mint\u001b[39m(mixup_pct \u001b[38;5;241m*\u001b[39m num_epochs):\n\u001b[1;32m    127\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mmixco_nce(\n\u001b[1;32m    128\u001b[0m         nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mnormalize(clip_voxels, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), \n\u001b[1;32m    129\u001b[0m         nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mnormalize(clip_target, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    130\u001b[0m         temp\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.006\u001b[39m, \n\u001b[1;32m    131\u001b[0m         distributed\u001b[38;5;241m=\u001b[39mdistributed, accelerator\u001b[38;5;241m=\u001b[39maccelerator, local_rank\u001b[38;5;241m=\u001b[39mlocal_rank)\n",
      "File \u001b[0;32m/fsx/home-paulscotti/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/fsx/proj-medarc/fmri/paulscotti/fMRI-reconstruction-NSD/src/model3d.py:233\u001b[0m, in \u001b[0;36mSimpleVoxel3dConvEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    229\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dropout(x)\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_blocks:\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[0;32m--> 233\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# print('\\t', x.shape)\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# import pdb; pdb.set_trace()\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# [*, c_out_last, x, y, z] -> [*, ]\u001b[39;00m\n\u001b[1;32m    239\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/fsx/home-paulscotti/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/fsx/home-paulscotti/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/fsx/home-paulscotti/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/fsx/home-paulscotti/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fsx/home-paulscotti/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2451\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2452\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 14.63 GiB (GPU 0; 39.56 GiB total capacity; 17.68 GiB already allocated; 11.25 GiB free; 22.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# need non-deterministic CuDNN for conv3D to work\n",
    "utils.seed_everything(seed, cudnn_deterministic=False)\n",
    "\n",
    "epoch = 0\n",
    "losses, val_losses, lrs = [], [], []\n",
    "best_val_loss = 1e9\n",
    "soft_loss_temps = utils.cosine_anneal(0.004, 0.0075, num_epochs - int(mixup_pct * num_epochs))\n",
    "\n",
    "val_voxel0 = val_image0 = None\n",
    "\n",
    "# Optionally resume from checkpoint #\n",
    "if resume_from_ckpt:\n",
    "    print(\"\\n---resuming from ckpt_path---\\n\",ckpt_path)\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    epoch = checkpoint['epoch']\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    try:\n",
    "        voxel2clip.load_state_dict(checkpoint['model_state_dict'])\n",
    "    except:\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "        for key in list(state_dict.keys()):\n",
    "            if 'module.' in key:\n",
    "                state_dict[key.replace('module.', '')] = state_dict[key]\n",
    "                del state_dict[key]\n",
    "        voxel2clip.load_state_dict(state_dict)\n",
    "\n",
    "progress_bar = tqdm(range(epoch,num_epochs), ncols=1200, disable=(local_rank!=0))\n",
    "for epoch in progress_bar:\n",
    "    voxel2clip.train()\n",
    "\n",
    "    sims = 0.\n",
    "    sims_base = 0.\n",
    "    val_sims = 0.\n",
    "    val_sims_base = 0.\n",
    "    fwd_percent_correct = 0.\n",
    "    bwd_percent_correct = 0.\n",
    "    val_fwd_percent_correct = 0.\n",
    "    val_bwd_percent_correct = 0.\n",
    "\n",
    "    for train_i, (voxel, image, trial) in enumerate(train_dl):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        repeat_index = train_i % 3\n",
    "\n",
    "        image = image.float()\n",
    "        voxel = voxel.float()[:,repeat_index].float()\n",
    "        \n",
    "        if voxel_dims == 3:\n",
    "            voxel = voxel[:,np.unique(x_inc),:,:]\n",
    "            voxel = voxel[:,:,np.unique(y_inc),:]\n",
    "            voxel = voxel[:,:,:,np.unique(z_inc)]\n",
    "\n",
    "        if epoch < int(mixup_pct * num_epochs):\n",
    "            voxel, perm, betas, select = utils.mixco(voxel)\n",
    "\n",
    "        if is_text:\n",
    "            annots = utils.select_annotations(subj01_annots[trial], random=True)\n",
    "            clip_target = clip_extractor.embed_text(annots).float()\n",
    "        else:\n",
    "            clip_target = clip_extractor.embed_image(image).float()\n",
    "        clip_target.to(voxel.dtype)\n",
    "        \n",
    "        clip_voxels = voxel2clip(voxel)\n",
    "        \n",
    "        if epoch < int(mixup_pct * num_epochs):\n",
    "            loss = utils.mixco_nce(\n",
    "                nn.functional.normalize(clip_voxels, dim=-1), \n",
    "                nn.functional.normalize(clip_target, dim=-1),\n",
    "                temp=0.006, perm=perm, betas=betas, select=select,\n",
    "                distributed=distributed, accelerator=accelerator, local_rank=local_rank)\n",
    "        else:\n",
    "            epoch_temp = soft_loss_temps[epoch-int(mixup_pct*num_epochs)]\n",
    "            loss = utils.soft_clip_loss(\n",
    "                nn.functional.normalize(clip_voxels, dim=-1), \n",
    "                nn.functional.normalize(clip_target, dim=-1),\n",
    "                temp=epoch_temp,\n",
    "                distributed=distributed, accelerator=accelerator)\n",
    "        utils.check_loss(loss)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        if distributed:\n",
    "            sims_base += F.cosine_similarity(accelerator.gather(clip_target),\n",
    "                                                  accelerator.gather(clip_voxels)).mean().item()\n",
    "        else:\n",
    "            sims_base += F.cosine_similarity(clip_target,clip_voxels).mean().item()\n",
    "\n",
    "        # forward and backward top 1 accuracy\n",
    "        labels = torch.arange(len(clip_target)).to(device)\n",
    "        fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target, clip_voxels), labels, k=1)\n",
    "        bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels, clip_target), labels, k=1)\n",
    "\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "    voxel2clip.eval()\n",
    "    for val_i, (voxel, image, trial) in enumerate(val_dl): \n",
    "        with torch.no_grad():\n",
    "            repeat_index = val_i % 3\n",
    "\n",
    "            image = image.float()\n",
    "            voxel = voxel[:,repeat_index].float()\n",
    "\n",
    "            if voxel_dims == 3:\n",
    "                voxel = voxel[:,np.unique(x_inc),:,:]\n",
    "                voxel = voxel[:,:,np.unique(y_inc),:]\n",
    "                voxel = voxel[:,:,:,np.unique(z_inc)]\n",
    "\n",
    "            if val_image0 is None:\n",
    "                val_image0 = image.detach().clone()\n",
    "                val_voxel0 = voxel.detach().clone()\n",
    "\n",
    "            if is_text:\n",
    "                annots = utils.select_annotations(subj01_annots[trial], random=False)\n",
    "                clip_target = clip_extractor.embed_text(annots).float()\n",
    "            else:\n",
    "                clip_target = clip_extractor.embed_image(image).float()\n",
    "            clip_target.to(voxel.dtype)\n",
    "\n",
    "            if distributed:\n",
    "                clip_voxels = voxel2clip.module(voxel)\n",
    "            else:\n",
    "                clip_voxels = voxel2clip(voxel)\n",
    "\n",
    "            if epoch < int(mixup_pct * num_epochs):\n",
    "                val_loss = utils.mixco_nce(\n",
    "                    nn.functional.normalize(clip_voxels, dim=-1), \n",
    "                    nn.functional.normalize(clip_target, dim=-1),\n",
    "                    temp=0.006, \n",
    "                    distributed=distributed, accelerator=accelerator, local_rank=local_rank)\n",
    "            else:\n",
    "                epoch_temp = soft_loss_temps[epoch-int(mixup_pct*num_epochs)]\n",
    "                val_loss = utils.soft_clip_loss(\n",
    "                    nn.functional.normalize(clip_voxels, dim=-1), \n",
    "                    nn.functional.normalize(clip_target, dim=-1),\n",
    "                    temp=epoch_temp, \n",
    "                    distributed=distributed, accelerator=accelerator)\n",
    "            utils.check_loss(val_loss)\n",
    "\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "            if distributed:\n",
    "                val_sims_base += F.cosine_similarity(accelerator.gather(clip_target),\n",
    "                                                      accelerator.gather(clip_voxels)).mean().item()\n",
    "            else:\n",
    "                val_sims_base += F.cosine_similarity(clip_target,clip_voxels).mean().item()\n",
    "\n",
    "            labels = torch.arange(len(clip_target)).to(device)\n",
    "            val_fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target, clip_voxels), labels, k=1)\n",
    "            val_bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels, clip_target), labels, k=1)\n",
    "\n",
    "    if local_rank==0:\n",
    "        if (not save_at_end and ckpt_saving) or (save_at_end and epoch == num_epochs - 1):\n",
    "            # save best model\n",
    "            val_loss = np.mean(val_losses[-(val_i+1):])\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                save_ckpt('best')\n",
    "            else:\n",
    "                print(f'not best - val_loss: {val_loss:.3f}, best_val_loss: {best_val_loss:.3f}')\n",
    "\n",
    "        # Save model checkpoint every `ckpt_interval`` epochs or on the last epoch\n",
    "        if (ckpt_interval is not None and (epoch + 1) % ckpt_interval == 0) or epoch == num_epochs - 1:\n",
    "            save_ckpt(f'epoch{epoch:03d}')\n",
    "\n",
    "        logs = {\"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "                \"val/loss\": np.mean(val_losses[-(val_i+1):]),\n",
    "                \"train/lr\": lrs[-1],\n",
    "                \"train/num_steps\": len(losses),\n",
    "                \"val/num_steps\": len(val_losses),\n",
    "                \"train/cosine_sim_base\": sims_base / (train_i + 1),\n",
    "                \"val/cosine_sim_base\": val_sims_base / (val_i + 1),\n",
    "                \"train/fwd_pct_correct\": fwd_percent_correct / (train_i + 1),\n",
    "                \"train/bwd_pct_correct\": bwd_percent_correct / (train_i + 1),\n",
    "                \"val/val_fwd_pct_correct\": val_fwd_percent_correct / (val_i + 1),\n",
    "                \"val/val_bwd_pct_correct\": val_bwd_percent_correct / (val_i + 1)}\n",
    "        progress_bar.set_postfix(**logs)\n",
    "\n",
    "        if wandb_log:\n",
    "            wandb.log(logs)\n",
    "            \n",
    "    if distributed:\n",
    "        dist.barrier()\n",
    "\n",
    "if wandb_log and local_rank==0:\n",
    "    wandb.finish()\n",
    "\n",
    "print(\"\\n===Finished!===\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b1eb64-f9a4-4bac-959b-d9d28da2aec9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "62aae01ef0cf7b6af841ab1c8ce59175c4332e693ab3d00bc32ceffb78a35376"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
