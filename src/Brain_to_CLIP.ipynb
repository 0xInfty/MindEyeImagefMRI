{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26ca08a0-e0e9-4cf0-b95c-2c93376f2eb7",
   "metadata": {},
   "source": [
    "This notebook takes brain voxels and maps them to CLIP-space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4d95fac-ac1d-473c-ab96-650f76e6aaf5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Brain_to_CLIP.ipynb to python\n",
      "[NbConvertApp] Writing 32570 bytes to Brain_to_CLIP.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # convert this notebook to .py such that you can then run it via slurm with \"sbatch *.slurm\"\n",
    "# from subprocess import call\n",
    "# command = \"jupyter nbconvert Brain_to_CLIP.ipynb --to python\"\n",
    "# call(command,shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f0f4f3",
   "metadata": {},
   "source": [
    "# Import packages & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bad764b-45c1-45ce-a716-8d055e09821a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-23 22:43:30.138801: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-23 22:43:33.588632: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "distributed = False num_devices = 1 local rank = 0 world size = 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from info_nce import InfoNCE\n",
    "from dalle2_pytorch import DiffusionPriorNetwork\n",
    "import kornia\n",
    "from kornia.augmentation.container import AugmentationSequential\n",
    "\n",
    "import torch.distributed as dist\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# uses tf32 data type which is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# Multi-GPU config #\n",
    "accelerator = Accelerator()\n",
    "print = accelerator.print # only print if local_rank=0\n",
    "\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0: num_devices = 1\n",
    "num_workers = num_devices\n",
    "\n",
    "print(accelerator.state)\n",
    "local_rank = accelerator.state.local_process_index\n",
    "world_size = accelerator.state.num_processes\n",
    "distributed = not accelerator.state.distributed_type == 'NO'\n",
    "print(\"distributed =\",distributed,\"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size)\n",
    "\n",
    "# custom models and functions #\n",
    "import utils\n",
    "from utils import torch_to_matplotlib, torch_to_Image\n",
    "from models import BrainNetwork, BrainDiffusionPrior\n",
    "# from model3d import SimpleVoxel3dConvEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018b82b-c054-4463-9527-4b0c2a75bda6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b61fec7-72a0-4b67-86da-1375f1d9fbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--model_name=testing', '--modality=image', '--batch_size=1280', '--n_samples_save=0', '--max_lr=3e-4', '--mixup_pct=.33', '--num_epochs=300', '--no-norm_embs', '--ckpt_interval=5']\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    jupyter_args=[]\n",
    "    jupyter_args.append(\"--model_name=testing\")\n",
    "    jupyter_args.append(\"--modality=image\")\n",
    "    # jupyter_args.append(\"--clip_variant=ViT-L/14\")\n",
    "    # jupyter_args.append(\"--clip_variant=ViT-B/32\")\n",
    "    jupyter_args.append(\"--batch_size=1280\")\n",
    "    # jupyter_args.append(\"--with_mse\")\n",
    "    # jupyter_args.append(\"--versatile\")\n",
    "    jupyter_args.append(\"--n_samples_save=0\")\n",
    "    # jupyter_args.append(\"--mse_amount=.1\")\n",
    "    jupyter_args.append(\"--max_lr=3e-4\")\n",
    "    jupyter_args.append(\"--mixup_pct=.33\")\n",
    "    jupyter_args.append(\"--num_epochs=300\")\n",
    "    jupyter_args.append(\"--no-norm_embs\")\n",
    "    # jupyter_args.append(\"--wandb_log\")\n",
    "    jupyter_args.append(\"--ckpt_interval=5\")\n",
    "    # jupyter_args.append(\"--resume_from_ckpt\")\n",
    "    print(jupyter_args)\n",
    "    \n",
    "    from IPython.display import clear_output\n",
    "    \n",
    "    %load_ext autoreload\n",
    "    %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2028bdf0-2f41-46d9-b6e7-86b870dbf16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--modality\", type=str, default=\"image\", choices=[\"image\", \"text\"],\n",
    "    help=\"image or text\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=300,\n",
    "    help=\"Our maximum for A100 was 300 for 1dim voxels and 128 for 3dim voxels\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--clip_variant\",type=str,default=\"ViT-L/14\",choices=[\"RN50\", \"ViT-L/14\", \"ViT-B/32\", \"ViT-H-14\", \"RN50x64\"],\n",
    "    help='clip / openclip variant',\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--outdir\",type=str,default=None,\n",
    "    help=\"output directory for logs and checkpoints\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_log\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to log to wandb\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--resume_from_ckpt\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"if not using wandb and want to resume from a ckpt\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_project\",type=str,default=\"stability\",\n",
    "    help=\"wandb project name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mixup_pct\",type=float,default=.5,\n",
    "    help=\"proportion of way through training when to switch from InfoNCE to soft_clip_loss\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--voxel_dims\",type=int,default=1,choices=[1, 3],\n",
    "    help=\"1 for flattened input, 3 for 3d input\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_image_aug\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to use image augmentation (only used for modality=image)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epochs\",type=int,default=120,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler_type\",type=str,default='cycle',choices=['cycle','fixed'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_saving\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_interval\",type=int,default=5,\n",
    "    help=\"save backup ckpt and reconstruct every x epochs\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--save_at_end\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"if False, will save best.ckpt whenever epoch shows best validation score\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--with_mse\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"Add mse loss to the other losses\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cos_base\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"Add scaled cosine similarity to the other losses\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--text_token\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"Map to SD token input 77x768 if modality is text, rather than CLIP's 768\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mse_mult\",type=int,default=1,\n",
    "    help=\"Multiplier for mse loss\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--versatile\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"Map to 257x768 versatile diffusion CLIP space, not including class token\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--norm_embs\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"Do norming (using cls token if VD) of CLIP embeddings\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mse_amount\",type=float,default=.5,\n",
    "    help=\"What percentage of to weight mse vs. soft_clip loss\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_lr\",type=float,default=3e-4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_samples_save\",type=int,default=0,\n",
    "    help=\"Number of reconstructions for monitoring progress, 0 will speed up training\",\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60cd7f2c-37fd-426b-a0c6-633e51bc4c4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if outdir is None:\n",
    "    outdir = os.path.abspath(f'../train_logs/{model_name}')\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir,exist_ok=True)\n",
    "if use_image_aug:\n",
    "    # img_augment = AugmentationSequential(\n",
    "    #     kornia.augmentation.RandomResizedCrop((224,224), (0.6,1), p=0.3),\n",
    "    #     kornia.augmentation.Resize((224, 224)),\n",
    "    #     kornia.augmentation.RandomHorizontalFlip(p=0.5),\n",
    "    #     kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1, p=0.3),\n",
    "    #     kornia.augmentation.RandomGrayscale(p=0.3),\n",
    "    #     data_keys=[\"input\"],\n",
    "    # )\n",
    "    img_augment = AugmentationSequential(\n",
    "        kornia.augmentation.RandomResizedCrop((240,240), (0.6,1), p=0.3),\n",
    "        kornia.augmentation.Resize((224, 224)),\n",
    "        kornia.augmentation.RandomGaussianBlur(kernel_size=(7,7), sigma=(5,5), p=0.3), #MedianBlur is better but computationally inefficient\n",
    "        # kornia.augmentation.RandomHorizontalFlip(p=0.5),\n",
    "        kornia.augmentation.ColorJiggle(brightness=0.2, contrast=0.2, saturation=0.3, hue=0., p=0.3),\n",
    "    )\n",
    "\n",
    "if modality=='text':\n",
    "    annots = np.load(\"/fsx/proj-medarc/fmri/natural-scenes-dataset/COCO_73k_annots_curated.npy\")\n",
    "    if text_token:\n",
    "        import logging\n",
    "        logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\n",
    "        from transformers import CLIPTextModelWithProjection, CLIPTokenizer\n",
    "        tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "        text_encoder = CLIPTextModelWithProjection.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "        text_encoder.eval()\n",
    "        text_encoder.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d13c25-1369-4c49-81d4-83d713586096",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prep models and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7966d5a-c8a9-4461-808a-2f89bb00fe9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using cudnn.deterministic\n",
      "Pulling NSD webdataset data...\n",
      "Prepping train and validation dataloaders...\n",
      "Getting dataloaders...\n",
      "\n",
      "num_train 8859\n",
      "global_batch_size 1280\n",
      "batch_size 1280\n",
      "num_workers 1\n",
      "num_batches 6\n",
      "num_worker_batches 6\n",
      "cache_dir None\n",
      "\n",
      "num_val 982\n",
      "val_num_batches 0\n",
      "val_batch_size 300\n"
     ]
    }
   ],
   "source": [
    "# need non-deterministic CuDNN for conv3D to work\n",
    "utils.seed_everything(seed, cudnn_deterministic=False)\n",
    "\n",
    "print('Pulling NSD webdataset data...')\n",
    "\n",
    "train_url = \"{/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split/train/train_subj01_{0..17}.tar,/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split/val/val_subj01_0.tar}\"\n",
    "val_url = \"/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split/test/test_subj01_{0..1}.tar\"\n",
    "meta_url = \"/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split/metadata_subj01.json\"\n",
    "num_train = 8559 + 300\n",
    "num_val = 982\n",
    "\n",
    "# which to use for the voxels\n",
    "if voxel_dims == 1:\n",
    "    voxels_key = 'nsdgeneral.npy'\n",
    "elif voxel_dims == 3:\n",
    "    voxels_key = 'wholebrain_3d.npy'\n",
    "else:\n",
    "    raise Exception(f\"voxel_dims must be 1 or 3, not {voxel_dims}\")\n",
    "\n",
    "print('Prepping train and validation dataloaders...')\n",
    "train_dl, val_dl, num_train, num_val = utils.get_dataloaders(\n",
    "    batch_size,'images',\n",
    "    num_devices=num_devices,\n",
    "    num_workers=num_workers,\n",
    "    train_url=train_url,\n",
    "    val_url=val_url,\n",
    "    meta_url=meta_url,\n",
    "    num_train=num_train,\n",
    "    num_val=num_val,\n",
    "    val_batch_size=300,\n",
    "    cache_dir=\"/tmp/wds-cache\",\n",
    "    seed=seed,\n",
    "    voxels_key=voxels_key,\n",
    "    to_tuple=[\"voxels\", \"images\", \"coco\"],\n",
    "    local_rank=local_rank,\n",
    "    world_size=world_size,\n",
    ")\n",
    "\n",
    "if voxel_dims == 3:\n",
    "    import nibabel as nib\n",
    "    noise_ceils_path = '/fsx/proj-medarc/fmri/natural-scenes-dataset/temp_s3/nsddata_betas/ppdata/subj01/func1pt8mm/betas_fithrf_GLMdenoise_RR/ncsnr.nii.gz'\n",
    "    noise_ceils = nib.load(noise_ceils_path).get_fdata()\n",
    "    x_inc,y_inc,z_inc = np.where(noise_ceils > .5) # voxel.shape torch.Size([300, 3, 68, 64, 47])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e14d0482-dc42-43b9-9ce1-953c32f2c9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Clipper...\n",
      "ViT-L/14 cuda\n",
      "out_dim: 768\n",
      "Creating voxel2clip...\n",
      "params of voxel2clip:\n",
      "param counts:\n",
      "134,722,305 total\n",
      "134,722,305 trainable\n",
      "\n",
      "Done with model preparations!\n"
     ]
    }
   ],
   "source": [
    "print('Creating Clipper...')\n",
    "    \n",
    "# Don't L2 norm the extracted CLIP embeddings since we want the prior \n",
    "# to learn un-normed embeddings for usage with the SD image variation pipeline.\n",
    "from models import Clipper\n",
    "clip_sizes = {\"RN50\": 1024, \"ViT-L/14\": 768, \"ViT-B/32\": 512}\n",
    "out_dim = clip_sizes[clip_variant]\n",
    "if versatile:\n",
    "    print(\"Using versatile CLIP space\")\n",
    "    clip_extractor = Clipper(clip_variant, device=device, hidden_state=True, norm_embs=norm_embs)\n",
    "    out_dim = 257 * out_dim\n",
    "elif modality=='image':\n",
    "    clip_extractor = Clipper(clip_variant, device=device, hidden_state=True, norm_embs=norm_embs)\n",
    "    out_dim = out_dim\n",
    "elif modality=='text' and not text_token:\n",
    "    clip_extractor = Clipper(clip_variant, device=device, hidden_state=True, norm_embs=norm_embs)\n",
    "    out_dim = out_dim\n",
    "elif text_token:\n",
    "    out_dim = 77 * out_dim\n",
    "print(\"out_dim:\",out_dim)\n",
    "\n",
    "print('Creating voxel2clip...')\n",
    "\n",
    "if voxel_dims == 1: # 1D data\n",
    "    voxel2clip_kwargs = dict(out_dim=out_dim)\n",
    "    voxel2clip = BrainNetwork(**voxel2clip_kwargs)\n",
    "elif voxel_dims == 3: # 3D data\n",
    "    if modality=='text':\n",
    "        voxel2clip_kwargs = dict(\n",
    "            out_dim=77*outdim,\n",
    "            dims=voxel.shape[2:],\n",
    "            channels=[64, 128, 256, 128],\n",
    "            strides=[1, 2, 3, 3],\n",
    "            padding=[1, 1, 1, 1],\n",
    "            dilation=[1, 1, 1, 1],\n",
    "            kernel=[3, 3, 3, 3],\n",
    "        )\n",
    "    else:\n",
    "        voxel2clip_kwargs = dict(\n",
    "            out_dim=out_dim,\n",
    "            dims=voxel.shape[2:],\n",
    "            channels=[64, 128, 256, 128],\n",
    "            strides=[1, 2, 3, 3],\n",
    "            padding=[1, 1, 1, 1],\n",
    "            dilation=[1, 1, 1, 1],\n",
    "            kernel=[3, 3, 3, 3],\n",
    "        )\n",
    "    voxel2clip = SimpleVoxel3dConvEncoder(**voxel2clip_kwargs)  \n",
    "    \n",
    "print(\"params of voxel2clip:\")\n",
    "if local_rank==0:\n",
    "    utils.count_params(voxel2clip)\n",
    "    \n",
    "# need to first prep the model before the optimizer for FullyShardedDataParallel (FSDP)\n",
    "voxel2clip = accelerator.prepare(voxel2clip)\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in voxel2clip.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in voxel2clip.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=3e-4) # lr doesnt get used if lr_scheduler_type='cycle'\n",
    "\n",
    "if lr_scheduler_type == 'fixed':\n",
    "    lr_scheduler = None\n",
    "elif lr_scheduler_type == 'cycle':\n",
    "    global_batch_size = batch_size * num_devices\n",
    "    total_steps=num_epochs*(num_train//global_batch_size)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=max_lr,\n",
    "        total_steps=total_steps,\n",
    "        final_div_factor=1000,\n",
    "        last_epoch=-1, pct_start=2/num_epochs\n",
    "    )\n",
    "    \n",
    "if n_samples_save > 0:\n",
    "    import umap\n",
    "    if local_rank == 0: print('Creating versatile diffusion reconstruction pipeline...')\n",
    "    from diffusers import VersatileDiffusionDualGuidedPipeline, UniPCMultistepScheduler\n",
    "    from diffusers.models import DualTransformer2DModel\n",
    "    vd_cache_dir = '/fsx/proj-medarc/fmri/cache/models--shi-labs--versatile-diffusion/snapshots/2926f8e11ea526b562cd592b099fcf9c2985d0b7'\n",
    "    vd_pipe =  VersatileDiffusionDualGuidedPipeline.from_pretrained(\n",
    "            # \"lambdalabs/sd-image-variations-diffusers\",\n",
    "            vd_cache_dir,\n",
    "            safety_checker=None,\n",
    "            requires_safety_checker=False,\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "    vd_pipe.image_unet.eval()\n",
    "    vd_pipe.vae.eval()\n",
    "    vd_pipe.image_unet.requires_grad_(False)\n",
    "    vd_pipe.vae.requires_grad_(False)\n",
    "\n",
    "    vd_pipe.scheduler = UniPCMultistepScheduler.from_pretrained(vd_cache_dir, subfolder=\"scheduler\")\n",
    "    num_inference_steps = 20\n",
    "\n",
    "    # Set weighting of Dual-Guidance \n",
    "    text_image_ratio = .0 # .5 means equally weight text and image, 0 means only use image\n",
    "    condition_types = (\"text\", \"image\")\n",
    "    for name, module in vd_pipe.image_unet.named_modules():\n",
    "        if isinstance(module, DualTransformer2DModel):\n",
    "            module.mix_ratio = text_image_ratio\n",
    "            for i, type in enumerate(condition_types):\n",
    "                if type == \"text\":\n",
    "                    module.condition_lengths[i] = 77\n",
    "                    module.transformer_index_for_condition[i] = 1  # use the second (text) transformer\n",
    "                else:\n",
    "                    module.condition_lengths[i] = 257\n",
    "                    module.transformer_index_for_condition[i] = 0  # use the first (image) transformer\n",
    "    \n",
    "def save_ckpt(tag):    \n",
    "    ckpt_path = outdir+f'/{tag}.pth'\n",
    "    print(f'\\nsaving {ckpt_path}',flush=True)\n",
    "    if local_rank==0:\n",
    "        unwrapped_model = accelerator.unwrap_model(voxel2clip)\n",
    "\n",
    "        if distributed:\n",
    "            accelerator.save_state(output_dir=outdir)\n",
    "            print(\"accelerate state saved!\")\n",
    "        else:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': unwrapped_model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'lr_scheduler': deepcopy(lr_scheduler.state_dict()),\n",
    "                'train_losses': losses,\n",
    "                'val_losses': val_losses,\n",
    "                'lrs': lrs,\n",
    "                }, ckpt_path)\n",
    "        print(\"finished saving!\\n\")\n",
    "            \n",
    "        del unwrapped_model\n",
    "        \n",
    "print(\"\\nDone with model preparations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983f458b-35b8-49f2-b6db-80296cece730",
   "metadata": {},
   "source": [
    "# Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a25a662-daa8-4de9-9233-8364800fcb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params for wandb\n",
    "if local_rank==0 and wandb_log:\n",
    "    import wandb\n",
    "    \n",
    "    wandb_project = 'stability'\n",
    "    wandb_run = model_name\n",
    "    wandb_notes = ''\n",
    "    \n",
    "    print(f\"wandb {wandb_project} run {wandb_run}\")\n",
    "    wandb.login(host='https://stability.wandb.io')#, relogin=True)\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"modality\": modality,\n",
    "      \"voxel_dims\": voxel_dims,\n",
    "      \"clip_variant\": clip_variant,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"use_image_aug\": use_image_aug,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"lr_scheduler_type\": lr_scheduler_type,\n",
    "      \"mixup_pct\": mixup_pct,\n",
    "      \"mse_amount\": mse_amount,\n",
    "      \"num_train\": num_train,\n",
    "      \"num_val\": num_val,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "      \"train_url\": train_url,\n",
    "      \"val_url\": val_url,\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    if True: # wandb_auto_resume\n",
    "        print(\"wandb_id:\",model_name)\n",
    "        wandb.init(\n",
    "            id = model_name,\n",
    "            project=wandb_project,\n",
    "            name=wandb_run,\n",
    "            config=wandb_config,\n",
    "            notes=wandb_notes,\n",
    "            resume=\"allow\",\n",
    "        )\n",
    "    else:\n",
    "        wandb.init(\n",
    "            project=wandb_project,\n",
    "            name=wandb_run,\n",
    "            config=wandb_config,\n",
    "            notes=wandb_notes,\n",
    "        )\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5690151-2131-4918-b750-e869cbd1a8a8",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12de6387-6e18-4e4b-b5ce-a847d625330a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using cudnn.deterministic\n"
     ]
    }
   ],
   "source": [
    "# need non-deterministic CuDNN for conv3D to work\n",
    "utils.seed_everything(seed, cudnn_deterministic=False)\n",
    "\n",
    "epoch = 0\n",
    "losses, val_losses, lrs = [], [], []\n",
    "nce_losses, val_nce_losses = [], []\n",
    "mse_losses, val_mse_losses =  [], []\n",
    "sim_losses, val_sim_losses = [], []\n",
    "best_val_loss = 1e9\n",
    "\n",
    "mse = nn.MSELoss()\n",
    "val_voxel0 = val_image0 = None\n",
    "\n",
    "# Optionally resume from checkpoint #\n",
    "if resume_from_ckpt:\n",
    "    print(\"\\n---resuming from last.pth ckpt---\\n\")\n",
    "    try:\n",
    "        # checkpoint = torch.load('/fsx/proj-medarc/fmri/paulscotti/fMRI-reconstruction-NSD/train_logs/mse_normL/last_backup.pth', map_location='cpu')\n",
    "        checkpoint = torch.load(outdir+'/last.pth', map_location='cpu')\n",
    "    except:\n",
    "        print('last.pth failed... trying last_backup.pth')\n",
    "        checkpoint = torch.load(outdir+'/last_backup.pth', map_location='cpu')\n",
    "    epoch = checkpoint['epoch']\n",
    "    print(\"Epoch\",epoch)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    voxel2clip.load_state_dict(checkpoint['model_state_dict'])\n",
    "    del checkpoint\n",
    "elif wandb_log:\n",
    "    if wandb.run.resumed:\n",
    "        print(\"\\n---resuming from last.pth ckpt---\\n\")\n",
    "        if distributed:\n",
    "            accelerator.load_state(outdir)\n",
    "            # epoch = checkpoint['epoch']\n",
    "            # print(\"Epoch\",epoch)\n",
    "        else:\n",
    "            try:\n",
    "                checkpoint = torch.load(outdir+'/last.pth', map_location='cpu')\n",
    "            except:\n",
    "                print('last.pth failed... trying last_backup.pth')\n",
    "                checkpoint = torch.load(outdir+'/last_backup.pth', map_location='cpu')\n",
    "            epoch = checkpoint['epoch']\n",
    "            print(\"Epoch\",epoch)\n",
    "            if not distributed:\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "            if distributed:\n",
    "                voxel2clip.module.load_state_dict(checkpoint['model_state_dict'])\n",
    "            else:\n",
    "                voxel2clip.load_state_dict(checkpoint['model_state_dict'])\n",
    "            del checkpoint\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d5ae0ca-02c2-43d1-b20a-d1a33801873f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrainNetwork(\n",
      "  (lin0): Sequential(\n",
      "    (0): Linear(in_features=15724, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (mlp): ModuleList(\n",
      "    (0-3): 4 x Sequential(\n",
      "      (0): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (lin1): Linear(in_features=4096, out_features=768, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "optimizer, train_dl, val_dl, lr_scheduler = accelerator.prepare(\n",
    "optimizer, train_dl, val_dl, lr_scheduler\n",
    ")\n",
    "print(voxel2clip)\n",
    "accelerator.register_for_checkpointing(lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "491d3481-ce75-41f3-a69c-821d6946b25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 0/300 [01:52<?, ?it/s, train/bwd_pct_correct=tensor(0.0017, device='cuda:0'), train/cosine_sim_base=0.011, train/fwd_pct_correct=tensor(0.0022, device='cuda:0'), train/loss=13.5, train/lr=0.000136, train/mse_losses=2.91, train/nce_losses=13.5, train/num_steps=6, train/sim_losses=0.989, train/temp=0.00629, val/cosine_sim_base=0.0942, val/loss=7.77, val/mse_losses=0.589, val/nce_losses=7.77, val/num_steps=3, val/sim_losses=0.906, val/val_bwd_pct_correct=tensor(0.0456, device='cuda:0'), val/val_fwd_pct_correct=tensor(0.0467, device='cuda:0')]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "saving /fsx/proj-medarc/fmri/paulscotti/fMRI-reconstruction-NSD/train_logs/testing/last.pth\n",
      "\n",
      "saving /fsx/proj-medarc/fmri/paulscotti/fMRI-reconstruction-NSD/train_logs/testing/last_backup.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|██                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | 1/300 [02:53<14:25:03, 173.59s/it, train/bwd_pct_correct=tensor(0.0017, device='cuda:0'), train/cosine_sim_base=0.011, train/fwd_pct_correct=tensor(0.0022, device='cuda:0'), train/loss=13.5, train/lr=0.000136, train/mse_losses=2.91, train/nce_losses=13.5, train/num_steps=6, train/sim_losses=0.989, train/temp=0.00629, val/cosine_sim_base=0.0942, val/loss=7.77, val/mse_losses=0.589, val/nce_losses=7.77, val/num_steps=3, val/sim_losses=0.906, val/val_bwd_pct_correct=tensor(0.0456, device='cuda:0'), val/val_fwd_pct_correct=tensor(0.0467, device='cuda:0')]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 5.02 GiB (GPU 0; 39.56 GiB total capacity; 28.58 GiB already allocated; 1.61 GiB free; 36.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m         clip_target \u001b[38;5;241m=\u001b[39m clip_extractor\u001b[38;5;241m.\u001b[39membed_text(prompt)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     clip_target \u001b[38;5;241m=\u001b[39m \u001b[43mclip_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m versatile \u001b[38;5;129;01mand\u001b[39;00m modality\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m clip_variant\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mViT-L/14\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     60\u001b[0m     clip_target \u001b[38;5;241m=\u001b[39m clip_target[:,:\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/fsx/proj-medarc/fmri/paulscotti/fMRI-reconstruction-NSD/src/models.py:90\u001b[0m, in \u001b[0;36mClipper.embed_image\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_variant\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mViT-L/14\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_state:\n\u001b[1;32m     89\u001b[0m     clip_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess((image\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1.5\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m.25\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)) \u001b[38;5;66;03m# for some reason the /1.5+.25 prevents oversaturation\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m     clip_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip_emb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     clip_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mversatile_normalize_embeddings(clip_emb)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/fsx/home-paulscotti/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/fsx/home-paulscotti/miniconda3/envs/medical-v1/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:1304\u001b[0m, in \u001b[0;36mCLIPVisionModelWithProjection.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;124;03m>>> image_embeds = outputs.image_embeds\u001b[39;00m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[1;32m   1302\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1304\u001b[0m vision_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1311\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m vision_outputs[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# pooled_output\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m image_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisual_projection(pooled_output)\n",
      "File \u001b[0;32m/fsx/home-paulscotti/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/fsx/home-paulscotti/miniconda3/envs/medical-v1/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:862\u001b[0m, in \u001b[0;36mCLIPVisionTransformer.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    859\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(pixel_values)\n\u001b[1;32m    860\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_layrnorm(hidden_states)\n\u001b[0;32m--> 862\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    869\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    870\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m last_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]\n",
      "File \u001b[0;32m/fsx/home-paulscotti/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/fsx/home-paulscotti/miniconda3/envs/medical-v1/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:654\u001b[0m, in \u001b[0;36mCLIPEncoder.forward\u001b[0;34m(self, inputs_embeds, attention_mask, causal_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    647\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    648\u001b[0m         create_custom_forward(encoder_layer),\n\u001b[1;32m    649\u001b[0m         hidden_states,\n\u001b[1;32m    650\u001b[0m         attention_mask,\n\u001b[1;32m    651\u001b[0m         causal_attention_mask,\n\u001b[1;32m    652\u001b[0m     )\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 654\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    661\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/fsx/home-paulscotti/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/fsx/home-paulscotti/miniconda3/envs/medical-v1/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:393\u001b[0m, in \u001b[0;36mCLIPEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    391\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    392\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm2(hidden_states)\n\u001b[0;32m--> 393\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    396\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m/fsx/home-paulscotti/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/fsx/home-paulscotti/miniconda3/envs/medical-v1/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:349\u001b[0m, in \u001b[0;36mCLIPMLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    348\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(hidden_states)\n\u001b[0;32m--> 349\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(hidden_states)\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/fsx/home-paulscotti/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/fsx/home-paulscotti/miniconda3/envs/medical-v1/lib/python3.10/site-packages/transformers/activations.py:96\u001b[0m, in \u001b[0;36mQuickGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.702\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5.02 GiB (GPU 0; 39.56 GiB total capacity; 28.58 GiB already allocated; 1.61 GiB free; 36.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "print(f\"{model_name} starting with epoch {epoch} / {num_epochs}\")\n",
    "progress_bar = tqdm(range(epoch,num_epochs), ncols=1200, disable=(local_rank!=0))\n",
    "for epoch in progress_bar:\n",
    "    voxel2clip.train()\n",
    "\n",
    "    sims_base = 0.\n",
    "    val_sims_base = 0.\n",
    "    fwd_percent_correct = 0.\n",
    "    bwd_percent_correct = 0.\n",
    "    val_fwd_percent_correct = 0.\n",
    "    val_bwd_percent_correct = 0.\n",
    "\n",
    "    for train_i, (voxel, image, coco) in enumerate(train_dl):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        repeat_index = train_i % 3\n",
    "\n",
    "        image = image.float()\n",
    "        voxel = voxel.float()[:,repeat_index].float()\n",
    "        \n",
    "        if use_image_aug:\n",
    "            image = img_augment(image)\n",
    "        \n",
    "        if voxel_dims == 3:\n",
    "            voxel = voxel[:,np.unique(x_inc),:,:]\n",
    "            voxel = voxel[:,:,np.unique(y_inc),:]\n",
    "            voxel = voxel[:,:,:,np.unique(z_inc)]\n",
    "\n",
    "        if epoch < int(mixup_pct * num_epochs):\n",
    "            voxel, perm, betas, select = utils.mixco(voxel)\n",
    "\n",
    "        if modality=='text':\n",
    "            prompt = utils.select_annotations(annots[coco.cpu().numpy()], random=True).tolist()\n",
    "            if text_token:\n",
    "                text_inputs = tokenizer(\n",
    "                            prompt,\n",
    "                            padding=\"max_length\",\n",
    "                            max_length=tokenizer.model_max_length,\n",
    "                            truncation=True,\n",
    "                            return_tensors=\"pt\",\n",
    "                        )\n",
    "                text_input_ids = text_inputs.input_ids\n",
    "                if hasattr(text_encoder.config, \"use_attention_mask\") and text_encoder.config.use_attention_mask:\n",
    "                    attention_mask = text_inputs.attention_mask.to(device)\n",
    "                else:\n",
    "                    attention_mask = None\n",
    "                prompt_embeds = text_encoder(\n",
    "                    text_input_ids.to(device),\n",
    "                    attention_mask=attention_mask,\n",
    "                )\n",
    "                embeds = text_encoder.text_projection(prompt_embeds.last_hidden_state)\n",
    "                embeds_pooled = prompt_embeds.text_embeds\n",
    "                clip_target = embeds / torch.norm(embeds_pooled.unsqueeze(1), dim=-1, keepdim=True)\n",
    "            else:\n",
    "                clip_target = clip_extractor.embed_text(prompt).float()\n",
    "        else:\n",
    "            clip_target = clip_extractor.embed_image(image).float()\n",
    "            \n",
    "        if not versatile and modality=='image' and clip_variant=='ViT-L/14':\n",
    "            clip_target = clip_target[:,:1]\n",
    "        clip_target = clip_target.view(len(clip_target),-1).to(voxel.dtype)\n",
    "        \n",
    "        clip_voxels = voxel2clip(voxel)\n",
    "        \n",
    "        clip_voxels_norm = nn.functional.normalize(clip_voxels, dim=-1)\n",
    "        clip_target_norm = nn.functional.normalize(clip_target, dim=-1)\n",
    "        \n",
    "        if epoch < int(mixup_pct * num_epochs):\n",
    "            loss = utils.mixco_nce(\n",
    "                clip_voxels_norm,\n",
    "                clip_target_norm,\n",
    "                temp=voxel2clip.module.temp if distributed else voxel2clip.temp, \n",
    "                perm=perm, betas=betas, select=select)\n",
    "        else:\n",
    "            loss = utils.soft_clip_loss(\n",
    "                clip_voxels_norm,\n",
    "                clip_target_norm,\n",
    "                temp=voxel2clip.module.temp if distributed else voxel2clip.temp)\n",
    "        nce_losses.append(loss.item())\n",
    "        \n",
    "        if epoch < int(mixup_pct * num_epochs):\n",
    "            mseloss = mse(clip_voxels[~select], clip_target[~select])\n",
    "        else:\n",
    "            mseloss = mse(clip_voxels, clip_target)\n",
    "        mse_losses.append(mseloss.item())\n",
    "        if with_mse:\n",
    "            mseloss *= 3000\n",
    "            loss += mseloss   \n",
    "        utils.check_loss(loss)\n",
    "        \n",
    "        sim = 1-F.cosine_similarity(clip_target_norm,clip_voxels_norm).mean()\n",
    "        if cos_base:\n",
    "            sim *= 8\n",
    "            loss += sim\n",
    "        sim_losses.append(sim.item())\n",
    "        utils.check_loss(loss)\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        lrs.append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        sims_base += F.cosine_similarity(clip_target_norm,clip_voxels_norm).mean().item()\n",
    "        \n",
    "        # forward and backward top 1 accuracy        \n",
    "        labels = torch.arange(len(clip_target)).to(device) \n",
    "        fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1)\n",
    "        bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1)\n",
    "        \n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "        if lr_scheduler_type is not None:\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "    voxel2clip.eval()\n",
    "    for val_i, (voxel, image, coco) in enumerate(val_dl): \n",
    "        with torch.no_grad():\n",
    "            # repeat_index = val_i % 3\n",
    "\n",
    "            image = image.float()\n",
    "            # voxel = voxel[:,repeat_index].float()\n",
    "            voxel = torch.mean(voxel,axis=1).float().to(device)\n",
    "\n",
    "            if voxel_dims == 3:\n",
    "                voxel = voxel[:,np.unique(x_inc),:,:]\n",
    "                voxel = voxel[:,:,np.unique(y_inc),:]\n",
    "                voxel = voxel[:,:,:,np.unique(z_inc)]\n",
    "\n",
    "            if val_image0 is None:\n",
    "                val_image0 = image.detach().clone()\n",
    "                val_voxel0 = voxel.detach().clone()\n",
    "\n",
    "            if modality=='text':\n",
    "                prompt = utils.select_annotations(annots[coco.cpu().numpy()], random=True).tolist()\n",
    "                if text_token:\n",
    "                    text_inputs = tokenizer(\n",
    "                                prompt,\n",
    "                                padding=\"max_length\",\n",
    "                                max_length=tokenizer.model_max_length,\n",
    "                                truncation=True,\n",
    "                                return_tensors=\"pt\",\n",
    "                            )\n",
    "                    text_input_ids = text_inputs.input_ids\n",
    "                    if hasattr(text_encoder.config, \"use_attention_mask\") and text_encoder.config.use_attention_mask:\n",
    "                        attention_mask = text_inputs.attention_mask.to(device)\n",
    "                    else:\n",
    "                        attention_mask = None\n",
    "                    prompt_embeds = text_encoder(\n",
    "                        text_input_ids.to(device),\n",
    "                        attention_mask=attention_mask,\n",
    "                    )\n",
    "                    embeds = text_encoder.text_projection(prompt_embeds.last_hidden_state)\n",
    "                    embeds_pooled = prompt_embeds.text_embeds\n",
    "                    clip_target = embeds / torch.norm(embeds_pooled.unsqueeze(1), dim=-1, keepdim=True)\n",
    "                else:\n",
    "                    clip_target = clip_extractor.embed_text(prompt).float()\n",
    "            else:\n",
    "                clip_target = clip_extractor.embed_image(image).float()\n",
    "\n",
    "            if not versatile and modality=='image' and clip_variant=='ViT-L/14':\n",
    "                clip_target = clip_target[:,:1]\n",
    "            clip_target = clip_target.view(len(clip_target),-1).to(voxel.dtype)\n",
    "\n",
    "            clip_voxels = voxel2clip(voxel)\n",
    "\n",
    "            clip_voxels_norm = nn.functional.normalize(clip_voxels, dim=-1)\n",
    "            clip_target_norm = nn.functional.normalize(clip_target, dim=-1)\n",
    "\n",
    "            # applying same mixco to val doesnt work if batch sizes are different\n",
    "            # if epoch < int(mixup_pct * num_epochs):\n",
    "            #     val_loss = utils.mixco_nce(\n",
    "            #         clip_voxels_norm,\n",
    "            #         clip_target_norm,\n",
    "            #         temp=voxel2clip.module.temp if distributed else voxel2clip.temp,\n",
    "            #         perm=perm, betas=betas, select=select)\n",
    "            # else:\n",
    "            val_loss = utils.soft_clip_loss(\n",
    "                clip_voxels_norm,\n",
    "                clip_target_norm,\n",
    "                temp=voxel2clip.module.temp if distributed else voxel2clip.temp)\n",
    "            val_nce_losses.append(val_loss.item())\n",
    "\n",
    "            # if epoch < int(mixup_pct * num_epochs):\n",
    "            #     val_mseloss = mse(clip_voxels[~select], clip_target[~select])\n",
    "            # else:\n",
    "            val_mseloss = mse(clip_voxels,clip_target)\n",
    "            val_mse_losses.append(val_mseloss.item())\n",
    "            if with_mse:\n",
    "                val_mseloss *= 3000\n",
    "                val_loss += val_mseloss\n",
    "            utils.check_loss(val_loss)\n",
    "\n",
    "            val_sim = 1-F.cosine_similarity(clip_target_norm,clip_voxels_norm).mean()\n",
    "            val_sim_losses.append(val_sim.item())\n",
    "            if cos_base:\n",
    "                val_sim *= 8\n",
    "                val_loss += val_sim\n",
    "\n",
    "            val_sims_base += F.cosine_similarity(clip_target_norm,clip_voxels_norm).mean().item()\n",
    "\n",
    "            labels = torch.arange(len(clip_target)).to(device)\n",
    "            val_fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1)\n",
    "            val_bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1)\n",
    "\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "    if (not save_at_end and ckpt_saving) or (save_at_end and epoch == num_epochs - 1):\n",
    "        # save best model\n",
    "        val_loss = np.mean(val_losses[-(val_i+1):])\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_ckpt('best')\n",
    "        else:\n",
    "            print(f'not best - val_loss: {val_loss:.3f}, best_val_loss: {best_val_loss:.3f}')\n",
    "\n",
    "    if utils.is_interactive():\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    logs = {\"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "        \"val/loss\": np.mean(val_losses[-(val_i+1):]),\n",
    "        \"train/lr\": lrs[-1],\n",
    "        \"train/num_steps\": len(losses),\n",
    "        \"val/num_steps\": len(val_losses),\n",
    "        \"train/cosine_sim_base\": sims_base / (train_i + 1),\n",
    "        \"val/cosine_sim_base\": val_sims_base / (val_i + 1),\n",
    "        \"train/fwd_pct_correct\": fwd_percent_correct / (train_i + 1),\n",
    "        \"train/bwd_pct_correct\": bwd_percent_correct / (train_i + 1),\n",
    "        \"val/val_fwd_pct_correct\": val_fwd_percent_correct / (val_i + 1),\n",
    "        \"val/val_bwd_pct_correct\": val_bwd_percent_correct / (val_i + 1),\n",
    "        \"train/nce_losses\": np.mean(nce_losses[-(train_i+1):]),\n",
    "        \"val/nce_losses\": np.mean(val_nce_losses[-(val_i+1):]),\n",
    "        \"train/mse_losses\": np.mean(mse_losses[-(train_i+1):]),\n",
    "        \"val/mse_losses\": np.mean(val_mse_losses[-(val_i+1):]),\n",
    "        \"train/sim_losses\": np.mean(sim_losses[-(train_i+1):]),\n",
    "        \"val/sim_losses\": np.mean(val_sim_losses[-(val_i+1):]),\n",
    "        \"train/temp\": voxel2clip.module.temp.item() if distributed else voxel2clip.temp.item()}\n",
    "    progress_bar.set_postfix(**logs)\n",
    "\n",
    "    # Save model checkpoint and reconstruct\n",
    "    save_ckpt(f'last')\n",
    "    if epoch % ckpt_interval == 0:\n",
    "        save_ckpt(f'last_backup')\n",
    "        if n_samples_save > 0 and local_rank==0:\n",
    "            print('reconstructing...')\n",
    "            with torch.no_grad():\n",
    "                grid, _, _, _, _ = utils.reconstruct_from_clip(\n",
    "                    val_image0, val_voxel0,\n",
    "                    clip_extractor, vd_pipe.image_unet, vd_pipe.vae, vd_pipe.scheduler,\n",
    "                    voxel2clip_img = None, \n",
    "                    diffusion_priors = diffusion_prior.module if distributed else diffusion_prior,\n",
    "                    text_token = None,\n",
    "                    img_lowlevel = None,\n",
    "                    num_inference_steps = num_inference_steps,\n",
    "                    n_samples_save = n_samples_save,\n",
    "                    recons_per_clip = 0,\n",
    "                    recons_per_brain = 1,\n",
    "                    guidance_scale = 3.5,\n",
    "                    img2img_strength = 1, # 0=fully rely on img_lowlevel, 1=not doing img2img\n",
    "                    timesteps_prior = timesteps,\n",
    "                    seed = seed,\n",
    "                    retrieve = False,\n",
    "                    plotting = True,\n",
    "                )\n",
    "            # grid.savefig(os.path.join(outdir, f'samples-val-epoch{epoch:03d}.png'))\n",
    "            if wandb_log:\n",
    "                logs[f\"val/recons\"] = wandb.Image(grid, caption=f\"epoch{epoch:03d}\")\n",
    "                plt.close()\n",
    "            else:\n",
    "                plt.show()\n",
    "            print('umap plotting...')\n",
    "            combined = np.concatenate((clip_target.flatten(1).detach().cpu().numpy(),\n",
    "                                       clip_voxels.flatten(1).detach().cpu().numpy()),axis=0)\n",
    "            reducer = umap.UMAP(random_state=42)\n",
    "            embedding = reducer.fit_transform(combined)\n",
    "\n",
    "            colors=np.array([[0,0,1,.5] for i in range(len(clip_target_norm))])\n",
    "            colors=np.concatenate((colors, np.array([[0,1,0,.5] for i in range(len(clip_voxels_norm))])))\n",
    "\n",
    "            fig = plt.figure(figsize=(5,5))\n",
    "            plt.scatter(\n",
    "                embedding[:, 0],\n",
    "                embedding[:, 1],\n",
    "                c=colors)\n",
    "            # plt.savefig(os.path.join(outdir, f'umap-val-epoch{epoch:03d}.png'))\n",
    "            if wandb_log:\n",
    "                logs[f\"val/umap\"] = wandb.Image(fig, caption=f\"epoch{epoch:03d}\")\n",
    "                plt.close()\n",
    "            else:\n",
    "                plt.show()\n",
    "    if wandb_log and local_rank==0: # save last ckpt so you can resume from it if need be\n",
    "        wandb.save(os.path.abspath(outdir)+'/last.pth', base_path=os.path.abspath(outdir))\n",
    "        if epoch % ckpt_interval == 0:\n",
    "            wandb.save(os.path.abspath(outdir)+'/last_backup.pth', base_path=os.path.abspath(outdir))\n",
    "\n",
    "    if wandb_log and local_rank==0:\n",
    "        wandb.log(logs)\n",
    "        \n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "if wandb_log and local_rank==0:\n",
    "    wandb.finish()\n",
    "\n",
    "print(\"\\n===Finished!===\\n\")\n",
    "if not utils.is_interactive():\n",
    "    sys.exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "62aae01ef0cf7b6af841ab1c8ce59175c4332e693ab3d00bc32ceffb78a35376"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
