{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26ca08a0-e0e9-4cf0-b95c-2c93376f2eb7",
   "metadata": {},
   "source": [
    "This notebook takes brain voxels and maps them to CLIP-space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4d95fac-ac1d-473c-ab96-650f76e6aaf5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Brain_to_CLIP.ipynb to python\n",
      "[NbConvertApp] Writing 35367 bytes to Brain_to_CLIP.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # convert this notebook to .py such that you can then run it via slurm with \"sbatch *.slurm\"\n",
    "# from subprocess import call\n",
    "# command = \"jupyter nbconvert Brain_to_CLIP.ipynb --to python\"\n",
    "# call(command,shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f0f4f3",
   "metadata": {},
   "source": [
    "# Import packages & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bad764b-45c1-45ce-a716-8d055e09821a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 16:02:16.967184: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-20 16:02:21.324578: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "distributed = False num_devices = 1 local rank = 0 world size = 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from info_nce import InfoNCE\n",
    "from dalle2_pytorch import DiffusionPriorNetwork\n",
    "import kornia\n",
    "from kornia.augmentation.container import AugmentationSequential\n",
    "import umap\n",
    "\n",
    "import torch.distributed as dist\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# uses tf32 data type which is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# Multi-GPU config #\n",
    "accelerator = Accelerator()\n",
    "print = accelerator.print # only print if local_rank=0\n",
    "\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0: num_devices = 1\n",
    "num_workers = num_devices\n",
    "\n",
    "print(accelerator.state)\n",
    "local_rank = accelerator.state.local_process_index\n",
    "world_size = accelerator.state.num_processes\n",
    "if num_devices<=1 and world_size<=1:\n",
    "    distributed=False\n",
    "else:\n",
    "    distributed=True\n",
    "print(\"distributed =\",distributed,\"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size)\n",
    "\n",
    "# custom models and functions #\n",
    "import utils\n",
    "from utils import torch_to_matplotlib, torch_to_Image\n",
    "from models import BrainNetwork, BrainDiffusionPrior\n",
    "# from model3d import SimpleVoxel3dConvEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018b82b-c054-4463-9527-4b0c2a75bda6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b61fec7-72a0-4b67-86da-1375f1d9fbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--model_name=testing', '--modality=image', '--clip_variant=ViT-L/14', '--batch_size=300', '--with_mse', '--versatile', '--n_samples_save=1', '--mse_amount=.1', '--max_lr=3e-4', '--mixup_pct=.33', '--num_epochs=400', '--no-norm_embs', '--ckpt_interval=5']\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    jupyter_args=[]\n",
    "    jupyter_args.append(\"--model_name=testing\")\n",
    "    jupyter_args.append(\"--modality=image\")\n",
    "    jupyter_args.append(\"--clip_variant=ViT-L/14\")\n",
    "    jupyter_args.append(\"--batch_size=300\")\n",
    "    jupyter_args.append(\"--with_mse\")\n",
    "    jupyter_args.append(\"--versatile\")\n",
    "    jupyter_args.append(\"--n_samples_save=1\")\n",
    "    jupyter_args.append(\"--mse_amount=.1\")\n",
    "    jupyter_args.append(\"--max_lr=3e-4\")\n",
    "    jupyter_args.append(\"--mixup_pct=.33\")\n",
    "    jupyter_args.append(\"--num_epochs=400\")\n",
    "    jupyter_args.append(\"--no-norm_embs\")\n",
    "    # jupyter_args.append(\"--wandb_log\")\n",
    "    jupyter_args.append(\"--ckpt_interval=5\")\n",
    "    # jupyter_args.append(\"--resume_from_ckpt\")\n",
    "    print(jupyter_args)\n",
    "    \n",
    "    from IPython.display import clear_output\n",
    "    \n",
    "    %load_ext autoreload\n",
    "    %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2028bdf0-2f41-46d9-b6e7-86b870dbf16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--modality\", type=str, default=\"image\", choices=[\"image\", \"text\"],\n",
    "    help=\"image or text\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=300,\n",
    "    help=\"Our maximum for A100 was 300 for 1dim voxels and 128 for 3dim voxels\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--clip_variant\",type=str,default=\"ViT-L/14\",choices=[\"RN50\", \"ViT-L/14\", \"ViT-B/32\", \"ViT-H-14\", \"RN50x64\"],\n",
    "    help='clip / openclip variant',\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--outdir\",type=str,default=None,\n",
    "    help=\"output directory for logs and checkpoints\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_log\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to log to wandb\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--resume_from_ckpt\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"if not using wandb and want to resume from a ckpt\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_project\",type=str,default=\"stability\",\n",
    "    help=\"wandb project name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mixup_pct\",type=float,default=.5,\n",
    "    help=\"proportion of way through training when to switch from InfoNCE to soft_clip_loss\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--voxel_dims\",type=int,default=1,choices=[1, 3],\n",
    "    help=\"1 for flattened input, 3 for 3d input\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_image_aug\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to use image augmentation (only used for modality=image)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epochs\",type=int,default=120,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler_type\",type=str,default='cycle',choices=['cycle','fixed'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_saving\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_interval\",type=int,default=5,\n",
    "    help=\"save backup ckpt and reconstruct every x epochs\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--save_at_end\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"if False, will save best.ckpt whenever epoch shows best validation score\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--with_mse\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"Add mse loss to the other losses\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cos_base\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"Add scaled cosine similarity to the other losses\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mse_mult\",type=int,default=1,\n",
    "    help=\"Multiplier for mse loss\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--versatile\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"Map to 257x768 versatile diffusion CLIP space, not including class token\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--norm_embs\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"Do norming (using cls token if VD) of CLIP embeddings\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mse_amount\",type=float,default=.5,\n",
    "    help=\"What percentage of to weight mse vs. soft_clip loss\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_lr\",type=float,default=3e-4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_samples_save\",type=int,default=0,\n",
    "    help=\"Number of reconstructions for monitoring progress, 0 will speed up training\",\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60cd7f2c-37fd-426b-a0c6-633e51bc4c4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if outdir is None:\n",
    "    outdir = os.path.abspath(f'../train_logs/{model_name}')\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir,exist_ok=True)\n",
    "if use_image_aug:\n",
    "    # img_augment = AugmentationSequential(\n",
    "    #     kornia.augmentation.RandomResizedCrop((224,224), (0.6,1), p=0.3),\n",
    "    #     kornia.augmentation.Resize((224, 224)),\n",
    "    #     kornia.augmentation.RandomHorizontalFlip(p=0.5),\n",
    "    #     kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1, p=0.3),\n",
    "    #     kornia.augmentation.RandomGrayscale(p=0.3),\n",
    "    #     data_keys=[\"input\"],\n",
    "    # )\n",
    "    img_augment = AugmentationSequential(\n",
    "        kornia.augmentation.RandomResizedCrop((240,240), (0.6,1), p=0.3),\n",
    "        kornia.augmentation.Resize((224, 224)),\n",
    "        kornia.augmentation.RandomGaussianBlur(kernel_size=(7,7), sigma=(5,5), p=0.3), #MedianBlur is better but computationally inefficient\n",
    "        # kornia.augmentation.RandomHorizontalFlip(p=0.5),\n",
    "        kornia.augmentation.ColorJiggle(brightness=0.2, contrast=0.2, saturation=0.3, hue=0., p=0.3),\n",
    "    )\n",
    "\n",
    "if modality=='text':\n",
    "    annots = np.load(\"/fsx/proj-medarc/fmri/natural-scenes-dataset/COCO_73k_annots_curated.npy\")\n",
    "    import logging\n",
    "    logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\n",
    "    from transformers import CLIPTextModelWithProjection, CLIPTokenizer\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "    text_encoder = CLIPTextModelWithProjection.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "    text_encoder.eval()\n",
    "    text_encoder.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d13c25-1369-4c49-81d4-83d713586096",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prep models and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7966d5a-c8a9-4461-808a-2f89bb00fe9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using cudnn.deterministic\n",
      "Pulling NSD webdataset data...\n",
      "Prepping train and validation dataloaders...\n",
      "Getting dataloaders...\n",
      "\n",
      "num_train 8859\n",
      "global_batch_size 300\n",
      "batch_size 300\n",
      "num_workers 1\n",
      "num_batches 29\n",
      "num_worker_batches 29\n",
      "cache_dir None\n",
      "\n",
      "num_val 982\n",
      "val_batch_size 300\n",
      "val_num_workers 1\n"
     ]
    }
   ],
   "source": [
    "# need non-deterministic CuDNN for conv3D to work\n",
    "utils.seed_everything(seed, cudnn_deterministic=False)\n",
    "\n",
    "print('Pulling NSD webdataset data...')\n",
    "\n",
    "train_url = \"{/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split/train/train_subj01_{0..17}.tar,/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split/val/val_subj01_0.tar}\"\n",
    "val_url = \"/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split/test/test_subj01_{0..1}.tar\"\n",
    "meta_url = \"/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split/metadata_subj01.json\"\n",
    "num_train = 8559 + 300\n",
    "num_val = 982\n",
    "\n",
    "# which to use for the voxels\n",
    "if voxel_dims == 1:\n",
    "    voxels_key = 'nsdgeneral.npy'\n",
    "elif voxel_dims == 3:\n",
    "    voxels_key = 'wholebrain_3d.npy'\n",
    "else:\n",
    "    raise Exception(f\"voxel_dims must be 1 or 3, not {voxel_dims}\")\n",
    "\n",
    "print('Prepping train and validation dataloaders...')\n",
    "train_dl, val_dl, num_train, num_val = utils.get_dataloaders(\n",
    "    batch_size,'images',\n",
    "    num_devices=num_devices,\n",
    "    num_workers=num_workers,\n",
    "    train_url=train_url,\n",
    "    val_url=val_url,\n",
    "    meta_url=meta_url,\n",
    "    num_train=num_train,\n",
    "    num_val=num_val,\n",
    "    val_batch_size=batch_size,\n",
    "    cache_dir=\"/tmp/wds-cache\",\n",
    "    seed=seed,\n",
    "    voxels_key=voxels_key,\n",
    "    to_tuple=[\"voxels\", \"images\", \"coco\"],\n",
    "    local_rank=local_rank,\n",
    ")\n",
    "\n",
    "if voxel_dims == 3:\n",
    "    import nibabel as nib\n",
    "    noise_ceils_path = '/fsx/proj-medarc/fmri/natural-scenes-dataset/temp_s3/nsddata_betas/ppdata/subj01/func1pt8mm/betas_fithrf_GLMdenoise_RR/ncsnr.nii.gz'\n",
    "    noise_ceils = nib.load(noise_ceils_path).get_fdata()\n",
    "    x_inc,y_inc,z_inc = np.where(noise_ceils > .5) # voxel.shape torch.Size([300, 3, 68, 64, 47])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e14d0482-dc42-43b9-9ce1-953c32f2c9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Clipper...\n",
      "Using versatile CLIP space\n",
      "ViT-L/14 cuda\n",
      "out_dim: 197376\n",
      "Creating voxel2clip...\n",
      "params of voxel2clip:\n",
      "param counts:\n",
      "940,225,281 total\n",
      "940,225,281 trainable\n",
      "Creating versatile diffusion reconstruction pipeline...\n",
      "\n",
      "Done with model preparations!\n"
     ]
    }
   ],
   "source": [
    "print('Creating Clipper...')\n",
    "    \n",
    "# Don't L2 norm the extracted CLIP embeddings since we want the prior \n",
    "# to learn un-normed embeddings for usage with the SD image variation pipeline.\n",
    "from models import Clipper\n",
    "if versatile:\n",
    "    print(\"Using versatile CLIP space\")\n",
    "    clip_extractor = Clipper(clip_variant, device=device, hidden_state=True, norm_embs=norm_embs)\n",
    "    out_dim = 257 * 768\n",
    "elif modality=='image':\n",
    "    clip_extractor = Clipper(clip_variant, device=device, hidden_state=True, norm_embs=norm_embs)\n",
    "    out_dim = 768\n",
    "else:\n",
    "    annots = np.load(\"/fsx/proj-medarc/fmri/natural-scenes-dataset/COCO_73k_annots_curated.npy\")\n",
    "    out_dim = 77 * 768\n",
    "print(\"out_dim:\",out_dim)\n",
    "\n",
    "print('Creating voxel2clip...')\n",
    "\n",
    "if voxel_dims == 1: # 1D data\n",
    "    voxel2clip_kwargs = dict(out_dim=out_dim)\n",
    "    voxel2clip = BrainNetwork(**voxel2clip_kwargs)\n",
    "elif voxel_dims == 3: # 3D data\n",
    "    if modality=='text':\n",
    "        voxel2clip_kwargs = dict(\n",
    "            out_dim=77*outdim,\n",
    "            dims=voxel.shape[2:],\n",
    "            channels=[64, 128, 256, 128],\n",
    "            strides=[1, 2, 3, 3],\n",
    "            padding=[1, 1, 1, 1],\n",
    "            dilation=[1, 1, 1, 1],\n",
    "            kernel=[3, 3, 3, 3],\n",
    "        )\n",
    "    else:\n",
    "        voxel2clip_kwargs = dict(\n",
    "            out_dim=out_dim,\n",
    "            dims=voxel.shape[2:],\n",
    "            channels=[64, 128, 256, 128],\n",
    "            strides=[1, 2, 3, 3],\n",
    "            padding=[1, 1, 1, 1],\n",
    "            dilation=[1, 1, 1, 1],\n",
    "            kernel=[3, 3, 3, 3],\n",
    "        )\n",
    "    voxel2clip = SimpleVoxel3dConvEncoder(**voxel2clip_kwargs)  \n",
    "\n",
    "# learnable logit scaling\n",
    "voxel2clip.temp = nn.Parameter(torch.tensor(.006))\n",
    "    \n",
    "print(\"params of voxel2clip:\")\n",
    "if local_rank==0:\n",
    "    utils.count_params(voxel2clip)\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in voxel2clip.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in voxel2clip.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=3e-4) # lr doesnt get used if lr_scheduler_type='cycle'\n",
    "\n",
    "if lr_scheduler_type == 'fixed':\n",
    "    lr_scheduler = None\n",
    "elif lr_scheduler_type == 'cycle':\n",
    "    global_batch_size = batch_size * num_devices\n",
    "    total_steps=num_epochs*(num_train//global_batch_size)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=max_lr,\n",
    "        total_steps=total_steps,\n",
    "        final_div_factor=1000,\n",
    "        last_epoch=-1, pct_start=2/num_epochs\n",
    "    )\n",
    "    \n",
    "if n_samples_save > 0:\n",
    "    if local_rank == 0: print('Creating versatile diffusion reconstruction pipeline...')\n",
    "    from diffusers import AutoencoderKL, PNDMScheduler, UNet2DConditionModel, UniPCMultistepScheduler, Transformer2DModel\n",
    "    from diffusers.models import DualTransformer2DModel\n",
    "    from diffusers.pipelines.versatile_diffusion.modeling_text_unet import UNetFlatConditionModel\n",
    "\n",
    "    with torch.no_grad():\n",
    "        sd_cache_dir = '/fsx/proj-medarc/fmri/cache/models--shi-labs--versatile-diffusion/snapshots/2926f8e11ea526b562cd592b099fcf9c2985d0b7'\n",
    "        if not os.path.isdir(sd_cache_dir): # download from huggingface if not already downloaded / cached\n",
    "            from diffusers import VersatileDiffusionPipeline\n",
    "            print(\"Downloading from huggingface...\")\n",
    "            sd_pipe = VersatileDiffusionPipeline.from_pretrained(\"shi-labs/versatile-diffusion\")\n",
    "            sd_cache_dir = \"shi-labs/versatile-diffusion\"\n",
    "        unet = UNet2DConditionModel.from_pretrained(sd_cache_dir,subfolder=\"image_unet\").to(device)\n",
    "        unet.eval() # dont want to train model\n",
    "        unet.requires_grad_(False) # dont need to calculate gradients\n",
    "\n",
    "        vae = AutoencoderKL.from_pretrained(sd_cache_dir,subfolder=\"vae\").to(device)\n",
    "        vae.eval()\n",
    "        vae.requires_grad_(False)\n",
    "\n",
    "        text_unet = UNetFlatConditionModel.from_pretrained(sd_cache_dir,subfolder=\"text_unet\").to(device)\n",
    "        text_unet.eval() # dont want to train model\n",
    "        text_unet.requires_grad_(False) # dont need to calculate gradients\n",
    "\n",
    "        noise_scheduler = PNDMScheduler.from_pretrained(sd_cache_dir, subfolder=\"scheduler\")\n",
    "        noise_scheduler = UniPCMultistepScheduler.from_config(noise_scheduler.config)\n",
    "        num_inference_steps = 20\n",
    "\n",
    "        # convert to dual attention         \n",
    "        for name, module in unet.named_modules():\n",
    "            if isinstance(module, Transformer2DModel):\n",
    "                parent_name, index = name.rsplit(\".\", 1)\n",
    "                index = int(index)\n",
    "\n",
    "                image_transformer = unet.get_submodule(parent_name)[index]\n",
    "                text_transformer = text_unet.get_submodule(parent_name)[index]\n",
    "\n",
    "                config = image_transformer.config\n",
    "                dual_transformer = DualTransformer2DModel(\n",
    "                    num_attention_heads=config.num_attention_heads,\n",
    "                    attention_head_dim=config.attention_head_dim,\n",
    "                    in_channels=config.in_channels,\n",
    "                    num_layers=config.num_layers,\n",
    "                    dropout=config.dropout,\n",
    "                    norm_num_groups=config.norm_num_groups,\n",
    "                    cross_attention_dim=config.cross_attention_dim,\n",
    "                    attention_bias=config.attention_bias,\n",
    "                    sample_size=config.sample_size,\n",
    "                    num_vector_embeds=config.num_vector_embeds,\n",
    "                    activation_fn=config.activation_fn,\n",
    "                    num_embeds_ada_norm=config.num_embeds_ada_norm,\n",
    "                )\n",
    "                dual_transformer.transformers[0] = image_transformer\n",
    "                dual_transformer.transformers[1] = text_transformer\n",
    "\n",
    "                unet.get_submodule(parent_name)[index] = dual_transformer\n",
    "                unet.register_to_config(dual_cross_attention=True)\n",
    "\n",
    "        # import logging\n",
    "        # logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\n",
    "        # from transformers import CLIPTextModelWithProjection, CLIPTokenizer\n",
    "        # tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "        # text_encoder = CLIPTextModelWithProjection.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "        # text_encoder.eval()\n",
    "        # text_encoder.requires_grad_(False)\n",
    "\n",
    "        # annots = np.load(\"/fsx/proj-medarc/fmri/natural-scenes-dataset/COCO_73k_annots_curated.npy\")\n",
    "\n",
    "        text_image_ratio = .0 # .5 means equally weight text and image, 0 means only use image\n",
    "        condition_types = (\"text\", \"image\")\n",
    "        for name, module in unet.named_modules():\n",
    "            if isinstance(module, DualTransformer2DModel):\n",
    "                module.mix_ratio = text_image_ratio\n",
    "                for i, type in enumerate(condition_types):\n",
    "                    if type == \"text\":\n",
    "                        module.condition_lengths[i] = 77\n",
    "                        module.transformer_index_for_condition[i] = 1  # use the second (text) transformer\n",
    "                    else:\n",
    "                        module.condition_lengths[i] = 257\n",
    "                        module.transformer_index_for_condition[i] = 0  # use the first (image) transformer\n",
    "    \n",
    "def save_ckpt(tag):\n",
    "    ckpt_path = outdir+f'/{tag}.pth'\n",
    "    print(f'saving {ckpt_path}',flush=True)\n",
    "    unwrapped_model = accelerator.unwrap_model(voxel2clip)\n",
    "    try:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': unwrapped_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            'train_losses': losses,\n",
    "            'val_losses': val_losses,\n",
    "            'lrs': lrs,\n",
    "            }, ckpt_path)\n",
    "    except:\n",
    "        print(\"Couldn't save... moving on to prevent crashing.\")\n",
    "    del unwrapped_model\n",
    "        \n",
    "print(\"\\nDone with model preparations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983f458b-35b8-49f2-b6db-80296cece730",
   "metadata": {},
   "source": [
    "# Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a25a662-daa8-4de9-9233-8364800fcb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb stability run scaling_testz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpaulscotti\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb_config:\n",
      " {'model_name': 'scaling_testz', 'modality': 'image', 'voxel_dims': 1, 'clip_variant': 'ViT-L/14', 'batch_size': 300, 'num_epochs': 300, 'use_image_aug': True, 'max_lr': 0.0003, 'lr_scheduler_type': 'cycle', 'mixup_pct': 0.33, 'mse_amount': 0.1, 'num_train': 8859, 'num_val': 982, 'seed': 42, 'distributed': False, 'num_devices': 1, 'world_size': 1, 'train_url': '{/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split/train/train_subj01_{0..17}.tar,/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split/val/val_subj01_0.tar}', 'val_url': '/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split/test/test_subj01_{0..1}.tar'}\n",
      "wandb_id: scaling_testz\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/fsx/proj-medarc/fmri/paulscotti/fMRI-reconstruction-NSD/src/wandb/run-20230419_104700-scaling_testz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://stability.wandb.io/paulscotti/stability/runs/scaling_testz\" target=\"_blank\">scaling_testz</a></strong> to <a href=\"https://stability.wandb.io/paulscotti/stability\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# params for wandb\n",
    "if local_rank==0 and wandb_log:\n",
    "    import wandb\n",
    "    \n",
    "    wandb_project = 'stability'\n",
    "    wandb_run = model_name\n",
    "    wandb_notes = ''\n",
    "    \n",
    "    print(f\"wandb {wandb_project} run {wandb_run}\")\n",
    "    wandb.login(host='https://stability.wandb.io')#, relogin=True)\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"modality\": modality,\n",
    "      \"voxel_dims\": voxel_dims,\n",
    "      \"clip_variant\": clip_variant,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"use_image_aug\": use_image_aug,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"lr_scheduler_type\": lr_scheduler_type,\n",
    "      \"mixup_pct\": mixup_pct,\n",
    "      \"mse_amount\": mse_amount,\n",
    "      \"num_train\": num_train,\n",
    "      \"num_val\": num_val,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "      \"train_url\": train_url,\n",
    "      \"val_url\": val_url,\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    if True: # wandb_auto_resume\n",
    "        print(\"wandb_id:\",model_name)\n",
    "        wandb.init(\n",
    "            id = model_name,\n",
    "            project=wandb_project,\n",
    "            name=wandb_run,\n",
    "            config=wandb_config,\n",
    "            notes=wandb_notes,\n",
    "            resume=\"allow\",\n",
    "        )\n",
    "    else:\n",
    "        wandb.init(\n",
    "            project=wandb_project,\n",
    "            name=wandb_run,\n",
    "            config=wandb_config,\n",
    "            notes=wandb_notes,\n",
    "        )\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5690151-2131-4918-b750-e869cbd1a8a8",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12de6387-6e18-4e4b-b5ce-a847d625330a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using cudnn.deterministic\n"
     ]
    }
   ],
   "source": [
    "# need non-deterministic CuDNN for conv3D to work\n",
    "utils.seed_everything(seed, cudnn_deterministic=False)\n",
    "\n",
    "epoch = 0\n",
    "losses, val_losses, lrs = [], [], []\n",
    "nce_losses, val_nce_losses = [], []\n",
    "mse_losses, val_mse_losses =  [], []\n",
    "sim_losses, val_sim_losses = [], []\n",
    "best_val_loss = 1e9\n",
    "\n",
    "mse = nn.MSELoss()\n",
    "val_voxel0 = val_image0 = None\n",
    "\n",
    "# Optionally resume from checkpoint #\n",
    "if resume_from_ckpt:\n",
    "    print(\"\\n---resuming from last.pth ckpt---\\n\")\n",
    "    try:\n",
    "        # checkpoint = torch.load('/fsx/proj-medarc/fmri/paulscotti/fMRI-reconstruction-NSD/train_logs/mse_normL/last_backup.pth', map_location='cpu')\n",
    "        checkpoint = torch.load(outdir+'/last.pth', map_location='cpu')\n",
    "    except:\n",
    "        print('last.pth failed... trying last_backup.pth')\n",
    "        checkpoint = torch.load(outdir+'/last_backup.pth', map_location='cpu')\n",
    "    epoch = checkpoint['epoch']\n",
    "    print(\"Epoch\",epoch)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    voxel2clip.load_state_dict(checkpoint['model_state_dict'])\n",
    "    del checkpoint\n",
    "elif wandb_log:\n",
    "    if wandb.run.resumed:\n",
    "        print(\"\\n---resuming from last.pth ckpt---\\n\")\n",
    "        try:\n",
    "            checkpoint = torch.load(outdir+'/last.pth', map_location='cpu')\n",
    "        except:\n",
    "            print('last.pth failed... trying last_backup.pth')\n",
    "            checkpoint = torch.load(outdir+'/last_backup.pth', map_location='cpu')\n",
    "        epoch = checkpoint['epoch']\n",
    "        print(\"Epoch\",epoch)\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "        voxel2clip.load_state_dict(checkpoint['model_state_dict'])\n",
    "        del checkpoint\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d5ae0ca-02c2-43d1-b20a-d1a33801873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel2clip, optimizer, train_dl, val_dl, lr_scheduler = accelerator.prepare(\n",
    "voxel2clip, optimizer, train_dl, val_dl, lr_scheduler\n",
    ")\n",
    "\n",
    "clip_target0 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a3368c-e6ce-49cc-b970-ee3dba12dfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{model_name} starting with epoch {epoch} / {num_epochs}\")\n",
    "progress_bar = tqdm(range(epoch,num_epochs), ncols=1200, disable=(local_rank!=0))\n",
    "for epoch in progress_bar:\n",
    "    voxel2clip.train()\n",
    "\n",
    "    sims_base = 0.\n",
    "    val_sims_base = 0.\n",
    "    fwd_percent_correct = 0.\n",
    "    bwd_percent_correct = 0.\n",
    "    val_fwd_percent_correct = 0.\n",
    "    val_bwd_percent_correct = 0.\n",
    "\n",
    "    for train_i, (voxel, image, coco) in enumerate(train_dl):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        repeat_index = train_i % 3\n",
    "\n",
    "        image = image.float()\n",
    "        voxel = voxel.float()[:,repeat_index].float()\n",
    "        \n",
    "        if use_image_aug:\n",
    "            image = img_augment(image)\n",
    "        \n",
    "        if voxel_dims == 3:\n",
    "            voxel = voxel[:,np.unique(x_inc),:,:]\n",
    "            voxel = voxel[:,:,np.unique(y_inc),:]\n",
    "            voxel = voxel[:,:,:,np.unique(z_inc)]\n",
    "\n",
    "        if epoch < int(mixup_pct * num_epochs):\n",
    "            voxel, perm, betas, select = utils.mixco(voxel)\n",
    "\n",
    "        if modality=='text':\n",
    "            prompt = utils.select_annotations(annots[coco.cpu().numpy()], random=True).tolist()\n",
    "            text_inputs = tokenizer(\n",
    "                        prompt,\n",
    "                        padding=\"max_length\",\n",
    "                        max_length=tokenizer.model_max_length,\n",
    "                        truncation=True,\n",
    "                        return_tensors=\"pt\",\n",
    "                    )\n",
    "            text_input_ids = text_inputs.input_ids\n",
    "            if hasattr(text_encoder.config, \"use_attention_mask\") and text_encoder.config.use_attention_mask:\n",
    "                attention_mask = text_inputs.attention_mask.to(device)\n",
    "            else:\n",
    "                attention_mask = None\n",
    "            prompt_embeds = text_encoder(\n",
    "                text_input_ids.to(device),\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            embeds = text_encoder.text_projection(prompt_embeds.last_hidden_state)\n",
    "            embeds_pooled = prompt_embeds.text_embeds\n",
    "            # clip_target = embeds / torch.norm(embeds_pooled.unsqueeze(1), dim=-1, keepdim=True)\n",
    "        else:\n",
    "            clip_target = clip_extractor.embed_image(image).float()\n",
    "            # clip_target = clip_target / torch.norm(clip_target[:, 0], dim=-1).reshape(-1, 1, 1)\n",
    "            \n",
    "        if not versatile and modality=='image':\n",
    "            clip_target = clip_target[:,:1]\n",
    "        clip_target = clip_target.view(len(clip_target),-1).to(voxel.dtype)\n",
    "        \n",
    "        clip_voxels = voxel2clip(voxel)\n",
    "        \n",
    "        clip_voxels_norm = nn.functional.normalize(clip_voxels, dim=-1)\n",
    "        clip_target_norm = nn.functional.normalize(clip_target, dim=-1)\n",
    "        \n",
    "        if epoch < int(mixup_pct * num_epochs):\n",
    "            loss = utils.mixco_nce(\n",
    "                clip_voxels_norm,\n",
    "                clip_target_norm,\n",
    "                temp=voxel2clip.temp, perm=perm, betas=betas, select=select,\n",
    "                distributed=distributed, accelerator=accelerator, local_rank=local_rank)\n",
    "        else:\n",
    "            loss = utils.soft_clip_loss(\n",
    "                clip_voxels_norm,\n",
    "                clip_target_norm,\n",
    "                temp=voxel2clip.temp,\n",
    "                distributed=distributed, accelerator=accelerator)\n",
    "        nce_losses.append(loss.item())\n",
    "        \n",
    "        if epoch < int(mixup_pct * num_epochs):\n",
    "            mseloss = mse(clip_voxels[~select], clip_target[~select])\n",
    "        else:\n",
    "            mseloss = mse(clip_voxels, clip_target)\n",
    "        mseloss *= 3000\n",
    "        if with_mse:\n",
    "            loss += mseloss   \n",
    "        mse_losses.append(mseloss.item())\n",
    "        utils.check_loss(loss)\n",
    "        \n",
    "        sim = 1-F.cosine_similarity(clip_target_norm,clip_voxels_norm).mean()\n",
    "        sim *= 8\n",
    "        if cos_base:\n",
    "            loss += sim\n",
    "        sim_losses.append(sim.item())\n",
    "        utils.check_loss(loss)\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        lrs.append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        # gather batches across multi-gpu if there's multiple\n",
    "        clip_voxel_gather = accelerator.gather(clip_voxels_norm.contiguous())\n",
    "        clip_target_gather = accelerator.gather(clip_target_norm.contiguous())\n",
    "        \n",
    "        sims_base += F.cosine_similarity(clip_target_gather,clip_voxel_gather).mean().item()\n",
    "        \n",
    "        # forward and backward top 1 accuracy        \n",
    "        labels = torch.arange(len(clip_target_gather)).to(device) \n",
    "        fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxel_gather,clip_target_gather), labels, k=1)\n",
    "        bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_gather, clip_voxel_gather), labels, k=1)\n",
    "        \n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "        if lr_scheduler_type is not None:\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "    voxel2clip.eval()\n",
    "    for val_i, (voxel, image, coco) in enumerate(val_dl): \n",
    "        with torch.no_grad():\n",
    "            # repeat_index = val_i % 3\n",
    "\n",
    "            image = image.float()\n",
    "            # voxel = voxel[:,repeat_index].float()\n",
    "            voxel = torch.mean(voxel,axis=1).float().to(device)\n",
    "\n",
    "            if voxel_dims == 3:\n",
    "                voxel = voxel[:,np.unique(x_inc),:,:]\n",
    "                voxel = voxel[:,:,np.unique(y_inc),:]\n",
    "                voxel = voxel[:,:,:,np.unique(z_inc)]\n",
    "\n",
    "            if val_image0 is None:\n",
    "                val_image0 = image.detach().clone()\n",
    "                val_voxel0 = voxel.detach().clone()\n",
    "\n",
    "            if modality=='text':\n",
    "                prompt = utils.select_annotations(annots[coco.cpu().numpy()], random=True).tolist()\n",
    "                text_inputs = tokenizer(\n",
    "                            prompt,\n",
    "                            padding=\"max_length\",\n",
    "                            max_length=tokenizer.model_max_length,\n",
    "                            truncation=True,\n",
    "                            return_tensors=\"pt\",\n",
    "                        )\n",
    "                text_input_ids = text_inputs.input_ids\n",
    "                if hasattr(text_encoder.config, \"use_attention_mask\") and text_encoder.config.use_attention_mask:\n",
    "                    attention_mask = text_inputs.attention_mask.to(device)\n",
    "                else:\n",
    "                    attention_mask = None\n",
    "                prompt_embeds = text_encoder(\n",
    "                    text_input_ids.to(device),\n",
    "                    attention_mask=attention_mask,\n",
    "                )\n",
    "                embeds = text_encoder.text_projection(prompt_embeds.last_hidden_state)\n",
    "                embeds_pooled = prompt_embeds.text_embeds\n",
    "                # clip_target = embeds / torch.norm(embeds_pooled.unsqueeze(1), dim=-1, keepdim=True)\n",
    "            else:\n",
    "                clip_target = clip_extractor.embed_image(image).float()\n",
    "                # clip_target = clip_target / torch.norm(clip_target[:, 0], dim=-1).reshape(-1, 1, 1)\n",
    "\n",
    "            if not versatile and modality=='image':\n",
    "                clip_target = clip_target[:,:1]\n",
    "            clip_target = clip_target.view(len(clip_target),-1).to(voxel.dtype)\n",
    "            \n",
    "            clip_voxels = voxel2clip(voxel)\n",
    "\n",
    "            clip_voxels_norm = nn.functional.normalize(clip_voxels, dim=-1)\n",
    "            clip_target_norm = nn.functional.normalize(clip_target, dim=-1)\n",
    "\n",
    "            if epoch < int(mixup_pct * num_epochs):\n",
    "                val_loss = utils.mixco_nce(\n",
    "                    clip_voxels_norm,\n",
    "                    clip_target_norm,\n",
    "                    temp=voxel2clip.temp, perm=perm, betas=betas, select=select,\n",
    "                    distributed=distributed, accelerator=accelerator, local_rank=local_rank)\n",
    "            else:\n",
    "                val_loss = utils.soft_clip_loss(\n",
    "                    clip_voxels_norm,\n",
    "                    clip_target_norm,\n",
    "                    temp=voxel2clip.temp,\n",
    "                    distributed=distributed, accelerator=accelerator)\n",
    "            val_nce_losses.append(val_loss.item())\n",
    "\n",
    "            if epoch < int(mixup_pct * num_epochs):\n",
    "                val_mseloss = mse(clip_voxels[~select], clip_target[~select])\n",
    "            else:\n",
    "                val_mseloss = mse(clip_voxels,clip_target)\n",
    "            val_mseloss *= 3000\n",
    "            if with_mse:\n",
    "                val_loss += val_mseloss\n",
    "            val_mse_losses.append(val_mseloss.item())\n",
    "            utils.check_loss(val_loss)\n",
    "            \n",
    "            val_sim = 1-F.cosine_similarity(clip_target_norm,clip_voxels_norm).mean()\n",
    "            val_sim *= 8\n",
    "            if cos_base:\n",
    "                val_loss += val_sim\n",
    "            val_sim_losses.append(val_sim.item())\n",
    "            \n",
    "            clip_voxel_gather = accelerator.gather(clip_voxels_norm.contiguous())\n",
    "            clip_target_gather = accelerator.gather(clip_target_norm.contiguous())\n",
    "\n",
    "            val_sims_base += F.cosine_similarity(clip_target_gather,clip_voxel_gather).mean().item()\n",
    "\n",
    "            labels = torch.arange(len(clip_target)).to(device)\n",
    "            val_fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxel_gather,clip_target_gather), labels, k=1)\n",
    "            val_bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_gather, clip_voxel_gather), labels, k=1)\n",
    "            \n",
    "            val_losses.append(val_loss.item())\n",
    "            \n",
    "    if local_rank==0:        \n",
    "        if (not save_at_end and ckpt_saving) or (save_at_end and epoch == num_epochs - 1):\n",
    "            # save best model\n",
    "            val_loss = np.mean(val_losses[-(val_i+1):])\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                save_ckpt('best')\n",
    "            else:\n",
    "                print(f'not best - val_loss: {val_loss:.3f}, best_val_loss: {best_val_loss:.3f}')\n",
    "                \n",
    "        if utils.is_interactive():\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "        logs = {\"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "            \"val/loss\": np.mean(val_losses[-(val_i+1):]),\n",
    "            \"train/lr\": lrs[-1],\n",
    "            \"train/num_steps\": len(losses),\n",
    "            \"val/num_steps\": len(val_losses),\n",
    "            \"train/cosine_sim_base\": sims_base / (train_i + 1),\n",
    "            \"val/cosine_sim_base\": val_sims_base / (val_i + 1),\n",
    "            \"train/fwd_pct_correct\": fwd_percent_correct / (train_i + 1),\n",
    "            \"train/bwd_pct_correct\": bwd_percent_correct / (train_i + 1),\n",
    "            \"val/val_fwd_pct_correct\": val_fwd_percent_correct / (val_i + 1),\n",
    "            \"val/val_bwd_pct_correct\": val_bwd_percent_correct / (val_i + 1),\n",
    "            \"train/nce_losses\": np.mean(nce_losses[-(train_i+1):]),\n",
    "            \"val/nce_losses\": np.mean(val_nce_losses[-(val_i+1):]),\n",
    "            \"train/mse_losses\": np.mean(mse_losses[-(train_i+1):]),\n",
    "            \"val/mse_losses\": np.mean(val_mse_losses[-(val_i+1):]),\n",
    "            \"train/sim_losses\": np.mean(sim_losses[-(train_i+1):]),\n",
    "            \"val/sim_losses\": np.mean(val_sim_losses[-(val_i+1):]),\n",
    "            \"train/temp\": voxel2clip.temp.item()}\n",
    "        progress_bar.set_postfix(**logs)\n",
    "\n",
    "        # Save model checkpoint and reconstruct\n",
    "        save_ckpt(f'last')\n",
    "        if epoch % ckpt_interval == 0:\n",
    "            save_ckpt(f'last_backup')\n",
    "            if n_samples_save > 0:\n",
    "                print('reconstructing...')\n",
    "                with torch.no_grad():\n",
    "                    grid, _, _, _, _ = utils.reconstruct_from_clip(\n",
    "                        val_image0, val_voxel0,\n",
    "                        clip_extractor, unet, vae, noise_scheduler,\n",
    "                        voxel2clip_img = voxel2clip, \n",
    "                        diffusion_priors = None,\n",
    "                        text_token = None,\n",
    "                        img_lowlevel = None,\n",
    "                        num_inference_steps = num_inference_steps,\n",
    "                        n_samples_save = n_samples_save,\n",
    "                        recons_per_clip = 0,\n",
    "                        recons_per_brain = 1,\n",
    "                        guidance_scale = 7.5,\n",
    "                        img2img_strength = 1, # 0=fully rely on img_lowlevel, 1=not doing img2img\n",
    "                        timesteps = 1000,\n",
    "                        seed = seed,\n",
    "                        retrieve = False,\n",
    "                        plotting = True,\n",
    "                    )\n",
    "                # grid.savefig(os.path.join(outdir, f'samples-val-epoch{epoch:03d}.png'))\n",
    "                if wandb_log:\n",
    "                    logs[f\"val/recons\"] = wandb.Image(grid, caption=f\"epoch{epoch:03d}\")\n",
    "                    plt.close()\n",
    "                else:\n",
    "                    plt.show()\n",
    "                print('umap plotting...')\n",
    "                combined = np.concatenate((clip_target.flatten(1).detach().cpu().numpy(),\n",
    "                                           clip_voxels.flatten(1).detach().cpu().numpy()),axis=0)\n",
    "                reducer = umap.UMAP(random_state=42)\n",
    "                embedding = reducer.fit_transform(combined)\n",
    "\n",
    "                colors=np.array([[0,0,1,.5] for i in range(len(clip_target_norm))])\n",
    "                colors=np.concatenate((colors, np.array([[0,1,0,.5] for i in range(len(clip_voxels_norm))])))\n",
    "\n",
    "                fig = plt.figure(figsize=(5,5))\n",
    "                plt.scatter(\n",
    "                    embedding[:, 0],\n",
    "                    embedding[:, 1],\n",
    "                    c=colors)\n",
    "                # plt.savefig(os.path.join(outdir, f'umap-val-epoch{epoch:03d}.png'))\n",
    "                if wandb_log:\n",
    "                    logs[f\"val/umap\"] = wandb.Image(fig, caption=f\"epoch{epoch:03d}\")\n",
    "                    plt.close()\n",
    "                else:\n",
    "                    plt.show()\n",
    "        if wandb_log: # save last ckpt so you can resume from it if need be\n",
    "            wandb.save(os.path.abspath(outdir)+'/last.pth', base_path=os.path.abspath(outdir))\n",
    "            if epoch % ckpt_interval == 0:\n",
    "                wandb.save(os.path.abspath(outdir)+'/last_backup.pth', base_path=os.path.abspath(outdir))\n",
    "\n",
    "        if wandb_log:\n",
    "            wandb.log(logs)\n",
    "            \n",
    "    if distributed:\n",
    "        dist.barrier()\n",
    "\n",
    "if wandb_log and local_rank==0:\n",
    "    wandb.finish()\n",
    "\n",
    "print(\"\\n===Finished!===\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d196df-7a6e-4c24-bf56-b2eca521d25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # for i in range(257):\n",
    "        #     pred_test_latent = clip_voxels[:,i]\n",
    "        #     std_norm_test_latent = (pred_test_latent - torch.mean(pred_test_latent,axis=0)) / torch.std(pred_test_latent,axis=0)\n",
    "        #     clip_voxels[:,i] = std_norm_test_latent * torch.std(clip_target[:,i],axis=0) + torch.mean(clip_target[:,i],axis=0)\n",
    "        # clip_voxels = clip_voxels.view(len(voxel),-1)\n",
    "        # clip_target = clip_target.view(len(voxel),-1).to(voxel.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a90c5bf-c8c0-4e67-b501-88658e19d44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "#         clip_weights = (clip_target_norm @ clip_target_norm.T).softmax(-1)\n",
    "#         squared_error = ((clip_target - clip_voxels) ** 2) \n",
    "#         mseloss = torch.sum(clip_weights @ squared_error) / len(clip_target)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "62aae01ef0cf7b6af841ab1c8ce59175c4332e693ab3d00bc32ceffb78a35376"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
