{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26ca08a0-e0e9-4cf0-b95c-2c93376f2eb7",
   "metadata": {},
   "source": [
    "This notebook takes brain voxels and maps them to CLIP-space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d95fac-ac1d-473c-ab96-650f76e6aaf5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Brain_to_CLIP_refine2.ipynb to python\n",
      "[NbConvertApp] Writing 37354 bytes to Brain_to_CLIP_refine2.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # convert this notebook to .py such that you can then run it via slurm with \"sbatch *.slurm\"\n",
    "# from subprocess import call\n",
    "# command = \"jupyter nbconvert Brain_to_CLIP_refine.ipynb --to python\"\n",
    "# call(command,shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f0f4f3",
   "metadata": {},
   "source": [
    "# Import packages & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bad764b-45c1-45ce-a716-8d055e09821a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "distributed = False num_devices = 1 local rank = 0 world size = 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from info_nce import InfoNCE\n",
    "from dalle2_pytorch import DiffusionPriorNetwork\n",
    "import kornia\n",
    "from kornia.augmentation.container import AugmentationSequential\n",
    "\n",
    "import torch.distributed as dist\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# uses tf32 data type which is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# Multi-GPU config #\n",
    "accelerator = Accelerator()\n",
    "print = accelerator.print # only print if local_rank=0\n",
    "\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0: num_devices = 1\n",
    "num_workers = num_devices\n",
    "\n",
    "print(accelerator.state)\n",
    "local_rank = accelerator.state.local_process_index\n",
    "world_size = accelerator.state.num_processes\n",
    "if num_devices<=1 and world_size<=1:\n",
    "    distributed=False\n",
    "else:\n",
    "    distributed=True\n",
    "print(\"distributed =\",distributed,\"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size)\n",
    "\n",
    "# custom models and functions #\n",
    "import utils\n",
    "from utils import torch_to_matplotlib, torch_to_Image\n",
    "from models import BrainNetwork#, BrainDiffusionPrior\n",
    "# from model3d import SimpleVoxel3dConvEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018b82b-c054-4463-9527-4b0c2a75bda6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b61fec7-72a0-4b67-86da-1375f1d9fbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--model_name=testing', '--modality=image', '--clip_variant=ViT-L/14', '--batch_size=256', '--num_epochs=100', '--with_mse', '--versatile', '--mixup_pct=0.', '--mse_pct=0.', '--max_lr=1e-4']\n"
     ]
    }
   ],
   "source": [
    "# can specify jupyter_args here for argparser to use if running this code interactively\n",
    "if utils.is_interactive():\n",
    "    jupyter_args=[]\n",
    "    jupyter_args.append(\"--model_name=testing\")\n",
    "    jupyter_args.append(\"--modality=image\")\n",
    "    jupyter_args.append(\"--clip_variant=ViT-L/14\")\n",
    "    jupyter_args.append(\"--batch_size=256\")\n",
    "    jupyter_args.append(\"--num_epochs=100\")\n",
    "    jupyter_args.append(\"--with_mse\")\n",
    "    jupyter_args.append(\"--versatile\")\n",
    "    jupyter_args.append(\"--mixup_pct=0.\")\n",
    "    jupyter_args.append(\"--mse_pct=0.\")\n",
    "    jupyter_args.append(\"--max_lr=1e-4\")\n",
    "    print(jupyter_args)\n",
    "    \n",
    "    %load_ext autoreload\n",
    "    %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2028bdf0-2f41-46d9-b6e7-86b870dbf16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--modality\", type=str, default=\"image\", choices=[\"image\", \"text\"],\n",
    "    help=\"image or text\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=300,\n",
    "    help=\"Our maximum for A100 was 300 for 1dim voxels and 128 for 3dim voxels\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--clip_variant\",type=str,default=\"ViT-L/14\",choices=[\"RN50\", \"ViT-L/14\", \"ViT-B/32\", \"ViT-H-14\", \"RN50x64\"],\n",
    "    help='clip / openclip variant',\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--outdir\",type=str,default=None,\n",
    "    help=\"output directory for logs and checkpoints\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_log\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to log to wandb\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--resume_from_ckpt\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"if not using wandb and want to resume from a ckpt\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_project\",type=str,default=\"stability\",\n",
    "    help=\"wandb project name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mixup_pct\",type=float,default=.5,\n",
    "    help=\"proportion of way through training when to switch from InfoNCE to soft_clip_loss\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--voxel_dims\",type=int,default=1,choices=[1, 3],\n",
    "    help=\"1 for flattened input, 3 for 3d input\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_image_aug\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to use image augmentation (only used for modality=image)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epochs\",type=int,default=120,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler\",type=str,default='cycle',choices=['cycle','fixed'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_saving\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_interval\",type=int,default=1,\n",
    "    help=\"save ckpt every x epochs\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--save_at_end\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"if False, will save best.ckpt whenever epoch shows best validation score\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--with_mse\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"Add mse loss to the other losses\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mse_mult\",type=int,default=1,\n",
    "    help=\"Multiplier for mse loss\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--text_token\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"Map to text token space instead of CLIP\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--versatile\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"Map to 257x768 versatile diffusion CLIP space\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mse_pct\",type=float,default=1.0,\n",
    "    help=\"What percentage of way through training to start adding mse loss\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--initial_lr\",type=float,default=3e-4,\n",
    "    help=\"lr if lr_scheduler is fixed\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_lr\",type=float,default=3e-4,\n",
    "    help=\"max_lr if lr_scheduler is onecycle\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--att\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"Map to 257x1024 instead of 257x768 versatile diffusion CLIP space\",\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60cd7f2c-37fd-426b-a0c6-633e51bc4c4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if outdir is None:\n",
    "    outdir = os.path.abspath(f'../train_logs/{model_name}')\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir,exist_ok=True)\n",
    "if use_image_aug:\n",
    "    train_augs = AugmentationSequential(\n",
    "        kornia.augmentation.RandomResizedCrop((224,224), (0.6,1), p=0.3),\n",
    "        kornia.augmentation.Resize((224, 224)),\n",
    "        kornia.augmentation.RandomHorizontalFlip(p=0.5),\n",
    "        kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1, p=0.3),\n",
    "        kornia.augmentation.RandomGrayscale(p=0.3),\n",
    "        data_keys=[\"input\"],\n",
    "    )\n",
    "else:\n",
    "    train_augs = None\n",
    "if modality=='text':\n",
    "    annots = np.load(\"/fsx/proj-medarc/fmri/natural-scenes-dataset/COCO_73k_annots_curated.npy\")\n",
    "if text_token and clip_variant!=\"ViT-H-14\":\n",
    "    from transformers import CLIPTextModel, CLIPTokenizer\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "    text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "    text_encoder.eval()\n",
    "    text_encoder.requires_grad_(False)\n",
    "elif text_token and clip_variant==\"ViT-H-14\":\n",
    "    from diffusers import StableUnCLIPImg2ImgPipeline\n",
    "    sd_cache_dir = '/fsx/proj-medarc/fmri/cache/models--stabilityai--stable-diffusion-2-1-unclip/snapshots/5eaf116f1b118d1756d5df9f578e8259befa95b7'\n",
    "    with torch.no_grad():\n",
    "        sd_pipe = StableUnCLIPImg2ImgPipeline.from_pretrained(\n",
    "            sd_cache_dir, torch_dtype=torch.float16,\n",
    "        )\n",
    "    tokenizer = sd_pipe.tokenizer\n",
    "    text_encoder = sd_pipe.text_encoder.to(device)\n",
    "    text_encoder.eval()\n",
    "    text_encoder.requires_grad_(False)\n",
    "    del sd_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d13c25-1369-4c49-81d4-83d713586096",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prep models and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "070d8d99-a7e8-4a3a-8bee-aa2ec1a735da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD NON-MSE CHECKPOINT\n",
    "checkpoint = torch.load('/fsx/proj-medarc/fmri/paulscotti/fMRI-reconstruction-NSD/train_logs/v2c_vers/last.pth',\n",
    "                       map_location='cpu')\n",
    "non_mse_ckpt = checkpoint['model_state_dict']\n",
    "del checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7966d5a-c8a9-4461-808a-2f89bb00fe9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using cudnn.deterministic\n",
      "Pulling NSD webdataset data...\n",
      "Prepping train and validation dataloaders...\n",
      "Getting dataloaders...\n",
      "\n",
      "num_train 8859\n",
      "global_batch_size 256\n",
      "batch_size 256\n",
      "num_workers 1\n",
      "num_batches 34\n",
      "num_worker_batches 34\n",
      "cache_dir None\n",
      "\n",
      "num_val 982\n",
      "val_batch_size 300\n",
      "val_num_workers 1\n"
     ]
    }
   ],
   "source": [
    "# need non-deterministic CuDNN for conv3D to work\n",
    "utils.seed_everything(seed, cudnn_deterministic=False)\n",
    "\n",
    "print('Pulling NSD webdataset data...')\n",
    "\n",
    "train_url = \"{/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split/train/train_subj01_{0..17}.tar,/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split/val/val_subj01_0.tar}\"\n",
    "val_url = \"/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split/test/test_subj01_{0..1}.tar\"\n",
    "meta_url = \"/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split/metadata_subj01.json\"\n",
    "num_train = 8559 + 300\n",
    "num_val = 982\n",
    "\n",
    "# which to use for the voxels\n",
    "if voxel_dims == 1:\n",
    "    voxels_key = 'nsdgeneral.npy'\n",
    "elif voxel_dims == 3:\n",
    "    voxels_key = 'wholebrain_3d.npy'\n",
    "else:\n",
    "    raise Exception(f\"voxel_dims must be 1 or 3, not {voxel_dims}\")\n",
    "\n",
    "print('Prepping train and validation dataloaders...')\n",
    "train_dl, val_dl, num_train, num_val = utils.get_dataloaders(\n",
    "    batch_size,'images',\n",
    "    num_devices=num_devices,\n",
    "    num_workers=num_workers,\n",
    "    train_url=train_url,\n",
    "    val_url=val_url,\n",
    "    meta_url=meta_url,\n",
    "    num_train=num_train,\n",
    "    num_val=num_val,\n",
    "    val_batch_size=300,\n",
    "    cache_dir=\"/tmp/wds-cache\",\n",
    "    seed=seed,\n",
    "    voxels_key=voxels_key,\n",
    "    to_tuple=[\"voxels\", \"images\", \"coco\"],\n",
    "    local_rank=local_rank,\n",
    ")\n",
    "\n",
    "if voxel_dims == 3:\n",
    "    import nibabel as nib\n",
    "    noise_ceils_path = '/fsx/proj-medarc/fmri/natural-scenes-dataset/temp_s3/nsddata_betas/ppdata/subj01/func1pt8mm/betas_fithrf_GLMdenoise_RR/ncsnr.nii.gz'\n",
    "    noise_ceils = nib.load(noise_ceils_path).get_fdata()\n",
    "    x_inc,y_inc,z_inc = np.where(noise_ceils > .5) # voxel.shape torch.Size([300, 3, 68, 64, 47])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e14d0482-dc42-43b9-9ce1-953c32f2c9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Clipper...\n",
      "Using versatile CLIP space\n",
      "ViT-L/14 cuda\n",
      "HIDDEN STATE AND REFINE BOTH TRUE\n",
      "out_dim: 263168\n",
      "Creating voxel2clip...\n",
      "params of voxel2clip:\n",
      "param counts:\n",
      "1,209,775,104 total\n",
      "1,209,775,104 trainable\n",
      "\n",
      "Done with model preparations!\n"
     ]
    }
   ],
   "source": [
    "print('Creating Clipper...')\n",
    "    \n",
    "# Don't L2 norm the extracted CLIP embeddings since we want the prior \n",
    "# to learn un-normed embeddings for usage with the SD image variation pipeline.\n",
    "if clip_variant == \"ViT-H-14\":\n",
    "    from models import OpenClipper\n",
    "    clip_extractor = OpenClipper(clip_variant, device=device, train_transforms=train_augs)\n",
    "    out_dim = 1024\n",
    "else:\n",
    "    from models import Clipper\n",
    "    if versatile:\n",
    "        print(\"Using versatile CLIP space\")\n",
    "        if not att:\n",
    "            clip_extractor = Clipper(clip_variant, device=device, hidden_state=True, refine=False, train_transforms=train_augs)\n",
    "            out_dim = 257 * 768\n",
    "        else:\n",
    "            clip_extractor = Clipper(clip_variant, device=device, hidden_state=True, refine=True, train_transforms=train_augs)\n",
    "            print(\"HIDDEN STATE AND REFINE BOTH TRUE\")\n",
    "            out_dim = 257 * 1024\n",
    "    else:\n",
    "        clip_extractor = Clipper(clip_variant, device=device, train_transforms=train_augs)\n",
    "        out_dim = 768\n",
    "print(\"out_dim:\",out_dim)\n",
    "\n",
    "print('Creating voxel2clip...')\n",
    "\n",
    "if voxel_dims == 1: # 1D data\n",
    "    voxel2clip_kwargs = dict(out_dim=out_dim)\n",
    "    voxel2clip = BrainNetwork(**voxel2clip_kwargs)\n",
    "elif voxel_dims == 3: # 3D data\n",
    "    if text_token:\n",
    "        voxel2clip_kwargs = dict(\n",
    "            out_dim=77*outdim,\n",
    "            dims=voxel.shape[2:],\n",
    "            channels=[64, 128, 256, 128],\n",
    "            strides=[1, 2, 3, 3],\n",
    "            padding=[1, 1, 1, 1],\n",
    "            dilation=[1, 1, 1, 1],\n",
    "            kernel=[3, 3, 3, 3],\n",
    "        )\n",
    "    else:\n",
    "        voxel2clip_kwargs = dict(\n",
    "            out_dim=out_dim,\n",
    "            dims=voxel.shape[2:],\n",
    "            channels=[64, 128, 256, 128],\n",
    "            strides=[1, 2, 3, 3],\n",
    "            padding=[1, 1, 1, 1],\n",
    "            dilation=[1, 1, 1, 1],\n",
    "            kernel=[3, 3, 3, 3],\n",
    "        )\n",
    "    voxel2clip = SimpleVoxel3dConvEncoder(**voxel2clip_kwargs)  \n",
    "\n",
    "print(\"params of voxel2clip:\")\n",
    "if local_rank==0:\n",
    "    utils.count_params(voxel2clip)\n",
    "\n",
    "no_decay = ['bias']\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in voxel2clip.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in voxel2clip.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=initial_lr) # lr doesnt get used if lr_scheduler='cycle'\n",
    "\n",
    "if lr_scheduler == 'fixed':\n",
    "    lr_scheduler = None\n",
    "elif lr_scheduler == 'cycle':\n",
    "    global_batch_size = batch_size * num_devices\n",
    "    total_steps=num_epochs*(num_train//global_batch_size)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=max_lr,\n",
    "        total_steps=total_steps,\n",
    "        final_div_factor=1000,\n",
    "        last_epoch=-1, pct_start=2/num_epochs\n",
    "    )\n",
    "    \n",
    "def save_ckpt(tag):\n",
    "    ckpt_path = outdir+f'/{tag}.pth'\n",
    "    print(f'saving {ckpt_path}',flush=True)\n",
    "    state_dict = voxel2clip.state_dict()\n",
    "    if lr_scheduler!='fixed':\n",
    "        lr_dict = lr_scheduler.state_dict()\n",
    "    else:\n",
    "        lr_dict = None\n",
    "    if with_mse:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': voxel2clip.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_dict,\n",
    "            'train_losses': losses,\n",
    "            'val_losses': val_losses,\n",
    "            'fwd_percent_correct': fwd_percent_correct,\n",
    "            'bwd_percent_correct': bwd_percent_correct,\n",
    "            'val_fwd_percent_correct': val_fwd_percent_correct,\n",
    "            'val_bwd_percent_correct': val_bwd_percent_correct,\n",
    "            'lrs': lrs,\n",
    "            \"mse_losses\": mse_losses,\n",
    "            \"val_mse_losses\": val_mse_losses,\n",
    "            }, ckpt_path)\n",
    "    else:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': voxel2clip.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_dict,\n",
    "            'train_losses': losses,\n",
    "            'val_losses': val_losses,\n",
    "            'fwd_percent_correct': fwd_percent_correct,\n",
    "            'bwd_percent_correct': bwd_percent_correct,\n",
    "            'val_fwd_percent_correct': val_fwd_percent_correct,\n",
    "            'val_bwd_percent_correct': val_bwd_percent_correct,\n",
    "            'lrs': lrs,\n",
    "            }, ckpt_path)\n",
    "        \n",
    "print(\"\\nDone with model preparations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983f458b-35b8-49f2-b6db-80296cece730",
   "metadata": {},
   "source": [
    "# Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a25a662-daa8-4de9-9233-8364800fcb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params for wandb\n",
    "if local_rank==0 and wandb_log:\n",
    "    import wandb\n",
    "    \n",
    "    wandb_project = 'stability'\n",
    "    wandb_run = model_name\n",
    "    wandb_notes = ''\n",
    "    \n",
    "    print(f\"wandb {wandb_project} run {wandb_run}\")\n",
    "    wandb.login(host='https://stability.wandb.io')#, relogin=True)\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"modality\": modality,\n",
    "      \"text_token\": text_token,\n",
    "      \"voxel_dims\": voxel_dims,\n",
    "      \"clip_variant\": clip_variant,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"use_image_aug\": use_image_aug,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"lr_scheduler\": lr_scheduler,\n",
    "      \"mixup_pct\": mixup_pct,\n",
    "      \"mse_pct\": mse_pct,\n",
    "      \"num_train\": num_train,\n",
    "      \"num_val\": num_val,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "      \"train_url\": train_url,\n",
    "      \"val_url\": val_url,\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    if True: # wandb_auto_resume\n",
    "        print(\"wandb_id:\",model_name)\n",
    "        wandb.init(\n",
    "            id = model_name,\n",
    "            project=wandb_project,\n",
    "            name=wandb_run,\n",
    "            config=wandb_config,\n",
    "            notes=wandb_notes,\n",
    "            resume=\"allow\",\n",
    "        )\n",
    "    else:\n",
    "        wandb.init(\n",
    "            project=wandb_project,\n",
    "            name=wandb_run,\n",
    "            config=wandb_config,\n",
    "            notes=wandb_notes,\n",
    "        )\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823a061f-ec5a-405e-8b64-4b7ce8470503",
   "metadata": {},
   "source": [
    "# Start from versatile-nonmse ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "804a870c-85b4-487b-bde7-c6e8d17974cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dict = non_mse_ckpt\n",
    "# for key in list(state_dict.keys()):\n",
    "#     if 'module.' in key:\n",
    "#         state_dict[key.replace('module.', '')] = state_dict[key]\n",
    "#         del state_dict[key]\n",
    "# voxel2clip.load_state_dict(non_mse_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd80e42-a111-4e6a-adbd-9def3a963306",
   "metadata": {},
   "source": [
    "# Huggingface Accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d5ae0ca-02c2-43d1-b20a-d1a33801873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel2clip, optimizer, train_dl, val_dl, lr_scheduler = accelerator.prepare(\n",
    "    voxel2clip, optimizer, train_dl, val_dl, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5690151-2131-4918-b750-e869cbd1a8a8",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00a67aa9-f935-410c-a1a3-b6bc6130d9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.utils import randn_tensor\n",
    "\n",
    "from transformers import CLIPVisionModelWithProjection\n",
    "sd_cache_dir = '/fsx/proj-medarc/fmri/cache/models--shi-labs--versatile-diffusion/snapshots/2926f8e11ea526b562cd592b099fcf9c2985d0b7'\n",
    "image_encoder = CLIPVisionModelWithProjection.from_pretrained(sd_cache_dir, subfolder='image_encoder').to(device)\n",
    "\n",
    "from diffusers import AutoencoderKL, PNDMScheduler, UNet2DConditionModel, UniPCMultistepScheduler\n",
    "sd_cache_dir = '/fsx/proj-medarc/fmri/cache/models--shi-labs--versatile-diffusion/snapshots/2926f8e11ea526b562cd592b099fcf9c2985d0b7'\n",
    "unet = UNet2DConditionModel.from_pretrained(sd_cache_dir,subfolder=\"image_unet\").to(device)\n",
    "\n",
    "unet.eval() # dont want to train model\n",
    "unet.requires_grad_(False) # dont need to calculate gradients\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(sd_cache_dir,subfolder=\"vae\").to(device)\n",
    "vae.eval()\n",
    "vae.requires_grad_(False)\n",
    "\n",
    "scheduler = \"unipc\" # \"pndms\" or \"unipc\"\n",
    "noise_scheduler = PNDMScheduler.from_pretrained(sd_cache_dir, subfolder=\"scheduler\")\n",
    "noise_scheduler = UniPCMultistepScheduler.from_config(noise_scheduler.config)\n",
    "num_inference_steps = 20\n",
    "\n",
    "def decode_latents(latents):\n",
    "    latents = 1 / 0.18215 * latents\n",
    "    image = vae.decode(latents).sample\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    return image\n",
    "\n",
    "guidance_scale=7.5\n",
    "do_classifier_free_guidance = guidance_scale > 1.0\n",
    "vae_scale_factor = 2 ** (len(vae.config.block_out_channels) - 1)\n",
    "height = unet.config.sample_size * vae_scale_factor\n",
    "width = unet.config.sample_size * vae_scale_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4a3368c-e6ce-49cc-b970-ee3dba12dfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need non-deterministic CuDNN for conv3D to work\n",
    "utils.seed_everything(seed, cudnn_deterministic=False)\n",
    "\n",
    "# mse_alphas = np.hstack((np.zeros(int(mse_pct * num_epochs)), np.linspace(0,1,int(num_epochs-(mse_pct * num_epochs)))**2))\n",
    "mse_alphas = np.ones(num_epochs)\n",
    "\n",
    "epoch = 0\n",
    "losses, val_losses, lrs, mse_losses, val_mse_losses = [], [], [], [], []\n",
    "best_val_loss = 1e9\n",
    "soft_loss_temps = utils.cosine_anneal(0.004, 0.0075, num_epochs - int(mixup_pct * num_epochs))\n",
    "\n",
    "mse = nn.MSELoss()\n",
    "val_voxel0 = val_image0 = None\n",
    "\n",
    "# Optionally resume from checkpoint #\n",
    "if resume_from_ckpt:\n",
    "    print(\"\\n---resuming from last.pth ckpt---\\n\")\n",
    "    checkpoint = torch.load(outdir+'/last.pth')\n",
    "    epoch = checkpoint['epoch']\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    voxel2clip.load_state_dict(checkpoint['model_state_dict'])\n",
    "elif wandb_log:\n",
    "    if wandb.run.resumed:\n",
    "        print(\"\\n---resuming from last.pth ckpt---\\n\")\n",
    "        checkpoint = torch.load(outdir+'/last.pth')\n",
    "        epoch = checkpoint['epoch']\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "        voxel2clip.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "print(f\"{model_name} starting with epoch {epoch} / {num_epochs}\")\n",
    "progress_bar = tqdm(range(epoch,num_epochs), ncols=1200, disable=(local_rank!=0))\n",
    "for epoch in progress_bar:\n",
    "    voxel2clip.train()\n",
    "\n",
    "    sims = 0.\n",
    "    sims_base = 0.\n",
    "    val_sims = 0.\n",
    "    val_sims_base = 0.\n",
    "    fwd_percent_correct = 0.\n",
    "    bwd_percent_correct = 0.\n",
    "    val_fwd_percent_correct = 0.\n",
    "    val_bwd_percent_correct = 0.\n",
    "\n",
    "    for train_i, (voxel, image, coco) in enumerate(train_dl):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        repeat_index = train_i % 3\n",
    "\n",
    "        image = image.float()\n",
    "        voxel = voxel.float()[:,repeat_index].float()\n",
    "        \n",
    "        if voxel_dims == 3:\n",
    "            voxel = voxel[:,np.unique(x_inc),:,:]\n",
    "            voxel = voxel[:,:,np.unique(y_inc),:]\n",
    "            voxel = voxel[:,:,:,np.unique(z_inc)]\n",
    "\n",
    "        if epoch < int(mixup_pct * num_epochs):\n",
    "            voxel, perm, betas, select = utils.mixco(voxel)\n",
    "\n",
    "        if text_token:\n",
    "            img_annots = utils.select_annotations(annots[coco.cpu().numpy()], random=True)\n",
    "            img_annots = tokenizer(img_annots.tolist(), padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "            img_annots = text_encoder(img_annots.input_ids.to(device))[0]\n",
    "            clip_target = img_annots.reshape(len(img_annots),-1).float()\n",
    "        elif modality==\"text\":\n",
    "            img_annots = utils.select_annotations(annots[coco.cpu().numpy()], random=True)\n",
    "            clip_target = clip_extractor.embed_text(img_annots).float()\n",
    "        else:\n",
    "            clip_target = clip_extractor.embed_image(image).float()\n",
    "        clip_target=clip_target.reshape(len(clip_target),-1).to(voxel.dtype)\n",
    "        \n",
    "        clip_voxels = voxel2clip(voxel)            \n",
    "            \n",
    "        # if epoch < int(mixup_pct * num_epochs):\n",
    "        #     loss = utils.mixco_nce(\n",
    "        #         nn.functional.normalize(clip_voxels, dim=-1), \n",
    "        #         nn.functional.normalize(clip_target, dim=-1),\n",
    "        #         temp=0.006, perm=perm, betas=betas, select=select,\n",
    "        #         distributed=distributed, accelerator=accelerator, local_rank=local_rank)\n",
    "        # else:\n",
    "        #     epoch_temp = soft_loss_temps[epoch-int(mixup_pct*num_epochs)]\n",
    "        #     loss = utils.soft_clip_loss(\n",
    "        #         nn.functional.normalize(clip_voxels, dim=-1), \n",
    "        #         nn.functional.normalize(clip_target, dim=-1),\n",
    "        #         temp=epoch_temp,\n",
    "        #         distributed=distributed, accelerator=accelerator)\n",
    "        \n",
    "        if with_mse:\n",
    "            # clip_voxels_norm = clip_voxels.reshape(-1, 257, 768)\n",
    "            # clip_voxels_norm = nn.functional.normalize(clip_voxels_norm,dim=-1)\n",
    "            # clip_target_norm = clip_target.reshape(-1, 257, 768)\n",
    "            # clip_target_norm = nn.functional.normalize(clip_target_norm,dim=-1)\n",
    "            \n",
    "            mse_amount = mse_alphas[epoch]\n",
    "            mseloss = mse(clip_voxels,clip_target)*mse_mult\n",
    "            # #loss = (loss*(1-mse_amount)) + (mseloss * mse_amount)\n",
    "            loss = mseloss\n",
    "            mse_losses.append(mseloss.item())\n",
    "        utils.check_loss(loss)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        if distributed:\n",
    "            sims_base += F.cosine_similarity(accelerator.gather(clip_target),\n",
    "                                                  accelerator.gather(clip_voxels)).mean().item()\n",
    "        else:\n",
    "            sims_base += F.cosine_similarity(clip_target,clip_voxels).mean().item()\n",
    "\n",
    "        # forward and backward top 1 accuracy\n",
    "        labels = torch.arange(len(clip_target)).to(device)\n",
    "        fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target, clip_voxels), labels, k=1)\n",
    "        bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels, clip_target), labels, k=1)\n",
    "\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "    voxel2clip.eval()\n",
    "    if local_rank==0: # i think its possible to remove this if statement though with some revisions\n",
    "        for val_i, (voxel, image, coco) in enumerate(val_dl): \n",
    "            with torch.no_grad():\n",
    "                #repeat_index = val_i % 3\n",
    "\n",
    "                image = image.float()\n",
    "                #voxel = voxel[:,repeat_index].float()\n",
    "                voxel = torch.mean(voxel,axis=1).float().to(device)\n",
    "\n",
    "                if voxel_dims == 3:\n",
    "                    voxel = voxel[:,np.unique(x_inc),:,:]\n",
    "                    voxel = voxel[:,:,np.unique(y_inc),:]\n",
    "                    voxel = voxel[:,:,:,np.unique(z_inc)]\n",
    "\n",
    "                if val_image0 is None:\n",
    "                    val_image0 = image.detach().clone()\n",
    "                    val_voxel0 = voxel.detach().clone()\n",
    "\n",
    "                if text_token:\n",
    "                    img_annots = utils.select_annotations(annots[coco.cpu().numpy()], random=True)\n",
    "                    img_annots = tokenizer(img_annots.tolist(), padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "                    img_annots = text_encoder(img_annots.input_ids.to(device))[0]\n",
    "                    clip_target = img_annots.reshape(len(img_annots),-1).float()\n",
    "                elif modality==\"text\":\n",
    "                    img_annots = utils.select_annotations(annots[coco.cpu().numpy()], random=False)\n",
    "                    clip_target = clip_extractor.embed_text(img_annots).float()\n",
    "                else:\n",
    "                    clip_target = clip_extractor.embed_image(image).float()\n",
    "                clip_target=clip_target.reshape(len(clip_target),-1).to(voxel.dtype)\n",
    "\n",
    "                clip_voxels = voxel2clip(voxel)\n",
    "\n",
    "                # if epoch < int(mixup_pct * num_epochs):\n",
    "                #     val_loss = utils.mixco_nce(\n",
    "                #         nn.functional.normalize(clip_voxels, dim=-1), \n",
    "                #         nn.functional.normalize(clip_target, dim=-1),\n",
    "                #         temp=0.006, \n",
    "                #         distributed=distributed, accelerator=accelerator, local_rank=local_rank)\n",
    "                # else:\n",
    "                #     epoch_temp = soft_loss_temps[epoch-int(mixup_pct*num_epochs)]\n",
    "                #     val_loss = utils.soft_clip_loss(\n",
    "                #         nn.functional.normalize(clip_voxels, dim=-1), \n",
    "                #         nn.functional.normalize(clip_target, dim=-1),\n",
    "                #         temp=epoch_temp, \n",
    "                #         distributed=distributed, accelerator=accelerator)\n",
    "\n",
    "                if with_mse:\n",
    "                    # clip_voxels_norm = clip_voxels.reshape(-1, 257, 768)\n",
    "                    # clip_voxels_norm = nn.functional.normalize(clip_voxels_norm,dim=-1)\n",
    "                    # clip_target_norm = clip_target.reshape(-1, 257, 768)\n",
    "                    # clip_target_norm = nn.functional.normalize(clip_target_norm,dim=-1)\n",
    "                    \n",
    "                    val_mseloss = mse(clip_voxels,clip_target)*mse_mult\n",
    "                    # # val_loss = (val_loss*(1-mse_amount)) + (val_mseloss * mse_amount)\n",
    "                    val_loss = val_mseloss\n",
    "                    val_mse_losses.append(val_mseloss.item())\n",
    "                    \n",
    "                utils.check_loss(val_loss)\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "                if distributed:\n",
    "                    val_sims_base += F.cosine_similarity(accelerator.gather(clip_target),\n",
    "                                                          accelerator.gather(clip_voxels)).mean().item()\n",
    "                else:\n",
    "                    val_sims_base += F.cosine_similarity(clip_target,clip_voxels).mean().item()\n",
    "\n",
    "                labels = torch.arange(len(clip_target)).to(device)\n",
    "                val_fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target, clip_voxels), labels, k=1)\n",
    "                val_bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels, clip_target), labels, k=1)\n",
    "\n",
    "        if (not save_at_end and ckpt_saving) or (save_at_end and epoch == num_epochs - 1):\n",
    "            # save best model\n",
    "            val_loss = np.mean(val_losses[-(val_i+1):])\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                save_ckpt('best')\n",
    "            else:\n",
    "                print(f'not best - val_loss: {val_loss:.3f}, best_val_loss: {best_val_loss:.3f}')\n",
    "        \n",
    "        # Save model checkpoint every `ckpt_interval`` epochs or on the last epoch\n",
    "        if (ckpt_interval is not None and (epoch + 1) % ckpt_interval == 0) or epoch == num_epochs - 1:\n",
    "            save_ckpt(f'last')\n",
    "            if wandb_log: # save last ckpt so you can resume from it if need be\n",
    "                wandb.save(os.path.abspath(outdir)+'/last.pth', base_path=os.path.abspath(outdir))\n",
    "\n",
    "        logs = {\"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "                \"val/loss\": np.mean(val_losses[-(val_i+1):]),\n",
    "                \"train/lr\": lrs[-1],\n",
    "                \"train/num_steps\": len(losses),\n",
    "                \"val/num_steps\": len(val_losses),\n",
    "                \"train/cosine_sim_base\": sims_base / (train_i + 1),\n",
    "                \"val/cosine_sim_base\": val_sims_base / (val_i + 1),\n",
    "                \"train/fwd_pct_correct\": fwd_percent_correct / (train_i + 1),\n",
    "                \"train/bwd_pct_correct\": bwd_percent_correct / (train_i + 1),\n",
    "                \"val/val_fwd_pct_correct\": val_fwd_percent_correct / (val_i + 1),\n",
    "                \"val/val_bwd_pct_correct\": val_bwd_percent_correct / (val_i + 1),\n",
    "                \"train/mse_losses\": np.mean(mse_losses[-(train_i+1):]),\n",
    "                \"val/mse_losses\": np.mean(val_mse_losses[-(val_i+1):])}\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if not att:\n",
    "                clip_target = clip_target.reshape(-1, 257, 768)\n",
    "                clip_voxels = clip_voxels.reshape(-1, 257, 768)\n",
    "            else:\n",
    "                clip_target = clip_target.reshape(-1, 257, 1024)\n",
    "                clip_voxels = clip_voxels.reshape(-1, 257, 1024)\n",
    "            ww = 2 # only reconstruct one sample in batch\n",
    "            for ee,in_emb in enumerate([clip_target, clip_voxels]):\n",
    "                if ee==0 and epoch>0:\n",
    "                    continue\n",
    "                if att:\n",
    "                    input_embedding = image_encoder.vision_model.post_layernorm(in_emb[[ww]])\n",
    "                    input_embedding = image_encoder.visual_projection(input_embedding)\n",
    "                    input_embedding = nn.functional.normalize(input_embedding,dim=-1)\n",
    "                else:\n",
    "                    input_embedding = nn.functional.normalize(in_emb[[ww]],dim=-1)\n",
    "\n",
    "                input_embedding = input_embedding.repeat(1, 1, 1)\n",
    "                input_embedding = torch.cat([torch.zeros_like(input_embedding), input_embedding]).to(device)\n",
    "\n",
    "                # 4. Prepare timesteps\n",
    "                noise_scheduler.set_timesteps(num_inference_steps=20, device=device)\n",
    "\n",
    "                # 5b. Prepare latent variables\n",
    "                batch_size = input_embedding.shape[0] // 2 # divide by 2 bc we doubled it for classifier-free guidance\n",
    "                shape = (batch_size, unet.in_channels, height // vae_scale_factor, width // vae_scale_factor)\n",
    "\n",
    "                timesteps = noise_scheduler.timesteps\n",
    "                latents = randn_tensor(shape, device=device, dtype=input_embedding.dtype)\n",
    "                latents = latents * noise_scheduler.init_noise_sigma\n",
    "\n",
    "                # 7. Denoising loop\n",
    "                for i, t in enumerate(timesteps):\n",
    "                    # expand the latents if we are doing classifier free guidance\n",
    "                    latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "                    latent_model_input = noise_scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "                    noise_pred = unet(latent_model_input, t, encoder_hidden_states=input_embedding).sample\n",
    "\n",
    "                    # perform guidance\n",
    "                    if do_classifier_free_guidance:\n",
    "                        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "                    # compute the previous noisy sample x_t -> x_t-1\n",
    "                    latents = noise_scheduler.step(noise_pred, t, latents).prev_sample\n",
    "                if ee==0:\n",
    "                    recons_clip = decode_latents(latents).detach().cpu()\n",
    "                else:\n",
    "                    recons = decode_latents(latents).detach().cpu()\n",
    "                \n",
    "            num_xaxis_subplots = 3\n",
    "            fig, ax = plt.subplots(1, num_xaxis_subplots, \n",
    "                               figsize=(9,3),facecolor=(1, 1, 1))\n",
    "            ax[0].set_title(f\"Original Image\")\n",
    "            ax[0].imshow(utils.torch_to_Image(image[[ww]]))\n",
    "            ax[1].set_title(f\"Recon from orig CLIP\")\n",
    "            ax[1].imshow(utils.torch_to_Image(recons_clip))\n",
    "            ax[2].set_title(\"Recon from brain\")\n",
    "            ax[2].imshow(utils.torch_to_Image(recons))\n",
    "            ax[0].axis('off'); ax[1].axis('off'); ax[2].axis('off')\n",
    "            if wandb_log and local_rank==0:\n",
    "                logs[f\"val/recons\"] = wandb.Image(fig, caption=f\"epoch{epoch:03d}\")\n",
    "                plt.close()\n",
    "            else:\n",
    "                plt.show()\n",
    "\n",
    "        if wandb_log:\n",
    "            wandb.log(logs)\n",
    "            \n",
    "    if distributed:\n",
    "        dist.barrier()\n",
    "\n",
    "if wandb_log and local_rank==0:\n",
    "    wandb.finish()\n",
    "\n",
    "print(\"\\n===Finished!===\\n\")\n",
    "exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff1cb2c-d5c3-47ef-9d16-14f980441a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     clip_target = clip_extractor.embed_image(image).float()\n",
    "#     clip_target=clip_target.reshape(len(clip_target),-1).to(voxel.dtype)\n",
    "#     in_emb = clip_target.reshape(-1, 257, 1024).detach().clone()\n",
    "#     ww = 2\n",
    "    \n",
    "#     in_emb[:,:200] = torch.rand_like(in_emb[:,:200]) * 10\n",
    "    \n",
    "#     input_embedding = image_encoder.vision_model.post_layernorm(in_emb[[ww]])\n",
    "#     input_embedding = image_encoder.visual_projection(input_embedding)\n",
    "#     input_embedding = nn.functional.normalize(input_embedding,dim=-1)\n",
    "\n",
    "#     input_embedding = input_embedding.repeat(1, 1, 1)\n",
    "#     input_embedding = torch.cat([torch.zeros_like(input_embedding), input_embedding]).to(device)\n",
    "\n",
    "#     # 4. Prepare timesteps\n",
    "#     noise_scheduler.set_timesteps(num_inference_steps=20, device=device)\n",
    "\n",
    "#     # 5b. Prepare latent variables\n",
    "#     batch_size = input_embedding.shape[0] // 2 # divide by 2 bc we doubled it for classifier-free guidance\n",
    "#     shape = (batch_size, unet.in_channels, height // vae_scale_factor, width // vae_scale_factor)\n",
    "\n",
    "#     timesteps = noise_scheduler.timesteps\n",
    "#     latents = randn_tensor(shape, device=device, dtype=input_embedding.dtype)\n",
    "#     latents = latents * noise_scheduler.init_noise_sigma\n",
    "\n",
    "#     # 7. Denoising loop\n",
    "#     for i, t in enumerate(timesteps):\n",
    "#         # expand the latents if we are doing classifier free guidance\n",
    "#         latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "#         latent_model_input = noise_scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "#         noise_pred = unet(latent_model_input, t, encoder_hidden_states=input_embedding).sample\n",
    "\n",
    "#         # perform guidance\n",
    "#         if do_classifier_free_guidance:\n",
    "#             noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "#             noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "#         # compute the previous noisy sample x_t -> x_t-1\n",
    "#         latents = noise_scheduler.step(noise_pred, t, latents).prev_sample\n",
    "#     recons_clip = decode_latents(latents).detach().cpu()\n",
    "#     plt.imshow(utils.torch_to_Image(image[ww]))\n",
    "#     plt.show()\n",
    "#     plt.imshow(utils.torch_to_Image(recons_clip))\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1580ee89-e22c-4e0a-ae2d-8012cc02d990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# targs = clip_target\n",
    "# preds = clip_voxels\n",
    "\n",
    "# temp=0.125\n",
    "# clip_clip = (targs @ targs.T)/temp\n",
    "# brain_clip = (preds @ targs.T)/temp\n",
    "\n",
    "# loss1 = -(brain_clip.log_softmax(-1) * clip_clip.softmax(-1)).sum(-1).mean()\n",
    "# loss2 = -(brain_clip.T.log_softmax(-1) * clip_clip.T.softmax(-1)).sum(-1).mean()\n",
    "\n",
    "# loss = (loss1 + loss2)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df8be49-62b9-4a5e-ad2e-5c56c28f5038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def soft_clip_loss(preds, targs, temp=0.125, distributed=False, accelerator=None):\n",
    "#     clip_clip = (targs @ targs.T)/temp\n",
    "#     brain_clip = (preds @ targs.T)/temp\n",
    "    \n",
    "#     loss1 = -(brain_clip.log_softmax(-1) * clip_clip.softmax(-1)).sum(-1).mean()\n",
    "#     loss2 = -(brain_clip.T.log_softmax(-1) * clip_clip.T.softmax(-1)).sum(-1).mean()\n",
    "    \n",
    "#     loss = (loss1 + loss2)/2\n",
    "#     return loss\n",
    "\n",
    "# def mixco_nce(preds, targs, temp=0.1, perm=None, betas=None, select=None, distributed=False, accelerator=None, local_rank=None):\n",
    "#     brain_clip = (preds @ targs.T)/temp\n",
    "    \n",
    "#     probs = torch.diag(betas)\n",
    "#     probs[torch.arange(preds.shape[0]).to(preds.device), perm] = 1 - betas\n",
    "\n",
    "#     loss = -(brain_clip.log_softmax(-1) * probs).sum(-1).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "62aae01ef0cf7b6af841ab1c8ce59175c4332e693ab3d00bc32ceffb78a35376"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
