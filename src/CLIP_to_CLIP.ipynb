{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26ca08a0-e0e9-4cf0-b95c-2c93376f2eb7",
   "metadata": {},
   "source": [
    "This notebook uses a diffusion prior approach to map CLIP-fMRI to CLIP-Image space. It is end-to-end in the sense that we are also fine-tuning the Brain-to-CLIP mapping, but this Brain-to-CLIP mapping is initialized separately first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4d95fac-ac1d-473c-ab96-650f76e6aaf5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook CLIP_to_CLIP.ipynb to python\n",
      "[NbConvertApp] Writing 28956 bytes to CLIP_to_CLIP.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # convert this notebook to .py such that you can then run it via slurm with \"sbatch *.slurm\"\n",
    "# from subprocess import call\n",
    "# command = \"jupyter nbconvert CLIP_to_CLIP.ipynb --to python\"\n",
    "# call(command,shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f0f4f3",
   "metadata": {},
   "source": [
    "# Import packages & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bad764b-45c1-45ce-a716-8d055e09821a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "distributed = False num_devices = 1 local rank = 0 world size = 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from info_nce import InfoNCE\n",
    "from dalle2_pytorch import DiffusionPriorNetwork\n",
    "import kornia\n",
    "from kornia.augmentation.container import AugmentationSequential\n",
    "\n",
    "import utils\n",
    "from utils import torch_to_matplotlib, torch_to_Image\n",
    "from models import Clipper, BrainNetwork, BrainDiffusionPrior, BrainSD\n",
    "from model3d import SimpleVoxel3dConvEncoder\n",
    "\n",
    "import torch.distributed as dist\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from diffusers import UniPCMultistepScheduler\n",
    "\n",
    "# uses tf32 data type which is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# Log to weights and biases?\n",
    "wandb_log = True\n",
    "\n",
    "# Resume from ckpt? #\n",
    "resume_from_ckpt = False\n",
    "if resume_from_ckpt:\n",
    "    ckpt_path = '../train_logs/vox2clip_indiv/ckpt-voxel2clip-epoch029.pth'\n",
    "else:\n",
    "    ckpt_path = 'none'\n",
    "\n",
    "# Multi-GPU config #\n",
    "accelerator = Accelerator()\n",
    "print = accelerator.print # only print if local_rank=0\n",
    "\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0: num_devices = 1\n",
    "num_workers = num_devices\n",
    "\n",
    "print(accelerator.state)\n",
    "local_rank = accelerator.state.local_process_index\n",
    "world_size = accelerator.state.num_processes\n",
    "if num_devices<=1 and world_size<=1:\n",
    "    distributed=False\n",
    "else:\n",
    "    distributed=True\n",
    "print(\"distributed =\",distributed,\"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018b82b-c054-4463-9527-4b0c2a75bda6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60cd7f2c-37fd-426b-a0c6-633e51bc4c4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "voxel2clip_path = '../train_logs/v2c_avg_v0_partialFalse/best.pth' # ckpt path for voxel2clip model\n",
    "\n",
    "combine_models = True # combine voxel2clip and prior into one model and train both end to end\n",
    "combine_losses = True # when combine_models=True, use two terms in the loss, NCE and MSE\n",
    "prior_pretrained = True # starting point = LAION aesthetics\n",
    "\n",
    "model_name = \"diffusion_prior_test\"\n",
    "if not wandb_log:\n",
    "    model_name = \"testing\"\n",
    "\n",
    "modality = \"image\"\n",
    "if modality == \"text\":\n",
    "    is_text = True\n",
    "else:\n",
    "    is_text = False\n",
    "clip_variant = \"ViT-L/14\" # (\"RN50\", \"ViT-L/14\", \"ViT-B/32\")\n",
    "clamp_embs = False # clamp embeddings to (-1.5, 1.5)\n",
    "alpha_schedule = \"constant\" # (\"constant\", \"linear\") - alpha is weight the MSE DP loss\n",
    "voxel_dims = 1 # 1 for flattened input, 3 for 3d input\n",
    "seed = 42\n",
    "\n",
    "# Currently, reconstructing back to image takes up too much memory. Better to save ckpts and separately evaluate!\n",
    "n_samples_save = 0 # how many SD reconstruction samples to save to monitor progress\n",
    "num_inference_steps = 20 # how many steps for diffusion model to output pixel image reconstruction\n",
    "img2img_strength = .6 # closer to 0 the more the recon will look like the input image \n",
    "recons_per_clip = 2\n",
    "recons_per_brain = 4\n",
    "\n",
    "use_mixco = False # use mixup-contrastive on the voxels\n",
    "mixup_pct = 0.5\n",
    "\n",
    "# clip_aug_mode = 'none' # ('none', 'x', 'y')\n",
    "# clip_aug_prob = 0.03 # prob of applying augmentation to a batch\n",
    "sd_scheduler = 'unipcm' # scheduler for SD image variation pipeline ('pndms', 'unipcm')\n",
    "use_image_aug = False # use image augmentation prior to getting CLIP embeddings\n",
    "\n",
    "num_epochs = 60\n",
    "if voxel_dims==1:\n",
    "    batch_size = 64\n",
    "else:\n",
    "    batch_size = 32\n",
    "\n",
    "lr_scheduler = 'cycle'\n",
    "initial_lr = 5e-4 # only used if lr_scheduler is 'fixed'\n",
    "max_lr = 3e-4\n",
    "ckpt_saving = True\n",
    "ckpt_interval = 1 #10\n",
    "save_at_end = True\n",
    "outdir = f'../train_logs/{model_name}'\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir,exist_ok=True)\n",
    "remote_data = False # if True, pull webdatasets from huggingface\n",
    "\n",
    "train_augs = AugmentationSequential(\n",
    "    kornia.augmentation.RandomResizedCrop((224,224), (0.6,1), p=0.3),\n",
    "    kornia.augmentation.Resize((224, 224)),\n",
    "    kornia.augmentation.RandomHorizontalFlip(p=0.5),\n",
    "    kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1, p=0.3),\n",
    "    kornia.augmentation.RandomGrayscale(p=0.3),\n",
    "    data_keys=[\"input\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d13c25-1369-4c49-81d4-83d713586096",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prep models and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7966d5a-c8a9-4461-808a-2f89bb00fe9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using cudnn.deterministic\n",
      "Pulling NSD webdataset data...\n",
      "Prepping train and validation dataloaders...\n",
      "Getting dataloaders...\n",
      "\n",
      "num_train 8859\n",
      "global_batch_size 64\n",
      "batch_size 64\n",
      "num_workers 1\n",
      "num_batches 138\n",
      "num_worker_batches 138\n",
      "cache_dir None\n",
      "\n",
      "num_val 982\n",
      "val_batch_size 300\n",
      "val_num_workers 1\n"
     ]
    }
   ],
   "source": [
    "# need non-deterministic CuDNN for conv3D to work\n",
    "utils.seed_everything(seed, cudnn_deterministic=False)\n",
    "\n",
    "if modality=='text':\n",
    "    print('Using CLIP-text, preparing COCO annotations...')\n",
    "    import h5py\n",
    "    # load COCO annotations curated in the same way as the mind_reader (Lin Sprague Singh) preprint\n",
    "    f = h5py.File('/scratch/gpfs/KNORMAN/nsdgeneral_hdf5/COCO_73k_subj_indices.hdf5', 'r')\n",
    "    subj01_order = f['subj01'][:]\n",
    "    f.close()\n",
    "    annots = np.load('/scratch/gpfs/KNORMAN/nsdgeneral_hdf5/COCO_73k_annots_curated.npy',allow_pickle=True)\n",
    "    subj01_annots = annots[subj01_order]\n",
    "\n",
    "print('Pulling NSD webdataset data...')\n",
    "if remote_data:\n",
    "    # pull data directly from huggingface\n",
    "    train_url, val_url = utils.get_huggingface_urls(data_commit)\n",
    "    meta_url = None\n",
    "else:\n",
    "    # local paths\n",
    "    # data_commit = '9947586218b6b7c8cab804009ddca5045249a38d'\n",
    "    # train_url = f\"/fsx/proj-medarc/fmri/natural-scenes-dataset/{data_commit}/datasets_pscotti_naturalscenesdataset_resolve_{data_commit}_webdataset_train/train_subj01_{{0..49}}.tar\"\n",
    "    # val_url = f\"/fsx/proj-medarc/fmri/natural-scenes-dataset/{data_commit}/datasets_pscotti_naturalscenesdataset_resolve_{data_commit}_webdataset_val/val_subj01_0.tar\"\n",
    "    # meta_url = None\n",
    "    # num_train = num_val = None # None means use all samples as specified in webdataset metadata.json\n",
    "    \n",
    "    train_url = \"{/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split/train/train_subj01_{0..17}.tar,/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split/val/val_subj01_0.tar}\"\n",
    "    val_url = \"/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split/test/test_subj01_{0..1}.tar\"\n",
    "    meta_url = \"/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split/metadata_subj01.json\"\n",
    "    num_train = 8559 + 300\n",
    "    num_val = 982\n",
    "\n",
    "# which to use for the voxels\n",
    "if voxel_dims == 1:\n",
    "    voxels_key = 'nsdgeneral.npy'\n",
    "elif voxel_dims == 3:\n",
    "    voxels_key = 'wholebrain_3d.npy'\n",
    "else:\n",
    "    raise Exception(f\"voxel_dims must be 1 or 3, not {voxel_dims}\")\n",
    "\n",
    "print('Prepping train and validation dataloaders...')\n",
    "train_dl, val_dl, num_train, num_val = utils.get_dataloaders(\n",
    "    batch_size,'images',\n",
    "    num_devices=num_devices,\n",
    "    num_workers=num_workers,\n",
    "    train_url=train_url,\n",
    "    val_url=val_url,\n",
    "    meta_url=meta_url,\n",
    "    num_train=num_train,\n",
    "    num_val=num_val,\n",
    "    val_batch_size=300,\n",
    "    cache_dir=\"/tmp/wds-cache\",\n",
    "    seed=seed,\n",
    "    voxels_key=voxels_key,\n",
    "    local_rank=local_rank,\n",
    ")\n",
    "\n",
    "if voxel_dims == 3:\n",
    "    import nibabel as nib\n",
    "    noise_ceils_path = '/fsx/proj-medarc/fmri/natural-scenes-dataset/temp_s3/nsddata_betas/ppdata/subj01/func1pt8mm/betas_fithrf_GLMdenoise_RR/ncsnr.nii.gz'\n",
    "    noise_ceils = nib.load(noise_ceils_path).get_fdata()\n",
    "    # plt.plot(np.sort(noise_ceils.flatten()))\n",
    "    # plt.show()\n",
    "    x_inc,y_inc,z_inc = np.where(noise_ceils > .5)\n",
    "\n",
    "    # check that your data loader is working and save voxel shape after excluding low signal voxels\n",
    "    for val_i, (voxel, img_input, key) in enumerate(val_dl):\n",
    "        voxel = voxel[:,:,np.unique(x_inc),:,:]\n",
    "        voxel = voxel[:,:,:,np.unique(y_inc),:]\n",
    "        voxel = voxel[:,:,:,:,np.unique(z_inc)]\n",
    "        print(\"voxel.shape\", voxel.shape) # voxel.shape torch.Size([300, 3, 68, 64, 47])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e14d0482-dc42-43b9-9ce1-953c32f2c9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Clipper...\n",
      "ViT-L/14 cuda\n",
      "Creating voxel2clip...\n",
      "params of voxel2clip:\n",
      "param counts:\n",
      "134,722,304 total\n",
      "134,722,304 trainable\n",
      "Creating diffusion prior...\n",
      "params of diffusionprior:\n",
      "param counts:\n",
      "236,616,336 total\n",
      "236,616,320 trainable\n",
      "Creating SD image variation pipeline...\n",
      "\n",
      "Done with model preparations!\n"
     ]
    }
   ],
   "source": [
    "print('Creating Clipper...')\n",
    "    \n",
    "# Don't L2 norm the extracted CLIP embeddings since we want the prior \n",
    "# to learn un-normed embeddings for usage with the SD image variation pipeline.\n",
    "clip_extractor = Clipper(clip_variant, clamp_embs=False, norm_embs=False, device=device, train_transforms=train_augs)\n",
    "\n",
    "print('Creating voxel2clip...')\n",
    "\n",
    "if voxel_dims == 1: # 1D data\n",
    "    voxel2clip_kwargs = dict(out_dim=768)\n",
    "    voxel2clip = BrainNetwork(**voxel2clip_kwargs)\n",
    "elif voxel_dims == 3: # 3D data\n",
    "    voxel2clip_kwargs = dict(\n",
    "        out_dim=768,\n",
    "        dims=voxel.shape[2:],\n",
    "        channels=[64, 128, 256, 128],\n",
    "        strides=[1, 2, 3, 3],\n",
    "        padding=[1, 1, 1, 1],\n",
    "        dilation=[1, 1, 1, 1],\n",
    "        kernel=[3, 3, 3, 3],\n",
    "    )\n",
    "    voxel2clip = SimpleVoxel3dConvEncoder(**voxel2clip_kwargs)  \n",
    "\n",
    "print(\"params of voxel2clip:\")\n",
    "if local_rank==0:\n",
    "    utils.count_params(voxel2clip)\n",
    "    \n",
    "if not combine_models:\n",
    "    # load voxel2clip model weights\n",
    "    ckpt = torch.load(voxel2clip_path, map_location=device)\n",
    "    if 'model_state_dict' in ckpt:\n",
    "        ckpt = ckpt['model_state_dict']\n",
    "    voxel2clip.load_state_dict(ckpt)\n",
    "\n",
    "    # freeze when not combining models\n",
    "    voxel2clip.eval()\n",
    "    voxel2clip.requires_grad_(False)\n",
    "    \n",
    "if local_rank==0: print('Creating diffusion prior...')\n",
    "prior_kwargs = dict(\n",
    "    pretrained=prior_pretrained,\n",
    "    network_kwargs=dict(),\n",
    "    prior_kwargs=dict(),\n",
    ")\n",
    "if not prior_kwargs['pretrained']:\n",
    "    # same as DALLE2-pytorch\n",
    "    prior_network = DiffusionPriorNetwork(\n",
    "        **prior_kwargs['network_kwargs'],\n",
    "    )\n",
    "\n",
    "    # custom version of DiffusionPrior from DALLE2-pytorch\n",
    "    diffusion_prior = BrainDiffusionPrior(\n",
    "        net=prior_network,\n",
    "        voxel2clip=voxel2clip,\n",
    "        **prior_kwargs['prior_kwargs'],\n",
    "    )\n",
    "else:\n",
    "    # not using prior_kwargs b/c the model is pretrained\n",
    "    diffusion_prior = BrainDiffusionPrior.from_pretrained(\n",
    "        # kwargs for DiffusionPriorNetwork\n",
    "        dict(),\n",
    "        # kwargs for DiffusionNetwork\n",
    "        dict(\n",
    "            condition_on_text_encodings=False,\n",
    "            timesteps=1000,\n",
    "            voxel2clip=voxel2clip if combine_models else None,\n",
    "        ),\n",
    "        voxel2clip_path=voxel2clip_path if combine_models else None,\n",
    "    )\n",
    "\n",
    "print(\"params of diffusionprior:\")\n",
    "if local_rank==0: utils.count_params(diffusion_prior)\n",
    "\n",
    "if n_samples_save > 0:\n",
    "    if local_rank == 0: print('Creating SD image variation pipeline...')\n",
    "    from diffusers import StableDiffusionImageVariationPipeline\n",
    "    from diffusers import AutoencoderKL, PNDMScheduler, UNet2DConditionModel, UniPCMultistepScheduler\n",
    "    \n",
    "    sd_cache_dir = os.path.join(\n",
    "                        os.path.expanduser('~'), \n",
    "                        \".cache/huggingface/diffusers/models--lambdalabs--sd-image-variations-diffusers/snapshots/a2a13984e57db80adcc9e3f85d568dcccb9b29fc/\"\n",
    "                    )\n",
    "    if not os.path.isdir(sd_cache_dir): # download from huggingface if not already downloaded / cached\n",
    "        from diffusers import StableDiffusionImageVariationPipeline\n",
    "        print(\"Downloading lambdalabs/sd-image-variations-diffusers from huggingface...\")\n",
    "        sd_pipe = StableDiffusionImageVariationPipeline.from_pretrained(\"lambdalabs/sd-image-variations-diffusers\", revision=\"v2.0\")\n",
    "        sd_cache_dir = \"lambdalabs/sd-image-variations-diffusers\"\n",
    "\n",
    "    unet = UNet2DConditionModel.from_pretrained(sd_cache_dir,subfolder=\"unet\").to(device)\n",
    "    vae = AutoencoderKL.from_pretrained(sd_cache_dir,subfolder=\"vae\").to(device)\n",
    "    noise_scheduler = PNDMScheduler.from_pretrained(sd_cache_dir, subfolder=\"scheduler\")\n",
    "    if sd_scheduler=='unipcm':\n",
    "        noise_scheduler = UniPCMultistepScheduler.from_config(noise_scheduler.config)\n",
    "\n",
    "    unet.eval() # dont want to train model\n",
    "    unet.requires_grad_(False) # dont need to calculate gradients\n",
    "\n",
    "    vae.eval()\n",
    "    vae.requires_grad_(False)\n",
    "    \n",
    "no_decay = ['bias']\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in voxel2clip.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in voxel2clip.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=initial_lr) # lr doesnt get used if lr_scheduler='cycle'\n",
    "\n",
    "if lr_scheduler == 'fixed':\n",
    "    lr_scheduler = None\n",
    "elif lr_scheduler == 'cycle':\n",
    "    total_steps=num_epochs*(num_train//batch_size)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=max_lr,\n",
    "        total_steps=total_steps,\n",
    "        final_div_factor=1000,\n",
    "        last_epoch=-1, pct_start=2/num_epochs\n",
    "    )\n",
    "    \n",
    "def save_ckpt(tag):\n",
    "    ckpt_path = os.path.join(outdir, f'{tag}.pth')\n",
    "    print(f'saving {ckpt_path}',flush=True)\n",
    "    state_dict = diffusion_prior.state_dict()\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': diffusion_prior.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_losses': losses,\n",
    "        'val_losses': val_losses,\n",
    "        'lrs': lrs,\n",
    "        }, ckpt_path)\n",
    "        \n",
    "print(\"\\nDone with model preparations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983f458b-35b8-49f2-b6db-80296cece730",
   "metadata": {},
   "source": [
    "# Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a25a662-daa8-4de9-9233-8364800fcb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params for wandb\n",
    "if local_rank==0 and wandb_log:\n",
    "    wandb_project = 'stability'\n",
    "    wandb_run = model_name\n",
    "    wandb_notes = ''\n",
    "    \n",
    "    if wandb_log: \n",
    "        import wandb\n",
    "        print(f\"wandb {wandb_project} run {wandb_run}\")\n",
    "        wandb.login(host='https://stability.wandb.io')#, relogin=True)\n",
    "        wandb_config = {\n",
    "          \"model_name\": model_name,\n",
    "          \"modality\": modality,\n",
    "          \"voxel_dims\": voxel_dims,\n",
    "          \"clip_variant\": clip_variant,\n",
    "          \"batch_size\": batch_size,\n",
    "          \"num_epochs\": num_epochs,\n",
    "          \"use_image_aug\": use_image_aug,\n",
    "          \"max_lr\": max_lr,\n",
    "          \"lr_scheduler\": lr_scheduler,\n",
    "          \"clamp_embs\": clamp_embs,\n",
    "          \"mixup_pct\": mixup_pct,\n",
    "          \"num_train\": num_train,\n",
    "          \"num_val\": num_val,\n",
    "          \"seed\": seed,\n",
    "          \"distributed\": distributed,\n",
    "          \"num_devices\": num_devices,\n",
    "          \"world_size\": world_size,\n",
    "          \"resume_from_ckpt\": resume_from_ckpt,\n",
    "          \"ckpt_path\": ckpt_path,\n",
    "          \"train_url\": train_url,\n",
    "          \"val_url\": val_url,\n",
    "          \"voxel2clip_path\": voxel2clip_path,\n",
    "          \"combine_models\": combine_models,\n",
    "          \"combine_losses\": combine_losses,\n",
    "          \"prior_pretrained\": prior_pretrained,\n",
    "          \"use_mixco\": use_mixco, \n",
    "          \"n_samples_save\": n_samples_save,\n",
    "          \"sd_scheduler\": sd_scheduler,\n",
    "        }\n",
    "        print(\"wandb_config:\\n\",wandb_config)\n",
    "        wandb.init(\n",
    "            project=wandb_project,\n",
    "            name=wandb_run,\n",
    "            config=wandb_config,\n",
    "            notes=wandb_notes,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd80e42-a111-4e6a-adbd-9def3a963306",
   "metadata": {},
   "source": [
    "# Huggingface Accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d5ae0ca-02c2-43d1-b20a-d1a33801873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_prior, optimizer, train_dl, val_dl, lr_scheduler = accelerator.prepare(\n",
    "    diffusion_prior, optimizer, train_dl, val_dl, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5690151-2131-4918-b750-e869cbd1a8a8",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a3368c-e6ce-49cc-b970-ee3dba12dfcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using cudnn.deterministic\n",
      "contrast_loss InfoNCE()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving ../train_logs/testing/epoch000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|███████████▉                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | 1/60 [01:56<1:54:44, 116.69s/it, train/alpha=0.01, train/bwd_pct_correct=tensor(0.9617, device='cuda:0'), train/cosine_sim_base=0.223, train/fwd_pct_correct=tensor(0.9786, device='cuda:0'), train/loss=5.6, train/loss_mse=338, train/loss_nce=2.24, train/lr=0.000155, train/num_steps=138, train/sim=0.732, val/cosine_sim_base=0.248, val/loss=7.37, val/loss_mse=346, val/loss_nce=3.95, val/num_steps=3, val/sim=0.747, val/val_bwd_pct_correct=0.23, val/val_fwd_pct_correct=0.256]"
     ]
    }
   ],
   "source": [
    "# need non-deterministic CuDNN for conv3D to work\n",
    "utils.seed_everything(seed, cudnn_deterministic=False)\n",
    "\n",
    "epoch = 0\n",
    "losses, val_losses, lrs = [], [], []\n",
    "best_val_loss = 1e9\n",
    "soft_loss_temps = utils.cosine_anneal(0.004, 0.0075, num_epochs - int(mixup_pct * num_epochs))\n",
    "\n",
    "if use_mixco:\n",
    "    contrast_loss = utils.mixco_nce\n",
    "else:\n",
    "    contrast_loss = InfoNCE()\n",
    "print('contrast_loss', contrast_loss)\n",
    "\n",
    "# weight for prior's MSE loss term\n",
    "if alpha_schedule == 'constant':\n",
    "    alphas = np.ones(num_epochs) * 0.01\n",
    "elif alpha_schedule == 'linear':\n",
    "    alphas = np.linspace(0.01, 0.05, num_epochs, endpoint=True)\n",
    "else:\n",
    "    raise ValueError(f'unknown alpha_schedule: {alpha_schedule}')\n",
    "\n",
    "val_voxel0 = val_image0 = None\n",
    "\n",
    "# Optionally resume from checkpoint #\n",
    "# PS: still need to check that this actually works\n",
    "if resume_from_ckpt:\n",
    "    print(\"\\n---resuming from ckpt_path---\\n\",ckpt_path)\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    epoch = checkpoint['epoch']\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    try:\n",
    "        diffusionprior.load_state_dict(checkpoint['model_state_dict'])\n",
    "    except:\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "        for key in list(state_dict.keys()):\n",
    "            if 'module.' in key:\n",
    "                state_dict[key.replace('module.', '')] = state_dict[key]\n",
    "                del state_dict[key]\n",
    "        diffusionprior.load_state_dict(state_dict)\n",
    "\n",
    "progress_bar = tqdm(range(epoch,num_epochs), ncols=1200, disable=(local_rank!=0))\n",
    "for epoch in progress_bar:\n",
    "    diffusion_prior.train()\n",
    "\n",
    "    sims = 0.\n",
    "    sims_base = 0.\n",
    "    val_sims = 0.\n",
    "    val_sims_base = 0.\n",
    "    fwd_percent_correct = 0.\n",
    "    bwd_percent_correct = 0.\n",
    "    val_fwd_percent_correct = 0.\n",
    "    val_bwd_percent_correct = 0.\n",
    "    loss_nce_sum = 0.\n",
    "    loss_prior_sum = 0.\n",
    "    val_loss_nce_sum = 0.\n",
    "    val_loss_prior_sum = 0.\n",
    "    # loss_on_aug = []\n",
    "    # loss_off_aug = []\n",
    "    # image_aug = None\n",
    "    \n",
    "    alpha = alphas[epoch]\n",
    "\n",
    "    for train_i, (voxel, image, trial) in enumerate(train_dl):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        repeat_index = train_i % 3\n",
    "\n",
    "        image = image.float()\n",
    "        voxel = voxel.float()[:,repeat_index].float()\n",
    "        \n",
    "        if voxel_dims == 3:\n",
    "            voxel = voxel[:,np.unique(x_inc),:,:]\n",
    "            voxel = voxel[:,:,np.unique(y_inc),:]\n",
    "            voxel = voxel[:,:,:,np.unique(z_inc)]\n",
    "\n",
    "        if is_text:\n",
    "            annots = utils.select_annotations(subj01_annots[trial], random=True)\n",
    "            clip_target = clip_extractor.embed_text(annots).float()\n",
    "        else:\n",
    "            clip_target = clip_extractor.embed_image(image).float()\n",
    "        clip_target.to(voxel.dtype)\n",
    "        \n",
    "        if use_mixco and epoch < int(mixup_pct * num_epochs):\n",
    "            voxel, perm, betas, select = utils.mixco(voxel)\n",
    "            clip_target = utils.mixco_clip_target(clip_target, perm, select, betas)\n",
    "        \n",
    "        if combine_models:\n",
    "            # loss here is MSE for the prior, clip_voxels are voxel2clip outputs\n",
    "            loss, pred, clip_voxels = diffusion_prior(image_embed=clip_target, voxel=voxel)\n",
    "            utils.check_loss(loss)\n",
    "            \n",
    "            if combine_losses:\n",
    "                # combine losses for contrastive learned voxel2clip mapper and the prior\n",
    "                if use_mixco:\n",
    "                    if epoch < int(mixup_pct * num_epochs):\n",
    "                        loss_nce = contrast_loss(\n",
    "                            nn.functional.normalize(clip_voxels, dim=-1), \n",
    "                            nn.functional.normalize(clip_target, dim=-1),\n",
    "                            temp=0.006, perm=perm, betas=betas, select=select,\n",
    "                            distributed=distributed, accelerator=accelerator, local_rank=local_rank)\n",
    "\n",
    "                    else:\n",
    "                        epoch_temp = soft_loss_temps[epoch-int(mixup_pct*num_epochs)]\n",
    "                        loss_nce = utils.soft_clip_loss(\n",
    "                            nn.functional.normalize(clip_voxels, dim=-1), \n",
    "                            nn.functional.normalize(clip_target, dim=-1),\n",
    "                            temp=epoch_temp,\n",
    "                            distributed=distributed, accelerator=accelerator)\n",
    "                else:\n",
    "                    loss_nce = contrast_loss(\n",
    "                        nn.functional.normalize(clip_voxels, dim=-1), \n",
    "                        nn.functional.normalize(clip_target, dim=-1),\n",
    "                    )\n",
    "                utils.check_loss(loss_nce)\n",
    "\n",
    "                loss_nce_sum += loss_nce.item()\n",
    "                loss_prior_sum += loss.item()\n",
    "\n",
    "                # MSE and NCE are weighted equally at the beginning,\n",
    "                # with alpha=0.01 we'll have something like .01*300 + .99*3 = 3 + 3\n",
    "                loss = alpha * loss + (1-alpha) * loss_nce\n",
    "            else:\n",
    "                loss_prior_sum += loss.item()\n",
    "        else:\n",
    "            # don't train end to end, just use the frozen voxel2clip to get clip_voxels\n",
    "            if distributed:\n",
    "                clip_voxels = voxel2clip.module(voxel)\n",
    "            else:\n",
    "                clip_voxels = voxel2clip(voxel)\n",
    "                \n",
    "            loss, pred, _ = diffusion_prior(text_embed=clip_voxels, image_embed=clip_target)\n",
    "            utils.check_loss(loss)\n",
    "            # loss here is MSE for the prior when not combining losses\n",
    "            loss_prior_sum += loss.item()\n",
    "            \n",
    "        losses.append(loss.item())\n",
    "        lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        # similarity after prior diffusion\n",
    "        sims += F.cosine_similarity(accelerator.gather(clip_target), \n",
    "                                    accelerator.gather(pred)).mean().item()\n",
    "        # baseline similarity before prior diffusion\n",
    "        sims_base += F.cosine_similarity(accelerator.gather(clip_target),\n",
    "                                    accelerator.gather(clip_voxels)).mean().item()\n",
    "\n",
    "        # forward and backward top 1 accuracy\n",
    "        labels = torch.arange(len(clip_target)).to(device)\n",
    "        fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target, clip_voxels), labels, k=1)\n",
    "        bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels, clip_target), labels, k=1)\n",
    "\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "    diffusion_prior.eval()\n",
    "    for val_i, (voxel, image, key) in enumerate(val_dl): \n",
    "        with torch.no_grad():\n",
    "            repeat_index = val_i % 3\n",
    "\n",
    "            image = image.float()\n",
    "            voxel = voxel[:,repeat_index].float()\n",
    "\n",
    "            if voxel_dims == 3:\n",
    "                voxel = voxel[:,np.unique(x_inc),:,:]\n",
    "                voxel = voxel[:,:,np.unique(y_inc),:]\n",
    "                voxel = voxel[:,:,:,np.unique(z_inc)]\n",
    "\n",
    "            if val_image0 is None:\n",
    "                val_image0 = image.detach().clone()\n",
    "                val_voxel0 = voxel.detach().clone()\n",
    "\n",
    "            if is_text:\n",
    "                annots = utils.select_annotations(subj01_annots[trial], random=False)\n",
    "                clip_target = clip_extractor.embed_text(annots).float()\n",
    "            else:\n",
    "                clip_target = clip_extractor.embed_image(image).float()\n",
    "            clip_target.to(voxel.dtype)\n",
    "            \n",
    "            if combine_models:\n",
    "                loss, pred, clip_voxels = diffusion_prior(image_embed=clip_target, voxel=voxel) \\\n",
    "                            if not distributed else diffusion_prior.module(image_embed=clip_target, voxel=voxel)\n",
    "                utils.check_loss(loss)\n",
    "                \n",
    "                if combine_losses:\n",
    "                    if use_mixco:\n",
    "                        if epoch < int(mixup_pct * num_epochs):\n",
    "                            loss_nce = contrast_loss(\n",
    "                                nn.functional.normalize(clip_voxels, dim=-1), \n",
    "                                nn.functional.normalize(clip_target, dim=-1),\n",
    "                                temp=0.006, perm=perm, betas=betas, select=select,\n",
    "                                distributed=distributed, accelerator=accelerator, local_rank=local_rank)\n",
    "\n",
    "                        else:\n",
    "                            epoch_temp = soft_loss_temps[epoch-int(mixup_pct*num_epochs)]\n",
    "                            loss_nce = utils.soft_clip_loss(\n",
    "                                nn.functional.normalize(clip_voxels, dim=-1), \n",
    "                                nn.functional.normalize(clip_target, dim=-1),\n",
    "                                temp=epoch_temp,\n",
    "                                distributed=distributed, accelerator=accelerator)\n",
    "                    else:\n",
    "                        loss_nce = contrast_loss(\n",
    "                            nn.functional.normalize(clip_voxels, dim=-1), \n",
    "                            nn.functional.normalize(clip_target, dim=-1),\n",
    "                        )\n",
    "                    utils.check_loss(loss_nce)\n",
    "                    \n",
    "                    val_loss_nce_sum += loss_nce.item()\n",
    "                    val_loss_prior_sum += loss.item()\n",
    "\n",
    "                    val_loss = alpha * loss + (1-alpha) * loss_nce\n",
    "                else:\n",
    "                    val_loss = loss\n",
    "                    val_loss_prior_sum += loss.item()\n",
    "            else:\n",
    "                clip_voxels = voxel2clip(voxel)\n",
    "                val_loss, pred, _ = diffusion_prior(text_embed=clip_voxels, image_embed=clip_target) \\\n",
    "                    if not distributed else diffusion_prior.module(text_embed=clip_voxels, image_embed=clip_target)\n",
    "\n",
    "            val_losses.append(val_loss.item())\n",
    "            val_sims += F.cosine_similarity(clip_target, pred).mean().item()\n",
    "            val_sims_base += F.cosine_similarity(clip_target, clip_voxels).mean().item()\n",
    "            labels = torch.arange(len(clip_voxels)).to(device)\n",
    "            val_fwd_percent_correct += utils.topk(\n",
    "                utils.batchwise_cosine_similarity(clip_target, clip_voxels), labels, k=1\n",
    "            ).item()\n",
    "            val_bwd_percent_correct += utils.topk(\n",
    "                utils.batchwise_cosine_similarity(clip_voxels, clip_target), labels, k=1\n",
    "            ).item()\n",
    "\n",
    "    if local_rank==0:\n",
    "        if (not save_at_end and ckpt_saving) or (save_at_end and epoch == num_epochs - 1):\n",
    "            # save best model\n",
    "            val_loss = np.mean(val_losses[-(val_i+1):])\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                save_ckpt('best')\n",
    "            else:\n",
    "                print(f'not best - val_loss: {val_loss:.3f}, best_val_loss: {best_val_loss:.3f}')\n",
    "\n",
    "        # Save model checkpoint every `ckpt_interval`` epochs or on the last epoch\n",
    "        if (ckpt_interval is not None and (epoch + 1) % ckpt_interval == 0) or epoch == num_epochs - 1:\n",
    "            save_ckpt(f'epoch{epoch:03d}')\n",
    "\n",
    "        logs = {\"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "                \"val/loss\": np.mean(val_losses[-(val_i+1):]),\n",
    "                \"val/loss\": np.mean(val_losses[-(val_i+1):]),\n",
    "                \"train/loss_nce\": loss_nce_sum / (train_i + 1),\n",
    "                \"train/loss_mse\": loss_prior_sum / (train_i + 1),\n",
    "                \"val/loss_nce\": val_loss_nce_sum / (val_i + 1),\n",
    "                \"val/loss_mse\": val_loss_prior_sum / (val_i + 1),\n",
    "                \"train/lr\": lrs[-1],\n",
    "                \"train/num_steps\": len(losses),\n",
    "                \"val/num_steps\": len(val_losses),\n",
    "                \"train/sim\": sims / (train_i + 1),\n",
    "                \"val/sim\": val_sims / (val_i + 1),\n",
    "                \"train/cosine_sim_base\": sims_base / (train_i + 1),\n",
    "                \"val/cosine_sim_base\": val_sims_base / (val_i + 1),\n",
    "                \"train/fwd_pct_correct\": fwd_percent_correct / (train_i + 1),\n",
    "                \"train/bwd_pct_correct\": bwd_percent_correct / (train_i + 1),\n",
    "                \"val/val_fwd_pct_correct\": val_fwd_percent_correct / (val_i + 1),\n",
    "                \"val/val_bwd_pct_correct\": val_bwd_percent_correct / (val_i + 1),\n",
    "                \"train/alpha\": alpha}\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        \n",
    "        # sample some images\n",
    "        if (ckpt_interval is not None and (epoch + 1) % ckpt_interval == 0) or epoch == num_epochs - 1:\n",
    "            if (not save_at_end and n_samples_save > 0) or (save_at_end and epoch == num_epochs - 1):\n",
    "                # training   \n",
    "                grid = utils.reconstruct_from_clip(\n",
    "                    image0, voxel0,\n",
    "                    diffusion_prior,\n",
    "                    clip_extractor, unet, vae, noise_scheduler,\n",
    "                    img_lowlevel = None,\n",
    "                    num_inference_steps = num_inference_steps,\n",
    "                    n_samples_save = n_samples_save,\n",
    "                    recons_per_clip = recons_per_clip,\n",
    "                    recons_per_brain = recons_per_brain,\n",
    "                    guidance_scale = 7.5,\n",
    "                    img2img_strength = img2img_strength,\n",
    "                    timesteps = recon_timesteps,\n",
    "                    seed = seed,\n",
    "                    distributed = distributed,\n",
    "                )\n",
    "                grid.savefig(os.path.join(outdir, f'samples-train-epoch{epoch:03d}.png'))\n",
    "                if wandb_log and local_rank==0:\n",
    "                    logs[f\"train/recons\"] = wandb.Image(grid, caption=f\"epoch{epoch:03d}\")\n",
    "\n",
    "                # validation\n",
    "                grid = utils.reconstruct_from_clip(\n",
    "                    val_image0, val_voxel0,\n",
    "                    diffusion_prior, \n",
    "                    clip_extractor, unet, vae, noise_scheduler,\n",
    "                    img_lowlevel = None,\n",
    "                    num_inference_steps = num_inference_steps,\n",
    "                    n_samples_save = n_samples_save,\n",
    "                    recons_per_clip = recons_per_clip,\n",
    "                    recons_per_brain = recons_per_brain,\n",
    "                    guidance_scale = 7.5,\n",
    "                    img2img_strength = img2img_strength,\n",
    "                    timesteps = recon_timesteps,\n",
    "                    seed = seed,\n",
    "                    distributed = distributed,\n",
    "                )\n",
    "                grid.savefig(os.path.join(outdir, f'samples-val-epoch{epoch:03d}.png'))\n",
    "                if wandb_log and local_rank==0:\n",
    "                    logs[f\"val/recons\"] = wandb.Image(grid, caption=f\"epoch{epoch:03d}\")\n",
    "            \n",
    "\n",
    "        if wandb_log:\n",
    "            wandb.log(logs)\n",
    "            \n",
    "    if distributed:\n",
    "        dist.barrier()\n",
    "\n",
    "if wandb_log and local_rank==0:\n",
    "    wandb.finish()\n",
    "\n",
    "print(\"\\n===Finished!===\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b1eb64-f9a4-4bac-959b-d9d28da2aec9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "62aae01ef0cf7b6af841ab1c8ce59175c4332e693ab3d00bc32ceffb78a35376"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
