{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53146de1-97c4-4bf9-9569-a6a400d377fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "device: cuda\n",
      "ViT-L/14 cuda\n",
      "torch.Size([982, 3, 256, 256])\n",
      "torch.Size([982, 3, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "local_rank = 0\n",
    "print(\"device:\",device)\n",
    "\n",
    "import utils\n",
    "seed=42\n",
    "utils.seed_everything(seed=seed)\n",
    "\n",
    "# Load CLIP extractor\n",
    "# from models import OpenClipper\n",
    "# clip_extractor = OpenClipper(\"ViT-H-14\", device=device)\n",
    "# imsize = 768\n",
    "\n",
    "from models import Clipper\n",
    "clip_extractor = Clipper(\"ViT-L/14\", hidden_state=False, norm_embs=True, device=device)\n",
    "imsize = 512\n",
    "\n",
    "# all_brain_recons = torch.load('evals/all_brain_recons')\n",
    "# all_brain_recons = torch.load('evals/all_brain_retrievals')\n",
    "# all_brain_recons = torch.load('evals/all_brain_imgtext_recons')\n",
    "\n",
    "model_name = 'autoencoder_subj01_16x' #'prior_257_final_subj01_bimixco_softclip_byol'\n",
    "all_brain_recons = torch.load(f'evals/{model_name}_brain_recons') #v2cranking img2img\n",
    "# all_brain_recons = torch.load('evals/all_brain_recons')\n",
    "# all_brain_recons = torch.load('evals/autoencoder_recons') \n",
    "# all_brain_recons = torch.load('evals/blurry_recons') \n",
    "# all_clip_recons = torch.load('evals/all_clip_recons')\n",
    "all_images = torch.load('evals/all_images')\n",
    "# all_laion_picks = torch.load('evals/all_laion_picks')\n",
    "\n",
    "# model_name = 'prior_257_final_subj01_bimixco_softclip_byol'\n",
    "# all_brain_recons = torch.load(f'evals/{model_name}_laion_retrievals') #v2cranking img2img\n",
    "\n",
    "# all_brain_recons = torch.load(f'evals/braindiffuser_brain_recons')\n",
    "# all_brain_recons = torch.load(f'evals/braindiffuser_brain_recons_no_vdvae')\n",
    "# all_brain_recons = torch.load(f'evals/braindiffuser_vdvae')\n",
    "# all_images = torch.Tensor(np.load('brain-diffuser/data/processed_data/subj01/nsd_test_stim_sub1.npy')/255.).permute(0,3,1,2)\n",
    "\n",
    "print(all_images.shape)\n",
    "print(all_brain_recons.shape)\n",
    "\n",
    "all_images = all_images.to(device)\n",
    "all_brain_recons = all_brain_recons.to(device).to(all_images.dtype).clamp(0,1)\n",
    "\n",
    "# all_images = transforms.Resize((425,425))(all_images)\n",
    "# all_brain_recons = transforms.Resize((425,425))(all_brain_recons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d51995e6-72d7-4068-b193-96dfa03f78e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ii in [0,5,13,15,18,8,45,77,114,159,225,342,394,451,467,500,531,609,776,882,916]:\n",
    "#     print(ii)\n",
    "#     display(utils.torch_to_Image(all_images[ii]))\n",
    "#     display(utils.torch_to_Image(all_brain_recons[ii]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7ba39c-cd18-4942-a86a-ee640865bcdc",
   "metadata": {},
   "source": [
    "# 2-Way Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "49f8056e-a80c-4abc-8c2d-3193bb43f945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "\n",
    "@torch.no_grad()\n",
    "def two_way_identification(all_brain_recons, all_images, model, preprocess, feature_layer=None, return_avg=True):\n",
    "    preds = model(torch.stack([preprocess(recon) for recon in all_brain_recons], dim=0).to(device))\n",
    "    reals = model(torch.stack([preprocess(indiv) for indiv in all_images], dim=0).to(device))\n",
    "    if feature_layer is None:\n",
    "        preds = preds.float().flatten(1).cpu().numpy()\n",
    "        reals = reals.float().flatten(1).cpu().numpy()\n",
    "    else:\n",
    "        preds = preds[feature_layer].float().flatten(1).cpu().numpy()\n",
    "        reals = reals[feature_layer].float().flatten(1).cpu().numpy()\n",
    "\n",
    "    r = np.corrcoef(reals, preds)\n",
    "    r = r[:len(all_images), len(all_images):]\n",
    "    congruents = np.diag(r)\n",
    "\n",
    "    success = r < congruents\n",
    "    success_cnt = np.sum(success, 0)\n",
    "\n",
    "    if return_avg:\n",
    "        perf = np.mean(success_cnt) / (len(all_images)-1)\n",
    "        return perf\n",
    "    else:\n",
    "        return success_cnt, len(all_images)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63801703-0f78-47ca-89c6-54dcf71156a4",
   "metadata": {},
   "source": [
    "## PixCorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d89db923-16c8-4ec8-9bf9-cc9749031188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([982, 541875])\n",
      "torch.Size([982, 541875])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 982/982 [00:07<00:00, 136.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4462545378979282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(425, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "])\n",
    "\n",
    "# Flatten images while keeping the batch dimension\n",
    "all_images_flattened = preprocess(all_images).reshape(len(all_images), -1).cpu()\n",
    "all_brain_recons_flattened = preprocess(all_brain_recons).view(len(all_brain_recons), -1).cpu()\n",
    "\n",
    "print(all_images_flattened.shape)\n",
    "print(all_brain_recons_flattened.shape)\n",
    "\n",
    "corrsum = 0\n",
    "for i in tqdm(range(982)):\n",
    "    corrsum += np.corrcoef(all_images_flattened[i], all_brain_recons_flattened[i])[0][1]\n",
    "corrmean = corrsum / 982\n",
    "\n",
    "print(corrmean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2740d15b-1a02-4d50-9fe4-9dbfae38800e",
   "metadata": {},
   "source": [
    "## SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a712ab76-1c8f-4232-995c-d06f8191d9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted, now calculating ssim...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 982/982 [00:14<00:00, 65.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4886934943681578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# see https://github.com/zijin-gu/meshconv-decoding/issues/3\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(425, interpolation=transforms.InterpolationMode.BILINEAR), \n",
    "])\n",
    "\n",
    "# convert image to grayscale with rgb2grey\n",
    "img_gray = rgb2gray(preprocess(all_images).permute((0,2,3,1)).cpu())\n",
    "recon_gray = rgb2gray(preprocess(all_brain_recons).permute((0,2,3,1)).cpu())\n",
    "print(\"converted, now calculating ssim...\")\n",
    "\n",
    "ssim_score=[]\n",
    "for im,rec in tqdm(zip(img_gray,recon_gray),total=len(all_images)):\n",
    "    ssim_score.append(ssim(rec, im, multichannel=True, gaussian_weights=True, sigma=1.5, use_sample_covariance=False, data_range=1.0))\n",
    "print(np.mean(ssim_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4554b22-7faa-4e59-a6db-83bf775dd9e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "efc906db-bc5f-4cce-87f4-878706c14d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---early, AlexNet(2)---\n",
      "2-way Percent Correct: 0.8397\n",
      "\n",
      "---mid, AlexNet(5)---\n",
      "2-way Percent Correct: 0.7677\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import alexnet, AlexNet_Weights\n",
    "alex_weights = AlexNet_Weights.IMAGENET1K_V1\n",
    "\n",
    "alex_model = create_feature_extractor(alexnet(weights=alex_weights), return_nodes=['features.4','features.11']).to(device)\n",
    "alex_model.eval().requires_grad_(False)\n",
    "\n",
    "# see alex_weights.transforms()\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "layer = 'early, AlexNet(2)'\n",
    "print(f\"\\n---{layer}---\")\n",
    "all_per_correct = two_way_identification(all_brain_recons.to(device).float(), all_images, \n",
    "                                                          alex_model, preprocess, 'features.4')\n",
    "print(f\"2-way Percent Correct: {np.mean(all_per_correct):.4f}\")\n",
    "\n",
    "layer = 'mid, AlexNet(5)'\n",
    "print(f\"\\n---{layer}---\")\n",
    "all_per_correct = two_way_identification(all_brain_recons.to(device).float(), all_images, \n",
    "                                                          alex_model, preprocess, 'features.11')\n",
    "print(f\"2-way Percent Correct: {np.mean(all_per_correct):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a8ac02-a024-442f-a0a9-638a8afdeb0d",
   "metadata": {},
   "source": [
    "### InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b088c-07b4-4164-9e38-bdb5d43d58ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import inception_v3, Inception_V3_Weights\n",
    "weights = Inception_V3_Weights.DEFAULT\n",
    "inception_model = create_feature_extractor(inception_v3(weights=weights), \n",
    "                                           return_nodes=['avgpool']).to(device)\n",
    "inception_model.eval().requires_grad_(False)\n",
    "\n",
    "# see weights.transforms()\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(342, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "all_per_correct = two_way_identification(all_brain_recons, all_images,\n",
    "                                        inception_model, preprocess, 'avgpool')\n",
    "        \n",
    "print(f\"2-way Percent Correct: {np.mean(all_per_correct):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12537959-3045-46f2-833f-80ff6327d40a",
   "metadata": {},
   "source": [
    "### CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc928ed-952c-4b2f-ae05-2cd8b8c770c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "clip_model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(224, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                         std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "])\n",
    "\n",
    "all_per_correct = two_way_identification(all_brain_recons, all_images,\n",
    "                                        clip_model.encode_image, preprocess, None) # final layer\n",
    "print(f\"2-way Percent Correct: {np.mean(all_per_correct):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00a5a08-8acb-4fd9-b581-697df7b845db",
   "metadata": {},
   "source": [
    "### Efficient Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d103b5af-ae7c-4f07-94e0-6b35d951ce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_b1, EfficientNet_B1_Weights\n",
    "weights = EfficientNet_B1_Weights.DEFAULT\n",
    "eff_model = create_feature_extractor(efficientnet_b1(weights=weights), \n",
    "                                    return_nodes=['avgpool']).to(device)\n",
    "eff_model.eval().requires_grad_(False)\n",
    "\n",
    "# see weights.transforms()\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(255, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "gt = eff_model(preprocess(all_images))['avgpool']\n",
    "gt = gt.reshape(len(gt),-1).cpu().numpy()\n",
    "fake = eff_model(preprocess(all_brain_recons))['avgpool']\n",
    "fake = fake.reshape(len(fake),-1).cpu().numpy()\n",
    "\n",
    "print(\"Distance:\",np.array([sp.spatial.distance.correlation(gt[i],fake[i]) for i in range(len(gt))]).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c764bd-4c46-4102-9342-5d63bc47c4e4",
   "metadata": {},
   "source": [
    "### SWAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eacb33d-06b7-4847-b106-3a8ae15798f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "swav_model = torch.hub.load('facebookresearch/swav:main', 'resnet50')\n",
    "swav_model = create_feature_extractor(swav_model, \n",
    "                                    return_nodes=['avgpool']).to(device)\n",
    "swav_model.eval().requires_grad_(False)\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(224, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "gt = swav_model(preprocess(all_images))['avgpool']\n",
    "gt = gt.reshape(len(gt),-1).cpu().numpy()\n",
    "fake = swav_model(preprocess(all_brain_recons))['avgpool']\n",
    "fake = fake.reshape(len(fake),-1).cpu().numpy()\n",
    "\n",
    "print(\"Distance:\",np.array([sp.spatial.distance.correlation(gt[i],fake[i]) for i in range(len(gt))]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79f394a-452f-4470-9dc7-3191868b0bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
