{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26ca08a0-e0e9-4cf0-b95c-2c93376f2eb7",
   "metadata": {},
   "source": [
    "This notebook takes brain voxels and maps them to CLIP-space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4d95fac-ac1d-473c-ab96-650f76e6aaf5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Prior.ipynb to python\n",
      "[NbConvertApp] Writing 35472 bytes to Prior.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # convert this notebook to .py such that you can then run it via slurm with \"sbatch *.slurm\"\n",
    "# from subprocess import call\n",
    "# command = \"jupyter nbconvert Prior.ipynb --to python\"\n",
    "# call(command,shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f0f4f3",
   "metadata": {},
   "source": [
    "# Import packages & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bad764b-45c1-45ce-a716-8d055e09821a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-23 18:14:19.343328: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-23 18:14:21.100638: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "distributed = False num_devices = 1 local rank = 0 world size = 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from info_nce import InfoNCE\n",
    "from dalle2_pytorch import DiffusionPriorNetwork\n",
    "import kornia\n",
    "from kornia.augmentation.container import AugmentationSequential\n",
    "\n",
    "import torch.distributed as dist\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# uses tf32 data type which is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# Multi-GPU config #\n",
    "accelerator = Accelerator()\n",
    "print = accelerator.print # only print if local_rank=0\n",
    "\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0: num_devices = 1\n",
    "num_workers = num_devices\n",
    "\n",
    "print(accelerator.state)\n",
    "local_rank = accelerator.state.local_process_index\n",
    "world_size = accelerator.state.num_processes\n",
    "distributed = not accelerator.state.distributed_type == 'NO'\n",
    "print(\"distributed =\",distributed,\"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size)\n",
    "\n",
    "# custom models and functions #\n",
    "import utils\n",
    "from utils import torch_to_matplotlib, torch_to_Image\n",
    "from models import Clipper, BrainNetwork, BrainDiffusionPrior, VersatileDiffusionPriorNetwork\n",
    "# from model3d import SimpleVoxel3dConvEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018b82b-c054-4463-9527-4b0c2a75bda6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b61fec7-72a0-4b67-86da-1375f1d9fbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--model_name=vers_prior', '--modality=image', '--clip_variant=ViT-L/14', '--batch_size=32', '--n_samples_save=1', '--voxel2clip_path=/fsx/proj-medarc/fmri/paulscotti/fMRI-reconstruction-NSD/train_logs/v2c_v2/last.pth', '--max_lr=3e-4', '--num_epochs=240', '--norm_embs', '--ckpt_interval=1']\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    jupyter_args=[]\n",
    "    jupyter_args.append(\"--model_name=vers_prior\")\n",
    "    jupyter_args.append(\"--modality=image\")\n",
    "    jupyter_args.append(\"--clip_variant=ViT-L/14\")\n",
    "    jupyter_args.append(\"--batch_size=32\")\n",
    "    # jupyter_args.append(\"--with_mse\")\n",
    "    jupyter_args.append(\"--n_samples_save=1\")\n",
    "    jupyter_args.append(\"--voxel2clip_path=/fsx/proj-medarc/fmri/paulscotti/fMRI-reconstruction-NSD/train_logs/v2c_v2/last.pth\")\n",
    "    jupyter_args.append(\"--max_lr=3e-4\")\n",
    "    # jupyter_args.append(\"--mixup_pct=.25\")\n",
    "    jupyter_args.append(\"--num_epochs=240\")\n",
    "    jupyter_args.append(\"--norm_embs\")\n",
    "    # jupyter_args.append(\"--wandb_log\")\n",
    "    jupyter_args.append(\"--ckpt_interval=1\")\n",
    "    # jupyter_args.append(\"--resume_from_ckpt\")\n",
    "    print(jupyter_args)\n",
    "    \n",
    "    from IPython.display import clear_output\n",
    "    \n",
    "    %load_ext autoreload\n",
    "    %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2028bdf0-2f41-46d9-b6e7-86b870dbf16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--modality\", type=str, default=\"image\", choices=[\"image\", \"text\"],\n",
    "    help=\"image or text\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--voxel2clip_path\", type=str, default=\"None\",\n",
    "    help=\"pretrained checkpoint to initialize voxel2clip\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=300,\n",
    "    help=\"Our maximum for A100 was 300 for 1dim voxels and 128 for 3dim voxels\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--clip_variant\",type=str,default=\"ViT-L/14\",choices=[\"RN50\", \"ViT-L/14\", \"ViT-B/32\", \"ViT-H-14\", \"RN50x64\"],\n",
    "    help='clip / openclip variant',\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--outdir\",type=str,default=None,\n",
    "    help=\"output directory for logs and checkpoints\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_log\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to log to wandb\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--resume_from_ckpt\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"if not using wandb and want to resume from a ckpt\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_project\",type=str,default=\"stability\",\n",
    "    help=\"wandb project name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mixup_pct\",type=float,default=.5,\n",
    "    help=\"proportion of way through training when to switch from InfoNCE to soft_clip_loss\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--norm_embs\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"Do norming (using cls token if VD) of CLIP embeddings\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--voxel_dims\",type=int,default=1,choices=[1, 3],\n",
    "    help=\"1 for flattened input, 3 for 3d input\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_image_aug\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to use image augmentation (only used for modality=image)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epochs\",type=int,default=120,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler_type\",type=str,default='cycle',choices=['cycle','fixed'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_saving\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_interval\",type=int,default=5,\n",
    "    help=\"save backup ckpt and reconstruct every x epochs\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--save_at_end\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"if False, will save best.ckpt whenever epoch shows best validation score\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--with_mse\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"Add mse loss to the other losses\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mse_mult\",type=int,default=1,\n",
    "    help=\"Multiplier for mse loss\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mse_amount\",type=float,default=.5,\n",
    "    help=\"What percentage of to weight mse vs. soft_clip loss\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_lr\",type=float,default=3e-4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_samples_save\",type=int,default=0,\n",
    "    help=\"Number of reconstructions for monitoring progress, 0 will speed up training\",\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60cd7f2c-37fd-426b-a0c6-633e51bc4c4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if outdir is None:\n",
    "    outdir = os.path.abspath(f'../train_logs/{model_name}')\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir,exist_ok=True)\n",
    "if use_image_aug:\n",
    "    # img_augment = AugmentationSequential(\n",
    "    #     kornia.augmentation.RandomResizedCrop((224,224), (0.6,1), p=0.3),\n",
    "    #     kornia.augmentation.Resize((224, 224)),\n",
    "    #     kornia.augmentation.RandomHorizontalFlip(p=0.5),\n",
    "    #     kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1, p=0.3),\n",
    "    #     kornia.augmentation.RandomGrayscale(p=0.3),\n",
    "    #     data_keys=[\"input\"],\n",
    "    # )\n",
    "    img_augment = AugmentationSequential(\n",
    "        kornia.augmentation.RandomResizedCrop((240,240), (0.6,1), p=0.3),\n",
    "        kornia.augmentation.Resize((224, 224)),\n",
    "        kornia.augmentation.RandomGaussianBlur(kernel_size=(7,7), sigma=(5,5), p=0.3), #MedianBlur is better but computationally inefficient\n",
    "        # kornia.augmentation.RandomHorizontalFlip(p=0.5),\n",
    "        kornia.augmentation.ColorJiggle(brightness=0.2, contrast=0.2, saturation=0.3, hue=0., p=0.3),\n",
    "    )\n",
    "\n",
    "if modality=='text':\n",
    "    annots = np.load(\"/fsx/proj-medarc/fmri/natural-scenes-dataset/COCO_73k_annots_curated.npy\")\n",
    "    import logging\n",
    "    logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\n",
    "    from transformers import CLIPTextModelWithProjection, CLIPTokenizer\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "    text_encoder = CLIPTextModelWithProjection.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "    text_encoder.eval()\n",
    "    text_encoder.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d13c25-1369-4c49-81d4-83d713586096",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prep models and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7966d5a-c8a9-4461-808a-2f89bb00fe9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using cudnn.deterministic\n",
      "Pulling NSD webdataset data...\n",
      "Prepping train and validation dataloaders...\n",
      "Getting dataloaders...\n",
      "\n",
      "num_train 8859\n",
      "global_batch_size 32\n",
      "batch_size 32\n",
      "num_workers 1\n",
      "num_batches 276\n",
      "num_worker_batches 276\n",
      "cache_dir None\n",
      "\n",
      "num_val 982\n",
      "val_num_batches 30\n",
      "val_batch_size 32\n"
     ]
    }
   ],
   "source": [
    "# need non-deterministic CuDNN for conv3D to work\n",
    "utils.seed_everything(seed, cudnn_deterministic=False)\n",
    "\n",
    "print('Pulling NSD webdataset data...')\n",
    "\n",
    "train_url = \"{/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split/train/train_subj01_{0..17}.tar,/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split/val/val_subj01_0.tar}\"\n",
    "val_url = \"/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split/test/test_subj01_{0..1}.tar\"\n",
    "meta_url = \"/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset_avg_split/metadata_subj01.json\"\n",
    "num_train = 8559 + 300\n",
    "num_val = 982\n",
    "\n",
    "# which to use for the voxels\n",
    "if voxel_dims == 1:\n",
    "    voxels_key = 'nsdgeneral.npy'\n",
    "elif voxel_dims == 3:\n",
    "    voxels_key = 'wholebrain_3d.npy'\n",
    "else:\n",
    "    raise Exception(f\"voxel_dims must be 1 or 3, not {voxel_dims}\")\n",
    "\n",
    "print('Prepping train and validation dataloaders...')\n",
    "train_dl, val_dl, num_train, num_val = utils.get_dataloaders(\n",
    "    batch_size,'images',\n",
    "    num_devices=num_devices,\n",
    "    num_workers=num_workers,\n",
    "    train_url=train_url,\n",
    "    val_url=val_url,\n",
    "    meta_url=meta_url,\n",
    "    num_train=num_train,\n",
    "    num_val=num_val,\n",
    "    val_batch_size=batch_size,\n",
    "    cache_dir=\"/tmp/wds-cache\",\n",
    "    seed=seed,\n",
    "    voxels_key=voxels_key,\n",
    "    to_tuple=[\"voxels\", \"images\", \"coco\"],\n",
    "    local_rank=local_rank,\n",
    ")\n",
    "\n",
    "if voxel_dims == 3:\n",
    "    import nibabel as nib\n",
    "    noise_ceils_path = '/fsx/proj-medarc/fmri/natural-scenes-dataset/temp_s3/nsddata_betas/ppdata/subj01/func1pt8mm/betas_fithrf_GLMdenoise_RR/ncsnr.nii.gz'\n",
    "    noise_ceils = nib.load(noise_ceils_path).get_fdata()\n",
    "    x_inc,y_inc,z_inc = np.where(noise_ceils > .5) # voxel.shape torch.Size([300, 3, 68, 64, 47])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e14d0482-dc42-43b9-9ce1-953c32f2c9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Clipper...\n",
      "Using versatile CLIP space\n",
      "ViT-L/14 cuda\n",
      "out_dim: 197376\n",
      "Creating voxel2clip...\n",
      "params of voxel2clip:\n",
      "param counts:\n",
      "940,225,281 total\n",
      "940,225,280 trainable\n",
      "prior_network loaded\n",
      "params of diffusion prior:\n",
      "param counts:\n",
      "996,280,465 total\n",
      "996,280,448 trainable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'safety_checker': None, 'requires_safety_checker': False} are not expected by VersatileDiffusionDualGuidedPipeline and will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating versatile diffusion reconstruction pipeline...\n",
      "\n",
      "Done with model preparations!\n"
     ]
    }
   ],
   "source": [
    "print('Creating Clipper...')\n",
    "    \n",
    "# Don't L2 norm the extracted CLIP embeddings since we want the prior \n",
    "# to learn un-normed embeddings for usage with the SD image variation pipeline.\n",
    "from models import Clipper\n",
    "if modality=='image':\n",
    "    print(\"Using versatile CLIP space\")\n",
    "    clip_extractor = Clipper(clip_variant, device=device, hidden_state=True, norm_embs=norm_embs)\n",
    "    out_dim = 257 * 768\n",
    "else:\n",
    "    annots = np.load(\"/fsx/proj-medarc/fmri/natural-scenes-dataset/COCO_73k_annots_curated.npy\")\n",
    "    out_dim = 77 * 768\n",
    "print(\"out_dim:\",out_dim)\n",
    "\n",
    "print('Creating voxel2clip...')\n",
    "\n",
    "if voxel_dims == 1: # 1D data\n",
    "    voxel2clip_kwargs = dict(out_dim=out_dim)\n",
    "    voxel2clip = BrainNetwork(**voxel2clip_kwargs)\n",
    "elif voxel_dims == 3: # 3D data\n",
    "    if modality=='text':\n",
    "        voxel2clip_kwargs = dict(\n",
    "            out_dim=77*outdim,\n",
    "            dims=voxel.shape[2:],\n",
    "            channels=[64, 128, 256, 128],\n",
    "            strides=[1, 2, 3, 3],\n",
    "            padding=[1, 1, 1, 1],\n",
    "            dilation=[1, 1, 1, 1],\n",
    "            kernel=[3, 3, 3, 3],\n",
    "        )\n",
    "    else:\n",
    "        voxel2clip_kwargs = dict(\n",
    "            out_dim=out_dim,\n",
    "            dims=voxel.shape[2:],\n",
    "            channels=[64, 128, 256, 128],\n",
    "            strides=[1, 2, 3, 3],\n",
    "            padding=[1, 1, 1, 1],\n",
    "            dilation=[1, 1, 1, 1],\n",
    "            kernel=[3, 3, 3, 3],\n",
    "        )\n",
    "    voxel2clip = SimpleVoxel3dConvEncoder(**voxel2clip_kwargs)  \n",
    "\n",
    "# load from ckpt\n",
    "if voxel2clip_path is not None:\n",
    "    checkpoint = torch.load(voxel2clip_path, map_location='cpu')\n",
    "    voxel2clip.load_state_dict(checkpoint['model_state_dict'])\n",
    "    voxel2clip.temp.requires_grad_(False)\n",
    "    #voxel2clip.eval()\n",
    "    del checkpoint\n",
    "    \n",
    "print(\"params of voxel2clip:\")\n",
    "if local_rank==0:\n",
    "    utils.count_params(voxel2clip)\n",
    "    \n",
    "# setup prior network\n",
    "out_dim = 768\n",
    "depth = 6\n",
    "dim_head = 64\n",
    "heads = 12 # heads * dim_head = 12 * 64 = 768\n",
    "timesteps = 100\n",
    "prior_network = VersatileDiffusionPriorNetwork(\n",
    "        dim=out_dim,\n",
    "        depth=depth,\n",
    "        dim_head=dim_head,\n",
    "        heads=heads,\n",
    "        causal=False,\n",
    "        learned_query_mode=\"pos_emb\"\n",
    "    ).to(device)\n",
    "print(\"prior_network loaded\")\n",
    "\n",
    "# custom version that can fix seeds\n",
    "diffusion_prior = BrainDiffusionPrior(\n",
    "    net=prior_network,\n",
    "    image_embed_dim=out_dim,\n",
    "    condition_on_text_encodings=False,\n",
    "    timesteps=timesteps,\n",
    "    cond_drop_prob=0.2,\n",
    "    image_embed_scale=None,\n",
    "    voxel2clip=voxel2clip,\n",
    ").to(device)\n",
    "\n",
    "print(\"params of diffusion prior:\")\n",
    "if local_rank==0:\n",
    "    utils.count_params(diffusion_prior)\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in diffusion_prior.net.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in diffusion_prior.net.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    {'params': [p for n, p in diffusion_prior.voxel2clip.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in diffusion_prior.voxel2clip.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=3e-4) # lr doesnt get used if lr_scheduler_type='cycle'\n",
    "\n",
    "if lr_scheduler_type == 'fixed':\n",
    "    lr_scheduler = None\n",
    "elif lr_scheduler_type == 'cycle':\n",
    "    global_batch_size = batch_size * num_devices\n",
    "    total_steps=num_epochs*(num_train//global_batch_size)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=max_lr, #[max_lr, max_lr, max_lr/5, max_lr/5],\n",
    "        total_steps=total_steps,\n",
    "        final_div_factor=1000,\n",
    "        last_epoch=-1, pct_start=2/num_epochs\n",
    "    )\n",
    "    \n",
    "if n_samples_save > 0:\n",
    "    import umap\n",
    "    if local_rank == 0: print('Creating versatile diffusion reconstruction pipeline...')\n",
    "    from diffusers import VersatileDiffusionDualGuidedPipeline, UniPCMultistepScheduler\n",
    "    from diffusers.models import DualTransformer2DModel\n",
    "    vd_cache_dir = '/fsx/proj-medarc/fmri/cache/models--shi-labs--versatile-diffusion/snapshots/2926f8e11ea526b562cd592b099fcf9c2985d0b7'\n",
    "    vd_pipe =  VersatileDiffusionDualGuidedPipeline.from_pretrained(\n",
    "            # \"lambdalabs/sd-image-variations-diffusers\",\n",
    "            vd_cache_dir,\n",
    "            safety_checker=None,\n",
    "            requires_safety_checker=False,\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "    vd_pipe.image_unet.eval()\n",
    "    vd_pipe.vae.eval()\n",
    "    vd_pipe.image_unet.requires_grad_(False)\n",
    "    vd_pipe.vae.requires_grad_(False)\n",
    "\n",
    "    vd_pipe.scheduler = UniPCMultistepScheduler.from_pretrained(vd_cache_dir, subfolder=\"scheduler\")\n",
    "    num_inference_steps = 20\n",
    "\n",
    "    # Set weighting of Dual-Guidance \n",
    "    text_image_ratio = .0 # .5 means equally weight text and image, 0 means only use image\n",
    "    condition_types = (\"text\", \"image\")\n",
    "    for name, module in vd_pipe.image_unet.named_modules():\n",
    "        if isinstance(module, DualTransformer2DModel):\n",
    "            module.mix_ratio = text_image_ratio\n",
    "            for i, type in enumerate(condition_types):\n",
    "                if type == \"text\":\n",
    "                    module.condition_lengths[i] = 77\n",
    "                    module.transformer_index_for_condition[i] = 1  # use the second (text) transformer\n",
    "                else:\n",
    "                    module.condition_lengths[i] = 257\n",
    "                    module.transformer_index_for_condition[i] = 0  # use the first (image) transformer\n",
    "    \n",
    "def save_ckpt(tag):\n",
    "    ckpt_path = outdir+f'/{tag}.pth'\n",
    "    print(f'saving {ckpt_path}',flush=True)\n",
    "    unwrapped_model = accelerator.unwrap_model(diffusion_prior)\n",
    "    try:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': unwrapped_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            'train_losses': losses,\n",
    "            'val_losses': val_losses,\n",
    "            'lrs': lrs,\n",
    "            }, ckpt_path)\n",
    "    except:\n",
    "        print(\"Couldn't save... moving on to prevent crashing.\")\n",
    "    del unwrapped_model\n",
    "        \n",
    "print(\"\\nDone with model preparations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983f458b-35b8-49f2-b6db-80296cece730",
   "metadata": {},
   "source": [
    "# Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a25a662-daa8-4de9-9233-8364800fcb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params for wandb\n",
    "if local_rank==0 and wandb_log:\n",
    "    import wandb\n",
    "    \n",
    "    wandb_project = 'stability'\n",
    "    wandb_run = model_name\n",
    "    wandb_notes = ''\n",
    "    \n",
    "    print(f\"wandb {wandb_project} run {wandb_run}\")\n",
    "    wandb.login(host='https://stability.wandb.io')#, relogin=True)\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"modality\": modality,\n",
    "      \"voxel_dims\": voxel_dims,\n",
    "      \"clip_variant\": clip_variant,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"use_image_aug\": use_image_aug,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"lr_scheduler_type\": lr_scheduler_type,\n",
    "      \"mixup_pct\": mixup_pct,\n",
    "      \"mse_amount\": mse_amount,\n",
    "      \"num_train\": num_train,\n",
    "      \"num_val\": num_val,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "      \"train_url\": train_url,\n",
    "      \"val_url\": val_url,\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    if True: # wandb_auto_resume\n",
    "        print(\"wandb_id:\",model_name)\n",
    "        wandb.init(\n",
    "            id = model_name,\n",
    "            project=wandb_project,\n",
    "            name=wandb_run,\n",
    "            config=wandb_config,\n",
    "            notes=wandb_notes,\n",
    "            resume=\"allow\",\n",
    "        )\n",
    "    else:\n",
    "        wandb.init(\n",
    "            project=wandb_project,\n",
    "            name=wandb_run,\n",
    "            config=wandb_config,\n",
    "            notes=wandb_notes,\n",
    "        )\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5690151-2131-4918-b750-e869cbd1a8a8",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12de6387-6e18-4e4b-b5ce-a847d625330a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using cudnn.deterministic\n"
     ]
    }
   ],
   "source": [
    "# need non-deterministic CuDNN for conv3D to work\n",
    "utils.seed_everything(seed, cudnn_deterministic=False)\n",
    "\n",
    "epoch = 0\n",
    "losses, val_losses, lrs = [], [], []\n",
    "nce_losses, val_nce_losses = [], []\n",
    "mse_losses, val_mse_losses =  [], []\n",
    "sim_losses, val_sim_losses = [], []\n",
    "best_val_loss = 1e9\n",
    "\n",
    "prior_mult = 30\n",
    "# mse_mult = 3000\n",
    "\n",
    "mse = nn.MSELoss()\n",
    "val_voxel0 = val_image0 = None\n",
    "\n",
    "# Optionally resume from checkpoint #\n",
    "if resume_from_ckpt:\n",
    "    print(\"\\n---resuming from last.pth ckpt---\\n\")\n",
    "    try:\n",
    "        # checkpoint = torch.load('/fsx/proj-medarc/fmri/paulscotti/fMRI-reconstruction-NSD/train_logs/prior9X/last_backup.pth', map_location='cpu')\n",
    "        checkpoint = torch.load(outdir+'/last.pth', map_location='cpu')\n",
    "    except:\n",
    "        print('last.pth failed... trying last_backup.pth')\n",
    "        checkpoint = torch.load(outdir+'/last_backup.pth', map_location='cpu')\n",
    "    epoch = checkpoint['epoch']\n",
    "    print(\"Epoch\",epoch)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    diffusion_prior.load_state_dict(checkpoint['model_state_dict'])\n",
    "    del checkpoint\n",
    "elif wandb_log:\n",
    "    if wandb.run.resumed:\n",
    "        print(\"\\n---resuming from last.pth ckpt---\\n\")\n",
    "        try:\n",
    "            checkpoint = torch.load(outdir+'/last.pth', map_location='cpu')\n",
    "        except:\n",
    "            print('last.pth failed... trying last_backup.pth')\n",
    "            checkpoint = torch.load(outdir+'/last_backup.pth', map_location='cpu')\n",
    "        epoch = checkpoint['epoch']\n",
    "        print(\"Epoch\",epoch)\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "        diffusion_prior.load_state_dict(checkpoint['model_state_dict'])\n",
    "        del checkpoint\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d5ae0ca-02c2-43d1-b20a-d1a33801873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_prior, optimizer, train_dl, val_dl, lr_scheduler = accelerator.prepare(\n",
    "diffusion_prior, optimizer, train_dl, val_dl, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4a3368c-e6ce-49cc-b970-ee3dba12dfcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fsx/home-paulscotti/miniconda3/envs/medical-v1/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/fsx/home-paulscotti/miniconda3/envs/medical-v1/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "  0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 0/240 [02:18<?, ?it/s, train/bwd_pct_correct=tensor(0.0478, device='cuda:0'), train/cosine_sim_base=0.0428, train/fwd_pct_correct=tensor(0.0419, device='cuda:0'), train/loss=166, train/loss_nce=11.8, train/loss_prior=0.0514, train/lr=0.000156, train/mse_losses=nan, train/num_steps=276, train/temp=0.00224, val/cosine_sim_base=0.0983, val/loss=35.4, val/loss_nce=12.5, val/loss_prior=0.00763, val/mse_losses=nan, val/num_steps=30, val/val_bwd_pct_correct=tensor(0.0469, device='cuda:0'), val/val_fwd_pct_correct=tensor(0.0740, device='cuda:0')]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving /fsx/proj-medarc/fmri/paulscotti/fMRI-reconstruction-NSD/train_logs/vers_prior/last.pth\n",
      "saving /fsx/proj-medarc/fmri/paulscotti/fMRI-reconstruction-NSD/train_logs/vers_prior/last_backup.pth\n",
      "reconstructing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 0/240 [03:07<?, ?it/s, train/bwd_pct_correct=tensor(0.0478, device='cuda:0'), train/cosine_sim_base=0.0428, train/fwd_pct_correct=tensor(0.0419, device='cuda:0'), train/loss=166, train/loss_nce=11.8, train/loss_prior=0.0514, train/lr=0.000156, train/mse_losses=nan, train/num_steps=276, train/temp=0.00224, val/cosine_sim_base=0.0983, val/loss=35.4, val/loss_nce=12.5, val/loss_prior=0.00763, val/mse_losses=nan, val/num_steps=30, val/val_bwd_pct_correct=tensor(0.0469, device='cuda:0'), val/val_fwd_pct_correct=tensor(0.0740, device='cuda:0')]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "reconstruct_from_clip() got an unexpected keyword argument 'timesteps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    211\u001b[0m     vd_pipe \u001b[38;5;241m=\u001b[39m vd_pipe\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m--> 212\u001b[0m     grid, _, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreconstruct_from_clip\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_image0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_voxel0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclip_extractor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvd_pipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_unet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvd_pipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvd_pipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvoxel2clip_img\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiffusion_priors\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdiffusion_prior\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdistributed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdiffusion_prior\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_token\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg_lowlevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_save\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_samples_save\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrecons_per_clip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrecons_per_brain\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg2img_strength\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 0=fully rely on img_lowlevel, 1=not doing img2img\u001b[39;49;00m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretrieve\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mplotting\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wandb_log:\n\u001b[1;32m    231\u001b[0m     logs[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval/recons\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39mImage(grid, caption\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/fsx/home-paulscotti/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: reconstruct_from_clip() got an unexpected keyword argument 'timesteps'"
     ]
    }
   ],
   "source": [
    "print(f\"{model_name} starting with epoch {epoch} / {num_epochs}\")\n",
    "progress_bar = tqdm(range(epoch,num_epochs), ncols=1200, disable=(local_rank!=0))\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    diffusion_prior.train()\n",
    "\n",
    "    sims_base = 0.\n",
    "    val_sims_base = 0.\n",
    "    fwd_percent_correct = 0.\n",
    "    bwd_percent_correct = 0.\n",
    "    val_fwd_percent_correct = 0.\n",
    "    val_bwd_percent_correct = 0.\n",
    "    loss_nce_sum = 0.\n",
    "    loss_prior_sum = 0.\n",
    "    val_loss_nce_sum = 0.\n",
    "    val_loss_prior_sum = 0.\n",
    "\n",
    "    for train_i, (voxel, image, coco) in enumerate(train_dl):\n",
    "        with torch.cuda.amp.autocast():\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            repeat_index = train_i % 3\n",
    "            \n",
    "            if use_image_aug:\n",
    "                image = img_augment(image)\n",
    "\n",
    "            voxel = voxel[:,repeat_index].float()\n",
    "\n",
    "            if voxel_dims == 3:\n",
    "                voxel = voxel[:,np.unique(x_inc),:,:]\n",
    "                voxel = voxel[:,:,np.unique(y_inc),:]\n",
    "                voxel = voxel[:,:,:,np.unique(z_inc)]\n",
    "\n",
    "            if epoch < int(mixup_pct * num_epochs):\n",
    "                voxel, perm, betas, select = utils.mixco(voxel)\n",
    "\n",
    "            if modality=='text':\n",
    "                prompt = utils.select_annotations(annots[coco.cpu().numpy()], random=True).tolist()\n",
    "                text_inputs = tokenizer(\n",
    "                            prompt,\n",
    "                            padding=\"max_length\",\n",
    "                            max_length=tokenizer.model_max_length,\n",
    "                            truncation=True,\n",
    "                            return_tensors=\"pt\",\n",
    "                        )\n",
    "                text_input_ids = text_inputs.input_ids\n",
    "                if hasattr(text_encoder.config, \"use_attention_mask\") and text_encoder.config.use_attention_mask:\n",
    "                    attention_mask = text_inputs.attention_mask.to(device)\n",
    "                else:\n",
    "                    attention_mask = None\n",
    "                prompt_embeds = text_encoder(\n",
    "                    text_input_ids.to(device),\n",
    "                    attention_mask=attention_mask,\n",
    "                )\n",
    "                embeds = text_encoder.text_projection(prompt_embeds.last_hidden_state)\n",
    "                embeds_pooled = prompt_embeds.text_embeds\n",
    "                clip_target = embeds / torch.norm(embeds_pooled.unsqueeze(1), dim=-1, keepdim=True)\n",
    "            else:\n",
    "                clip_target = clip_extractor.embed_image(image).float()   \n",
    "\n",
    "            clip_voxels = diffusion_prior.voxel2clip(voxel).view(len(voxel),-1,768)\n",
    "            loss_prior, aligned_clip_voxels = diffusion_prior(text_embed=clip_voxels, image_embed=clip_target)\n",
    "            \n",
    "            # mseloss = mse(aligned_clip_voxels,clip_target)\n",
    "            # mse_losses.append(mseloss.item())\n",
    "\n",
    "            clip_voxels_norm = nn.functional.normalize(aligned_clip_voxels.flatten(1), dim=-1)\n",
    "            clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "\n",
    "            loss_nce = utils.soft_clip_loss(\n",
    "                clip_voxels_norm,\n",
    "                clip_target_norm,\n",
    "                temp=diffusion_prior.module.voxel2clip.temp if distributed else diffusion_prior.voxel2clip.temp)\n",
    "            loss_nce_sum += loss_nce.item()\n",
    "            loss_prior_sum += loss_prior.item()\n",
    "            loss = loss_nce + (prior_mult * loss_prior) # + (mse_mult * mseloss)\n",
    "            utils.check_loss(loss)\n",
    "            \n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            # gather batches across multi-gpu if there's multiple\n",
    "            clip_voxel_gather = accelerator.gather(clip_voxels_norm.view(len(voxel),-1).contiguous())\n",
    "            clip_target_gather = accelerator.gather(clip_target_norm.view(len(voxel),-1).contiguous())\n",
    "\n",
    "            sims_base += F.cosine_similarity(clip_target_gather,clip_voxel_gather).mean().item()\n",
    "\n",
    "            # forward and backward top 1 accuracy        \n",
    "            labels = torch.arange(len(clip_target_gather)).to(device) \n",
    "            fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxel_gather,clip_target_gather), labels, k=1)\n",
    "            bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_gather, clip_voxel_gather), labels, k=1)\n",
    "\n",
    "            if lr_scheduler_type is not None:\n",
    "                lr_scheduler.step()\n",
    "\n",
    "    diffusion_prior.eval()\n",
    "    for val_i, (voxel, image, coco) in enumerate(val_dl): \n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                # repeat_index = val_i % 3\n",
    "\n",
    "                # voxel = voxel[:,repeat_index].float()\n",
    "                voxel = torch.mean(voxel,axis=1).float()\n",
    "\n",
    "                if voxel_dims == 3:\n",
    "                    voxel = voxel[:,np.unique(x_inc),:,:]\n",
    "                    voxel = voxel[:,:,np.unique(y_inc),:]\n",
    "                    voxel = voxel[:,:,:,np.unique(z_inc)]\n",
    "\n",
    "                if val_image0 is None:\n",
    "                    val_image0 = image.detach().clone()\n",
    "                    val_voxel0 = voxel.detach().clone()\n",
    "\n",
    "                if modality=='text':\n",
    "                    prompt = utils.select_annotations(annots[coco.cpu().numpy()], random=True).tolist()\n",
    "                    text_inputs = tokenizer(\n",
    "                                prompt,\n",
    "                                padding=\"max_length\",\n",
    "                                max_length=tokenizer.model_max_length,\n",
    "                                truncation=True,\n",
    "                                return_tensors=\"pt\",\n",
    "                            )\n",
    "                    text_input_ids = text_inputs.input_ids\n",
    "                    if hasattr(text_encoder.config, \"use_attention_mask\") and text_encoder.config.use_attention_mask:\n",
    "                        attention_mask = text_inputs.attention_mask.to(device)\n",
    "                    else:\n",
    "                        attention_mask = None\n",
    "                    prompt_embeds = text_encoder(\n",
    "                        text_input_ids.to(device),\n",
    "                        attention_mask=attention_mask,\n",
    "                    )\n",
    "                    embeds = text_encoder.text_projection(prompt_embeds.last_hidden_state)\n",
    "                    embeds_pooled = prompt_embeds.text_embeds\n",
    "                    clip_target = embeds / torch.norm(embeds_pooled.unsqueeze(1), dim=-1, keepdim=True)\n",
    "                else:\n",
    "                    clip_target = clip_extractor.embed_image(image).float()\n",
    "\n",
    "                clip_voxels = diffusion_prior.voxel2clip(voxel).view(len(voxel),-1,768)\n",
    "                val_loss_prior, aligned_clip_voxels = diffusion_prior(text_embed=clip_voxels, image_embed=clip_target)\n",
    "\n",
    "                # val_mseloss = mse(aligned_clip_voxels,clip_target)\n",
    "                # val_mse_losses.append(val_mseloss.item())\n",
    "\n",
    "                clip_voxels_norm = nn.functional.normalize(aligned_clip_voxels.flatten(1), dim=-1)\n",
    "                clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "\n",
    "                val_loss_nce = utils.soft_clip_loss(\n",
    "                    clip_voxels_norm,\n",
    "                    clip_target_norm,\n",
    "                    temp=diffusion_prior.module.voxel2clip.temp if distributed else diffusion_prior.voxel2clip.temp)\n",
    "\n",
    "                val_loss_nce_sum += val_loss_nce.item()\n",
    "                val_loss_prior_sum += val_loss_prior.item()\n",
    "                val_loss = val_loss_nce + (prior_mult * val_loss_prior) #+ (mse_mult * val_mseloss)\n",
    "                utils.check_loss(val_loss)\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "                clip_voxel_gather = accelerator.gather(clip_voxels_norm.view(len(voxel),-1).contiguous())\n",
    "                clip_target_gather = accelerator.gather(clip_target_norm.view(len(voxel),-1).contiguous())\n",
    "\n",
    "                val_sims_base += F.cosine_similarity(clip_target_gather,clip_voxel_gather).mean().item()\n",
    "\n",
    "                labels = torch.arange(len(clip_target)).to(device)\n",
    "                val_fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxel_gather,clip_target_gather), labels, k=1)\n",
    "                val_bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_gather, clip_voxel_gather), labels, k=1)\n",
    "\n",
    "    if local_rank==0:        \n",
    "        if (not save_at_end and ckpt_saving) or (save_at_end and epoch == num_epochs - 1):\n",
    "            # save best model\n",
    "            val_loss = np.mean(val_losses[-(val_i+1):])\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                save_ckpt('best')\n",
    "            else:\n",
    "                print(f'not best - val_loss: {val_loss:.3f}, best_val_loss: {best_val_loss:.3f}')\n",
    "                \n",
    "        if utils.is_interactive():\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "        logs = {\"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "            \"val/loss\": np.mean(val_losses[-(val_i+1):]),\n",
    "            \"train/lr\": lrs[-1],\n",
    "            \"train/num_steps\": len(losses),\n",
    "            \"val/num_steps\": len(val_losses),\n",
    "            \"train/cosine_sim_base\": sims_base / (train_i + 1),\n",
    "            \"val/cosine_sim_base\": val_sims_base / (val_i + 1),\n",
    "            \"train/fwd_pct_correct\": fwd_percent_correct / (train_i + 1),\n",
    "            \"train/bwd_pct_correct\": bwd_percent_correct / (train_i + 1),\n",
    "            \"val/val_fwd_pct_correct\": val_fwd_percent_correct / (val_i + 1),\n",
    "            \"val/val_bwd_pct_correct\": val_bwd_percent_correct / (val_i + 1),\n",
    "            \"train/mse_losses\": np.mean(mse_losses[-(train_i+1):]),\n",
    "            \"val/mse_losses\": np.mean(val_mse_losses[-(val_i+1):]),\n",
    "            \"train/loss_nce\": loss_nce_sum / (train_i + 1),\n",
    "            \"train/loss_prior\": loss_prior_sum / (train_i + 1),\n",
    "            \"val/loss_nce\": val_loss_nce_sum / (val_i + 1),\n",
    "            \"val/loss_prior\": val_loss_prior_sum / (val_i + 1),\n",
    "            \"train/temp\": voxel2clip.temp.item()}\n",
    "        progress_bar.set_postfix(**logs)\n",
    "\n",
    "        # Save model checkpoint and reconstruct\n",
    "        save_ckpt(f'last')\n",
    "        if epoch % ckpt_interval == 0:\n",
    "            save_ckpt(f'last_backup')\n",
    "            if n_samples_save > 0:\n",
    "                print('reconstructing...')\n",
    "                with torch.no_grad():\n",
    "                    vd_pipe = vd_pipe.to(device).to(torch.float32)\n",
    "                    grid, _, _, _, _ = utils.reconstruct_from_clip(\n",
    "                        val_image0, val_voxel0,\n",
    "                        clip_extractor, vd_pipe.image_unet, vd_pipe.vae, vd_pipe.scheduler,\n",
    "                        voxel2clip_img = None, \n",
    "                        diffusion_priors = diffusion_prior.module if distributed else diffusion_prior,\n",
    "                        text_token = None,\n",
    "                        img_lowlevel = None,\n",
    "                        num_inference_steps = num_inference_steps,\n",
    "                        n_samples_save = n_samples_save,\n",
    "                        recons_per_clip = 0,\n",
    "                        recons_per_brain = 1,\n",
    "                        guidance_scale = 3.5,\n",
    "                        img2img_strength = 1, # 0=fully rely on img_lowlevel, 1=not doing img2img\n",
    "                        timesteps_prior = timesteps,\n",
    "                        seed = seed,\n",
    "                        retrieve = False,\n",
    "                        plotting = True,\n",
    "                    )\n",
    "                if wandb_log:\n",
    "                    logs[f\"val/recons\"] = wandb.Image(grid, caption=f\"epoch{epoch:03d}\")\n",
    "                    plt.close()\n",
    "                else:\n",
    "                    grid.savefig(os.path.join(outdir, f'samples-val-epoch{epoch:03d}.png'))\n",
    "                    plt.show()\n",
    "                print('umap plotting...')\n",
    "                combined = np.concatenate((clip_target.flatten(1).detach().cpu().numpy(),\n",
    "                                           aligned_clip_voxels.flatten(1).detach().cpu().numpy()),axis=0)\n",
    "                reducer = umap.UMAP(random_state=42)\n",
    "                embedding = reducer.fit_transform(combined)\n",
    "\n",
    "                colors=np.array([[0,0,1,.5] for i in range(len(clip_target))])\n",
    "                colors=np.concatenate((colors, np.array([[0,1,0,.5] for i in range(len(clip_voxels))])))\n",
    "\n",
    "                fig = plt.figure(figsize=(5,5))\n",
    "                plt.scatter(\n",
    "                    embedding[:, 0],\n",
    "                    embedding[:, 1],\n",
    "                    c=colors)\n",
    "                if wandb_log:\n",
    "                    logs[f\"val/umap\"] = wandb.Image(fig, caption=f\"epoch{epoch:03d}\")\n",
    "                    plt.close()\n",
    "                else:\n",
    "                    plt.savefig(os.path.join(outdir, f'umap-val-epoch{epoch:03d}.png'))\n",
    "                    plt.show()\n",
    "                vd_pipe = vd_pipe.to('cpu').to(torch.float16)\n",
    "                del clip_voxels, clip_target, image, voxel\n",
    "                torch.cuda.empty_cache()\n",
    "        if wandb_log: # save last ckpt so you can resume from it if need be\n",
    "            wandb.save(os.path.abspath(outdir)+'/last.pth', base_path=os.path.abspath(outdir))\n",
    "            if epoch % ckpt_interval == 0:\n",
    "                wandb.save(os.path.abspath(outdir)+'/last_backup.pth', base_path=os.path.abspath(outdir))\n",
    "\n",
    "        if wandb_log:\n",
    "            wandb.log(logs)\n",
    "\n",
    "if wandb_log and local_rank==0:\n",
    "    wandb.finish()\n",
    "\n",
    "print(\"\\n===Finished!===\\n\")\n",
    "if not utils.is_interactive():\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc68f690-1a9a-463f-a2cf-b1df36a75090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     # _, pred = diffusion_prior(image_embed=clip_target, text_embed=clip_voxels)\n",
    "#     with torch.cuda.amp.autocast():\n",
    "#         vd_pipe = vd_pipe.to(device).to(torch.float32)\n",
    "#         grid, _, _, _, _ = utils.reconstruct_from_clip(\n",
    "#             val_image0, val_voxel0,\n",
    "#             clip_extractor, vd_pipe.image_unet, vd_pipe.vae, noise_scheduler,\n",
    "#             voxel2clip_img = None, \n",
    "#             diffusion_priors = diffusion_prior.module if distributed else diffusion_prior,\n",
    "#             text_token = None,\n",
    "#             img_lowlevel = None,\n",
    "#             num_inference_steps = num_inference_steps,\n",
    "#             n_samples_save = n_samples_save,\n",
    "#             recons_per_clip = 0,\n",
    "#             recons_per_brain = 1,\n",
    "#             guidance_scale = 3.5,\n",
    "#             img2img_strength = 1, # 0=fully rely on img_lowlevel, 1=not doing img2img\n",
    "#             timesteps = 100,\n",
    "#             seed = seed,\n",
    "#             retrieve = False,\n",
    "#             plotting = True,\n",
    "#             fixed = clip_target,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e71dcdd-03fb-40c5-9c24-6d82134b0a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for val_i, (voxel, image, coco) in enumerate(val_dl): \n",
    "#     with torch.no_grad():\n",
    "#         # repeat_index = val_i % 3\n",
    "\n",
    "#         with torch.cuda.amp.autocast():\n",
    "#             image = image\n",
    "#             # voxel = voxel[:,repeat_index].float()\n",
    "#             voxel = torch.mean(voxel,axis=1)\n",
    "\n",
    "#             if voxel_dims == 3:\n",
    "#                 voxel = voxel[:,np.unique(x_inc),:,:]\n",
    "#                 voxel = voxel[:,:,np.unique(y_inc),:]\n",
    "#                 voxel = voxel[:,:,:,np.unique(z_inc)]\n",
    "\n",
    "#             if val_image0 is None:\n",
    "#                 val_image0 = image.detach().clone()\n",
    "#                 val_voxel0 = voxel.detach().clone()\n",
    "\n",
    "#             if modality=='text':\n",
    "#                 prompt = utils.select_annotations(annots[coco.cpu().numpy()], random=True).tolist()\n",
    "#                 text_inputs = tokenizer(\n",
    "#                             prompt,\n",
    "#                             padding=\"max_length\",\n",
    "#                             max_length=tokenizer.model_max_length,\n",
    "#                             truncation=True,\n",
    "#                             return_tensors=\"pt\",\n",
    "#                         )\n",
    "#                 text_input_ids = text_inputs.input_ids\n",
    "#                 if hasattr(text_encoder.config, \"use_attention_mask\") and text_encoder.config.use_attention_mask:\n",
    "#                     attention_mask = text_inputs.attention_mask.to(device)\n",
    "#                 else:\n",
    "#                     attention_mask = None\n",
    "#                 prompt_embeds = text_encoder(\n",
    "#                     text_input_ids.to(device),\n",
    "#                     attention_mask=attention_mask,\n",
    "#                 )\n",
    "#                 embeds = text_encoder.text_projection(prompt_embeds.last_hidden_state)\n",
    "#                 embeds_pooled = prompt_embeds.text_embeds\n",
    "#                 clip_target = embeds / torch.norm(embeds_pooled.unsqueeze(1), dim=-1, keepdim=True)\n",
    "#             else:\n",
    "#                 clip_target = clip_extractor.embed_image(image).float()\n",
    "\n",
    "#             clip_voxels = diffusion_prior.module.voxel2clip(voxel) if distributed else diffusion_prior.voxel2clip(voxel)\n",
    "#             clip_voxels = clip_voxels.view(len(voxel),-1,768)\n",
    "#             _, pred = diffusion_prior(image_embed=clip_target, text_embed=clip_voxels)\n",
    "#             err"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "62aae01ef0cf7b6af841ab1c8ce59175c4332e693ab3d00bc32ceffb78a35376"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
