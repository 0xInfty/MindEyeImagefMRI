{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a9a8162-755a-4b23-af7c-2bb98f6ccd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Recon_Evaluation.ipynb to python\n",
      "[NbConvertApp] Writing 16306 bytes to Recon_Evaluation.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Code to convert this notebook to .py if you want to run it via command line or with Slurm\n",
    "# from subprocess import call\n",
    "# command = \"jupyter nbconvert Recon_Evaluation.ipynb --to python\"\n",
    "# call(command,shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d437e335-a02b-4383-8fd1-cc80641b483f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "Interactive. Subj =  2\n",
      "subj 2 num_voxels 14278\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import webdataset as wds\n",
    "import PIL\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "local_rank = 0\n",
    "print(\"device:\",device)\n",
    "\n",
    "import utils\n",
    "from models import Clipper, OpenClipper, BrainNetwork, BrainDiffusionPrior, BrainDiffusionPriorOld, Voxel2StableDiffusionModel, VersatileDiffusionPriorNetwork\n",
    "\n",
    "if utils.is_interactive():\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "seed=42\n",
    "utils.seed_everything(seed=seed)\n",
    "\n",
    "subj = 2 #note: we only trained subjects 1 2 5 7, since they have data across full sessions\n",
    "if not utils.is_interactive():\n",
    "    subj = int(sys.argv[1])\n",
    "    print(\"Subj = \",subj)\n",
    "else:\n",
    "    print(\"Interactive. Subj = \",subj)\n",
    "if subj == 1:\n",
    "    num_voxels = 15724\n",
    "elif subj == 2:\n",
    "    num_voxels = 14278\n",
    "elif subj == 3:\n",
    "    num_voxels = 15226\n",
    "elif subj == 4:\n",
    "    num_voxels = 13153\n",
    "elif subj == 5:\n",
    "    num_voxels = 13039\n",
    "elif subj == 6:\n",
    "    num_voxels = 17907\n",
    "elif subj == 7:\n",
    "    num_voxels = 12682\n",
    "elif subj == 8:\n",
    "    num_voxels = 14386\n",
    "print(\"subj\",subj,\"num_voxels\",num_voxels)\n",
    "\n",
    "print(\"PID of this process=\",os.getpid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd1bc7cd-a709-4314-a357-56f9397db01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 0\n",
      "voxel.shape torch.Size([1, 3, 14278])\n",
      "img_input.shape torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/fsx/proj-medarc/fmri/natural-scenes-dataset\"\n",
    "val_url = f\"{data_path}/webdataset_avg_split/test/test_subj0{subj}_\" + \"{0..1}.tar\"\n",
    "meta_url = f\"{data_path}/webdataset_avg_split/metadata_subj0{subj}.json\"\n",
    "num_train = 8559 + 300\n",
    "num_val = 982\n",
    "batch_size = val_batch_size = 1\n",
    "voxels_key = 'nsdgeneral.npy' # 1d inputs\n",
    "\n",
    "val_data = wds.WebDataset(val_url, resampled=False)\\\n",
    "    .decode(\"torch\")\\\n",
    "    .rename(images=\"jpg;png\", voxels=voxels_key, trial=\"trial.npy\", coco=\"coco73k.npy\", reps=\"num_uniques.npy\")\\\n",
    "    .to_tuple(\"voxels\", \"images\", \"coco\")\\\n",
    "    .batched(val_batch_size, partial=False)\n",
    "\n",
    "val_dl = torch.utils.data.DataLoader(val_data, batch_size=None, shuffle=False)\n",
    "\n",
    "# check that your data loader is working\n",
    "for val_i, (voxel, img_input, coco) in enumerate(val_dl):\n",
    "    print(\"idx\",val_i)\n",
    "    print(\"voxel.shape\",voxel.shape)\n",
    "    print(\"img_input.shape\",img_input.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c10121a-87fa-4f4a-95d1-9b9d4f7c6a04",
   "metadata": {},
   "source": [
    "## Load autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7622029d-ea95-43fc-8688-81057dcadf42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded low-level model!\n"
     ]
    }
   ],
   "source": [
    "from diffusers.models.vae import Decoder\n",
    "class Voxel2StableDiffusionModel(torch.nn.Module):\n",
    "    def __init__(self, in_dim=15724, h=4096, n_blocks=4, use_cont=False, ups_mode='4x'):\n",
    "        super().__init__()\n",
    "        self.lin0 = nn.Sequential(\n",
    "            nn.Linear(in_dim, h, bias=False),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        self.mlp = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(h, h, bias=False),\n",
    "                nn.LayerNorm(h),\n",
    "                nn.SiLU(inplace=True),\n",
    "                nn.Dropout(0.25)\n",
    "            ) for _ in range(n_blocks)\n",
    "        ])\n",
    "        self.ups_mode = ups_mode\n",
    "        if ups_mode=='4x':\n",
    "            self.lin1 = nn.Linear(h, 16384, bias=False)\n",
    "            self.norm = nn.GroupNorm(1, 64)\n",
    "            \n",
    "            self.upsampler = Decoder(\n",
    "                in_channels=64,\n",
    "                out_channels=4,\n",
    "                up_block_types=[\"UpDecoderBlock2D\",\"UpDecoderBlock2D\",\"UpDecoderBlock2D\"],\n",
    "                block_out_channels=[64, 128, 256],\n",
    "                layers_per_block=1,\n",
    "            )\n",
    "\n",
    "            if use_cont:\n",
    "                self.maps_projector = nn.Sequential(\n",
    "                    nn.Conv2d(64, 512, 1, bias=False),\n",
    "                    nn.GroupNorm(1,512),\n",
    "                    nn.ReLU(True),\n",
    "                    nn.Conv2d(512, 512, 1, bias=False),\n",
    "                    nn.GroupNorm(1,512),\n",
    "                    nn.ReLU(True),\n",
    "                    nn.Conv2d(512, 512, 1, bias=True),\n",
    "                )\n",
    "            else:\n",
    "                self.maps_projector = nn.Identity()\n",
    "        \n",
    "        if ups_mode=='8x':  # prev best\n",
    "            self.lin1 = nn.Linear(h, 16384, bias=False)\n",
    "            self.norm = nn.GroupNorm(1, 256)\n",
    "            \n",
    "            self.upsampler = Decoder(\n",
    "                in_channels=256,\n",
    "                out_channels=4,\n",
    "                up_block_types=[\"UpDecoderBlock2D\",\"UpDecoderBlock2D\",\"UpDecoderBlock2D\",\"UpDecoderBlock2D\"],\n",
    "                block_out_channels=[64, 128, 256, 256],\n",
    "                layers_per_block=1,\n",
    "            )\n",
    "            self.maps_projector = nn.Identity()\n",
    "        \n",
    "        if ups_mode=='16x':\n",
    "            self.lin1 = nn.Linear(h, 8192, bias=False)\n",
    "            self.norm = nn.GroupNorm(1, 512)\n",
    "            \n",
    "            self.upsampler = Decoder(\n",
    "                in_channels=512,\n",
    "                out_channels=4,\n",
    "                up_block_types=[\"UpDecoderBlock2D\",\"UpDecoderBlock2D\",\"UpDecoderBlock2D\",\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n",
    "                block_out_channels=[64, 128, 256, 256, 512],\n",
    "                layers_per_block=1,\n",
    "            )\n",
    "            self.maps_projector = nn.Identity()\n",
    "\n",
    "    def forward(self, x, return_transformer_feats=False):\n",
    "        x = self.lin0(x)\n",
    "        residual = x\n",
    "        for res_block in self.mlp:\n",
    "            x = res_block(x)\n",
    "            x = x + residual\n",
    "            residual = x\n",
    "        x = x.reshape(len(x), -1)\n",
    "        x = self.lin1(x)  # bs, 4096\n",
    "\n",
    "        if self.ups_mode == '4x':\n",
    "            side = 16\n",
    "        if self.ups_mode == '8x':\n",
    "            side = 8\n",
    "        if self.ups_mode == '16x':\n",
    "            side = 4\n",
    "        \n",
    "        # decoder\n",
    "        x = self.norm(x.reshape(x.shape[0], -1, side, side).contiguous())\n",
    "        if return_transformer_feats:\n",
    "            return self.upsampler(x), self.maps_projector(x).flatten(2).permute(0,2,1)\n",
    "        return self.upsampler(x)\n",
    "\n",
    "voxel2sd = Voxel2StableDiffusionModel(in_dim=num_voxels)\n",
    "\n",
    "model_name = f'autoencoder_subj0{subj}_4x_locont_no_reconst/test'\n",
    "ckpt_path = f'/fsx/proj-medarc/fmri/fMRI-reconstruction-NSD/train_logs/models/{model_name}/epoch120.pth'\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "# state_dict = torch.load('../train_logs/autoencoder/last.pth', \n",
    "#                         map_location='cpu')\n",
    "# print(state_dict[\"epoch\"])\n",
    "# state_dict = state_dict[\"model_state_dict\"]\n",
    "\n",
    "voxel2sd.load_state_dict(state_dict,strict=False)\n",
    "voxel2sd.eval()\n",
    "voxel2sd.to(device)\n",
    "print(\"Loaded low-level model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403bb740-db7a-4b1d-9c2b-2c78a9ace10d",
   "metadata": {},
   "source": [
    "## Load Versatile Diffusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c427f9b1-afbe-43df-a0bc-8c28842c7f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT-L/14 cuda\n",
      "Model name: prior_257_final_subj02_bimixco_softclip_byol\n",
      "ckpt_path /fsx/proj-medarc/fmri/fMRI-reconstruction-NSD/train_logs/models/prior_257_final_subj02_bimixco_softclip_byol/epoch239.pth\n",
      "EPOCH:  239\n"
     ]
    }
   ],
   "source": [
    "out_dim = 257 * 768\n",
    "clip_extractor = Clipper(\"ViT-L/14\", hidden_state=True, norm_embs=True, device=device)\n",
    "voxel2clip_kwargs = dict(in_dim=num_voxels,out_dim=out_dim)\n",
    "voxel2clip = BrainNetwork(**voxel2clip_kwargs)\n",
    "voxel2clip.requires_grad_(False)\n",
    "voxel2clip.eval()\n",
    "\n",
    "out_dim = 768\n",
    "depth = 6\n",
    "dim_head = 64\n",
    "heads = 12 # heads * dim_head = 12 * 64 = 768\n",
    "timesteps = 100\n",
    "\n",
    "prior_network = VersatileDiffusionPriorNetwork(\n",
    "        dim=out_dim,\n",
    "        depth=depth,\n",
    "        dim_head=dim_head,\n",
    "        heads=heads,\n",
    "        causal=False,\n",
    "        learned_query_mode=\"pos_emb\"\n",
    "    )\n",
    "\n",
    "diffusion_prior = BrainDiffusionPrior(\n",
    "    net=prior_network,\n",
    "    image_embed_dim=out_dim,\n",
    "    condition_on_text_encodings=False,\n",
    "    timesteps=timesteps,\n",
    "    cond_drop_prob=0.2,\n",
    "    image_embed_scale=None,\n",
    "    voxel2clip=voxel2clip,\n",
    ")\n",
    "\n",
    "# model_name = \"prior_257_subj01\"\n",
    "# outdir = f'../train_logs/{model_name}'\n",
    "# ckpt_path = os.path.join(outdir, f'last.pth')\n",
    "\n",
    "model_name = f'prior_257_final_subj0{subj}_bimixco_softclip_byol'\n",
    "# if not utils.is_interactive():\n",
    "#     model_name = sys.argv[2]\n",
    "print(\"Model name:\",model_name)\n",
    "ckpt_path = f'/fsx/proj-medarc/fmri/fMRI-reconstruction-NSD/train_logs/models/{model_name}/epoch239.pth'\n",
    "\n",
    "print(\"ckpt_path\",ckpt_path)\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "print(\"EPOCH: \",checkpoint['epoch'])\n",
    "diffusion_prior.load_state_dict(state_dict,strict=False)\n",
    "diffusion_prior.eval().to(device)\n",
    "diffusion_priors = [diffusion_prior]\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef19a234-da4d-497c-8f28-325820be922c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ckpt_path ../train_logs/final_subj02_1x768/last.pth\n",
      "EPOCH:  299\n"
     ]
    }
   ],
   "source": [
    "# CLS model\n",
    "out_dim = 768\n",
    "voxel2clip_kwargs = dict(in_dim=num_voxels,out_dim=out_dim)\n",
    "voxel2clip_cls = BrainNetwork(**voxel2clip_kwargs)\n",
    "voxel2clip_cls.requires_grad_(False)\n",
    "voxel2clip_cls.eval()\n",
    "\n",
    "diffusion_prior_cls = BrainDiffusionPriorOld.from_pretrained(\n",
    "    # kwargs for DiffusionPriorNetwork\n",
    "    dict(),\n",
    "    # kwargs for DiffusionNetwork\n",
    "    dict(\n",
    "        condition_on_text_encodings=False,\n",
    "        timesteps=1000,\n",
    "        voxel2clip=voxel2clip_cls,\n",
    "    ),\n",
    "    voxel2clip_path=None,\n",
    ")\n",
    "\n",
    "model_name_cls = f\"final_subj0{subj}_1x768\"\n",
    "outdir = f'../train_logs/{model_name_cls}'\n",
    "ckpt_path = os.path.join(outdir, f'last.pth')\n",
    "print(\"ckpt_path\",ckpt_path)\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "print(\"EPOCH: \",checkpoint['epoch'])\n",
    "diffusion_prior_cls.load_state_dict(state_dict,strict=False)\n",
    "diffusion_prior_cls.eval().to(device)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "871b3fcf-15eb-42ce-8bec-f624a0c220b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'safety_checker': None, 'requires_safety_checker': False} are not expected by VersatileDiffusionDualGuidedPipeline and will be ignored.\n"
     ]
    }
   ],
   "source": [
    "from diffusers import VersatileDiffusionDualGuidedPipeline, UniPCMultistepScheduler\n",
    "from diffusers.models import DualTransformer2DModel\n",
    "vd_cache_dir = '/fsx/proj-medarc/fmri/cache/models--shi-labs--versatile-diffusion/snapshots/2926f8e11ea526b562cd592b099fcf9c2985d0b7'\n",
    "try:\n",
    "    vd_pipe =  VersatileDiffusionDualGuidedPipeline.from_pretrained(\n",
    "            # \"lambdalabs/sd-image-variations-diffusers\",\n",
    "            vd_cache_dir,\n",
    "            safety_checker=None,\n",
    "            requires_safety_checker=False,\n",
    "        ).to('cpu').to(torch.float16)\n",
    "except:\n",
    "    vd_cache_dir = 'specify a path to save the pretrained Versatile Diffusion model'\n",
    "    print(\"vd_cache_dir\", vd_cache_dir)\n",
    "    vd_pipe =  VersatileDiffusionDualGuidedPipeline.from_pretrained(\n",
    "            \"lambdalabs/sd-image-variations-diffusers\",\n",
    "            safety_checker=None,\n",
    "            requires_safety_checker=False,\n",
    "            cache_dir = vd_cache_dir,\n",
    "        )\n",
    "vd_pipe.image_unet.eval()\n",
    "vd_pipe.vae.eval()\n",
    "vd_pipe.image_unet.requires_grad_(False)\n",
    "vd_pipe.vae.requires_grad_(False)\n",
    "\n",
    "vd_pipe.scheduler = UniPCMultistepScheduler.from_pretrained(vd_cache_dir, subfolder=\"scheduler\")\n",
    "num_inference_steps = 20\n",
    "\n",
    "# Set weighting of Dual-Guidance \n",
    "text_image_ratio = .0 # .5 means equally weight text and image, 0 means use only image\n",
    "for name, module in vd_pipe.image_unet.named_modules():\n",
    "    if isinstance(module, DualTransformer2DModel):\n",
    "        module.mix_ratio = text_image_ratio\n",
    "        for i, type in enumerate((\"text\", \"image\")):\n",
    "            if type == \"text\":\n",
    "                module.condition_lengths[i] = 77\n",
    "                module.transformer_index_for_condition[i] = 1  # use the second (text) transformer\n",
    "            else:\n",
    "                module.condition_lengths[i] = 257\n",
    "                module.transformer_index_for_condition[i] = 0  # use the first (image) transformer\n",
    "\n",
    "unet = vd_pipe.image_unet.to(device)\n",
    "vae = vd_pipe.vae.to(device)\n",
    "noise_scheduler = vd_pipe.scheduler\n",
    "img_variations = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8940c9cb-5bf7-4381-8e2e-096cbc381e4a",
   "metadata": {},
   "source": [
    "## Load Image Variations model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc1b0710-78a5-4d07-8940-363072eaa789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CLS model\n",
    "# out_dim = 768\n",
    "# voxel2clip_kwargs = dict(in_dim=num_voxels,out_dim=out_dim)\n",
    "# voxel2clip = BrainNetwork(**voxel2clip_kwargs)\n",
    "# voxel2clip.requires_grad_(False)\n",
    "# voxel2clip.eval()\n",
    "\n",
    "# diffusion_prior = BrainDiffusionPriorOld.from_pretrained(\n",
    "#     # kwargs for DiffusionPriorNetwork\n",
    "#     dict(),\n",
    "#     # kwargs for DiffusionNetwork\n",
    "#     dict(\n",
    "#         condition_on_text_encodings=False,\n",
    "#         timesteps=1000,\n",
    "#         voxel2clip=voxel2clip,\n",
    "#     ),\n",
    "#     voxel2clip_path=None,\n",
    "# )\n",
    "\n",
    "# model_name = \"final_subj01_1x768\"\n",
    "# outdir = f'../train_logs/{model_name}'\n",
    "# ckpt_path = os.path.join(outdir, f'last.pth')\n",
    "# print(\"ckpt_path\",ckpt_path)\n",
    "# checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "# state_dict = checkpoint['model_state_dict']\n",
    "# print(\"EPOCH: \",checkpoint['epoch'])\n",
    "# diffusion_prior.load_state_dict(state_dict,strict=False)\n",
    "# diffusion_prior.eval().to(device)\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71e65940-f62f-4091-bc9a-faeb545bc18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from diffusers import AutoencoderKL, UNet2DConditionModel, UniPCMultistepScheduler\n",
    "\n",
    "# sd_cache_dir = '/fsx/home-paulscotti/.cache/huggingface/diffusers/models--lambdalabs--sd-image-variations-diffusers/snapshots/a2a13984e57db80adcc9e3f85d568dcccb9b29fc'\n",
    "# unet = UNet2DConditionModel.from_pretrained(sd_cache_dir,subfolder=\"unet\").to(device)\n",
    "\n",
    "# unet.eval() # dont want to train model\n",
    "# unet.requires_grad_(False) # dont need to calculate gradients\n",
    "\n",
    "# vae = AutoencoderKL.from_pretrained(sd_cache_dir,subfolder=\"vae\").to(device)\n",
    "# vae.eval()\n",
    "# vae.requires_grad_(False)\n",
    "\n",
    "# noise_scheduler = UniPCMultistepScheduler.from_pretrained(sd_cache_dir, subfolder=\"scheduler\")\n",
    "# num_inference_steps = 20\n",
    "\n",
    "# img_variations = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d66a1c-cb99-4484-8789-4b9f33d1f994",
   "metadata": {},
   "source": [
    "# Reconstruct one-at-a-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f7d4d45-6a22-4150-a987-285f2f630a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-07 21:32:31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                     | 0/982 [00:00<?, ?it/s]/fsx/home-paulscotti/miniconda3/envs/medical-v1/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "  0%|                                                                                                     | 0/982 [00:31<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'err' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# grid.savefig(f'evals/{model_name}_{val_i}.png')\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# plt.close()\u001b[39;00m\n\u001b[1;32m     72\u001b[0m brain_recons \u001b[38;5;241m=\u001b[39m brain_recons[:,laion_best_picks\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint8)]\n\u001b[0;32m---> 73\u001b[0m \u001b[43merr\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m all_brain_recons \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m     all_brain_recons \u001b[38;5;241m=\u001b[39m brain_recons\n",
      "\u001b[0;31mNameError\u001b[0m: name 'err' is not defined"
     ]
    }
   ],
   "source": [
    "print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "recons_per_sample = 16\n",
    "img2img = True\n",
    "img2img_strength = .85\n",
    "retrieve = False\n",
    "plotting = False\n",
    "saving = True\n",
    "verbose = False\n",
    "imsize = 512\n",
    "\n",
    "if img_variations:\n",
    "    guidance_scale = 7.5\n",
    "else:\n",
    "    guidance_scale = 3.5\n",
    "    \n",
    "ind_include = np.arange(num_val)\n",
    "\n",
    "for img2img_strength in [.85, .7, .5, .3, 0]:\n",
    "    print(\"img2img_strength\",img2img_strength)\n",
    "    all_brain_recons = None\n",
    "    if img2img_strength == 0:\n",
    "        img2img = False\n",
    "    else:\n",
    "        img2img = True\n",
    "    for val_i, (voxel, img, coco) in enumerate(tqdm(val_dl,total=len(ind_include))):\n",
    "        if val_i<np.min(ind_include):\n",
    "            continue\n",
    "        voxel = torch.mean(voxel,axis=1).to(device)\n",
    "        # voxel = voxel[:,0].to(device)\n",
    "        with torch.no_grad():\n",
    "            if img2img:\n",
    "                ae_preds = voxel2sd(voxel.float())\n",
    "                blurry_recons = vd_pipe.vae.decode(ae_preds.to(device).half()/0.18215).sample / 2 + 0.5\n",
    "\n",
    "                if val_i==0:\n",
    "                    plt.imshow(utils.torch_to_Image(blurry_recons))\n",
    "                    plt.show()\n",
    "\n",
    "                # blurry_recons = PIL.Image.open(f\"blurry_recons/{coco.item()}.png\").convert('RGB')\n",
    "                # blurry_recons = transforms.PILToTensor()(blurry_recons)\n",
    "                # blurry_recons = transforms.Resize((512,512))(blurry_recons)\n",
    "                # blurry_recons = (blurry_recons.float() / 255)[None]\n",
    "            else:\n",
    "                blurry_recons = None\n",
    "\n",
    "            grid, brain_recons, laion_best_picks, recon_img = utils.reconstruction(\n",
    "                img, voxel,\n",
    "                clip_extractor, unet, vae, noise_scheduler,\n",
    "                voxel2clip_cls = diffusion_prior_cls.voxel2clip,\n",
    "                diffusion_priors = diffusion_priors,\n",
    "                text_token = None,\n",
    "                img_lowlevel = blurry_recons,\n",
    "                num_inference_steps = num_inference_steps,\n",
    "                n_samples_save = batch_size,\n",
    "                recons_per_sample = recons_per_sample,\n",
    "                guidance_scale = guidance_scale,\n",
    "                img2img_strength = .85, # 0=fully rely on img_lowlevel, 1=not doing img2img\n",
    "                timesteps_prior = 100,\n",
    "                seed = seed,\n",
    "                retrieve = retrieve,\n",
    "                plotting = plotting,\n",
    "                img_variations = img_variations,\n",
    "                verbose = verbose,\n",
    "            )\n",
    "\n",
    "            if plotting:\n",
    "                plt.show()\n",
    "                # grid.savefig(f'evals/{model_name}_{val_i}.png')\n",
    "                # plt.close()\n",
    "\n",
    "            brain_recons = brain_recons[:,laion_best_picks.astype(np.int8)]\n",
    "            \n",
    "            if all_brain_recons is None:\n",
    "                all_brain_recons = brain_recons\n",
    "                all_images = img\n",
    "            else:\n",
    "                all_brain_recons = torch.vstack((all_brain_recons,brain_recons))\n",
    "                all_images = torch.vstack((all_images,img))\n",
    "\n",
    "        if val_i>=np.max(ind_include):\n",
    "            break\n",
    "\n",
    "    all_brain_recons = all_brain_recons.view(-1,3,imsize,imsize)\n",
    "    print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "    if saving:\n",
    "        # torch.save(all_images,'evals/all_images')\n",
    "\n",
    "        # print(\"BLURRY RECONS. CHECK THE -5 INDEX\")\n",
    "        # torch.save(all_brain_recons,f'evals/{model_name[:-5]}_brain_recons_full_img2img{img2img_strength}')\n",
    "\n",
    "        torch.save(all_brain_recons,f'evals/{model_name}_brain_recons_full_img2img{img2img_strength}')\n",
    "    print(f'evals/{model_name}_brain_recons_full_img2img{img2img_strength} done!')\n",
    "print(\"DONE!\")\n",
    "sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f50f891-6b17-4d5b-bec1-0ffe48f40475",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
