{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26ca08a0-e0e9-4cf0-b95c-2c93376f2eb7",
   "metadata": {},
   "source": [
    "This notebook takes brain voxels and maps them to CLIP-space via a contrastive learning to CLIP space + diffusion prior approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d95fac-ac1d-473c-ab96-650f76e6aaf5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook train_prior_w_voxel2clip.ipynb to python\n",
      "[NbConvertApp] Writing 17292 bytes to train_prior_w_voxel2clip.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # convert this notebook to .py such that you can then run it via slurm with \"sbatch main.slurm\"\n",
    "# from subprocess import call\n",
    "# command = \"jupyter nbconvert train_prior_w_voxel2clip.ipynb --to python\"\n",
    "# call(command,shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f0f4f3",
   "metadata": {},
   "source": [
    "# Import packages & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bad764b-45c1-45ce-a716-8d055e09821a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "NOT using distributed parallel processing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from info_nce import InfoNCE\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from dalle2_pytorch import DiffusionPriorNetwork\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device:\",device)\n",
    "\n",
    "import ddp_config\n",
    "distributed,local_rank = ddp_config.ddp_test()\n",
    "if device=='cuda': torch.cuda.set_device(local_rank)\n",
    "\n",
    "import utils\n",
    "from models import Clipper, BrainNetwork, BrainDiffusionPrior, BrainSD\n",
    "from model3d import NewVoxel3dConvEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018b82b-c054-4463-9527-4b0c2a75bda6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60cd7f2c-37fd-426b-a0c6-633e51bc4c4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with -f:\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '-f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# read in any command line args or config file values and override the above params\u001b[39;00m\n\u001b[1;32m     31\u001b[0m config_keys \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m     32\u001b[0m                \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m))]\n\u001b[0;32m---> 33\u001b[0m \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconfigurator.py\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# overrides from command line or config file\u001b[39;00m\n\u001b[1;32m     34\u001b[0m config \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28mglobals\u001b[39m()[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m config_keys} \u001b[38;5;66;03m# will be useful for logging\u001b[39;00m\n\u001b[1;32m     36\u001b[0m num_devices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n",
      "File \u001b[0;32m<string>:28\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '-f'"
     ]
    }
   ],
   "source": [
    "model_name = \"prior-w-voxel2clip\"\n",
    "modality = \"image\" # (\"image\", \"text\")\n",
    "image_var = 'images' if modality=='image' else trial\n",
    "clip_variant = \"ViT-L/14\" # (\"RN50\", \"ViT-L/14\", \"ViT-B/32\")\n",
    "clamp_embs = False # clamp embeddings to (-1.5, 1.5)\n",
    "timesteps = 1000 # for diffusion prior\n",
    "alpha_schedule = \"constant\" # (\"constant\", \"linear\") - for weighting the loss\n",
    "voxel2clip_kwargs = dict(out_dim=768)\n",
    "\n",
    "voxel_dims = 1 # 1 for flattened 3 for 3d\n",
    "n_samples_save = 4 # how many SD samples from train and val to save\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 60\n",
    "lr_scheduler = 'cycle'\n",
    "initial_lr = 1e-3\n",
    "max_lr = 3e-4\n",
    "first_batch = False\n",
    "ckpt_saving = True\n",
    "ckpt_interval = 5\n",
    "save_at_end = False\n",
    "outdir = f'../train_logs/models/{model_name}/test'\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "use_mp = False\n",
    "remote_data = False\n",
    "\n",
    "# if running command line, read in args or config file values and override above params\n",
    "try:\n",
    "    config_keys = [k for k,v in globals().items() if not k.startswith('_') \\\n",
    "                   and isinstance(v, (int, float, bool, str))]\n",
    "    exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "    config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
    "except:\n",
    "    pass\n",
    "\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0: num_devices = 1\n",
    "num_workers = num_devices\n",
    "\n",
    "wandb_log = False\n",
    "wandb_project = 'stability'\n",
    "wandb_run_name = 'pstest'\n",
    "wandb_notes = ''\n",
    "if wandb_log: \n",
    "    import wandb\n",
    "    config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"modality\": modality,\n",
    "      \"clip_variant\": clip_variant,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"initial_lr\": initial_lr,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"lr_scheduler\": lr_scheduler,\n",
    "      \"alpha_schedule\": alpha_schedule,\n",
    "      \"clamp_embs\": clamp_embs,\n",
    "    }\n",
    "\n",
    "cache_dir = 'cache'\n",
    "n_cache_recs = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d13c25-1369-4c49-81d4-83d713586096",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prep models and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7966d5a-c8a9-4461-808a-2f89bb00fe9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Clipper...\n",
      "ViT-L/14 cuda\n",
      "Creating voxel2clip...\n",
      "param counts:\n",
      "134,722,304 total\n",
      "134,722,304 trainable\n",
      "Creating diffusion prior...\n",
      "net_config {'dim': 768, 'depth': 12, 'num_timesteps': 1000, 'num_time_embeds': 1, 'num_image_embeds': 1, 'num_text_embeds': 1, 'dim_head': 64, 'heads': 12, 'ff_mult': 4, 'norm_out': True, 'attn_dropout': 0.0, 'ff_dropout': 0.0, 'final_proj': True, 'normformer': True, 'rotary_emb': True, 'max_text_len': 256}\n",
      "prior_config {'image_embed_dim': 768, 'image_size': 224, 'image_channels': 3, 'timesteps': 1000, 'sample_timesteps': 64, 'cond_drop_prob': 0.0, 'loss_type': 'l2', 'predict_x_start': True, 'beta_schedule': 'cosine', 'condition_on_text_encodings': False, 'voxel2clip': BrainNetwork(\n",
      "  (lin0): Sequential(\n",
      "    (0): Linear(in_features=15724, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (mlp): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (lin1): Linear(in_features=4096, out_features=768, bias=True)\n",
      ")}\n",
      "missing keys in prior checkpoint (probably ok) ['net.null_text_encodings', 'net.null_text_embeds', 'net.null_image_embed', 'voxel2clip.lin0.0.weight', 'voxel2clip.lin0.0.bias', 'voxel2clip.lin0.2.weight', 'voxel2clip.lin0.2.bias', 'voxel2clip.lin0.2.running_mean', 'voxel2clip.lin0.2.running_var', 'voxel2clip.mlp.0.0.weight', 'voxel2clip.mlp.0.0.bias', 'voxel2clip.mlp.0.2.weight', 'voxel2clip.mlp.0.2.bias', 'voxel2clip.mlp.0.2.running_mean', 'voxel2clip.mlp.0.2.running_var', 'voxel2clip.mlp.1.0.weight', 'voxel2clip.mlp.1.0.bias', 'voxel2clip.mlp.1.2.weight', 'voxel2clip.mlp.1.2.bias', 'voxel2clip.mlp.1.2.running_mean', 'voxel2clip.mlp.1.2.running_var', 'voxel2clip.mlp.2.0.weight', 'voxel2clip.mlp.2.0.bias', 'voxel2clip.mlp.2.2.weight', 'voxel2clip.mlp.2.2.bias', 'voxel2clip.mlp.2.2.running_mean', 'voxel2clip.mlp.2.2.running_var', 'voxel2clip.mlp.3.0.weight', 'voxel2clip.mlp.3.0.bias', 'voxel2clip.mlp.3.2.weight', 'voxel2clip.mlp.3.2.bias', 'voxel2clip.mlp.3.2.running_mean', 'voxel2clip.mlp.3.2.running_var', 'voxel2clip.lin1.weight', 'voxel2clip.lin1.bias']\n",
      "param counts:\n",
      "236,616,336 total\n",
      "236,616,320 trainable\n",
      "Creating SD image variation pipeline...\n",
      "Pulling NSD webdataset data...\n",
      "Prepping train and validation dataloaders...\n",
      "Getting dataloaders...\n",
      "train_url /fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset/train/train_subj01_{0..49}.tar\n",
      "val_url /fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset/val/val_subj01_0.tar\n",
      "num_devices 1\n",
      "num_workers 1\n",
      "batch_size 32\n",
      "global_batch_size 32\n",
      "num_worker_batches 780\n",
      "cache_dir None\n",
      "validation: num_worker_batches 16\n",
      "\n",
      "Done with model preparations!\n"
     ]
    }
   ],
   "source": [
    "if local_rank == 0: print('Creating Clipper...')\n",
    "    \n",
    "# Don't L2 norm the extracted CLIP embeddings since we want the prior \n",
    "# to learn un-normed embeddings for usage with the SD image variation pipeline.\n",
    "clip_extractor = Clipper(clip_variant, clamp_embs=clamp_embs, norm_embs=False, device=device)\n",
    "\n",
    "if local_rank == 0: print('Creating voxel2clip...')\n",
    "\n",
    "if voxel_dims == 1: # 1D data\n",
    "    voxel2clip = BrainNetwork(**voxel2clip_kwargs)\n",
    "    # 134M params\n",
    "elif voxel_dims == 3: # 3D data\n",
    "    voxel2clip = NewVoxel3dConvEncoder(**voxel2clip_kwargs)\n",
    "    # 58M params for original version\n",
    "    # 5M params for smaller version\n",
    "    # Projection input features: 5120\n",
    "    # param counts:\n",
    "    # 5,584,448 total\n",
    "    # 5,584,448 trainable\n",
    "    \n",
    "try:\n",
    "    utils.count_params(voxel2clip)\n",
    "except:\n",
    "    if local_rank == 0: print('Cannot count params for voxel2clip (probably because it has Lazy layers)')\n",
    "\n",
    "if local_rank == 0: print('Creating diffusion prior...')\n",
    "\n",
    "# initializing diffusion prior with https://huggingface.co/nousr/conditioned-prior\n",
    "assert timesteps == 1000\n",
    "diffusion_prior = BrainDiffusionPrior.from_pretrained(\n",
    "    # kwargs for DiffusionPriorNetwork\n",
    "    dict(),\n",
    "    # kwargs for DiffusionNetwork\n",
    "    dict(\n",
    "        condition_on_text_encodings=False,\n",
    "        timesteps=timesteps,\n",
    "        # cond_drop_prob=cond_drop_prob,\n",
    "        # image_embed_scale=image_embed_scale,\n",
    "        voxel2clip=voxel2clip,\n",
    "    ),\n",
    ")\n",
    "\n",
    "if distributed:\n",
    "    diffusion_prior = diffusion_prior.to(local_rank)\n",
    "    diffusion_prior = DDP(diffusion_prior, device_ids=[local_rank])\n",
    "else:\n",
    "    diffusion_prior = diffusion_prior.to(device)\n",
    "try:\n",
    "    utils.count_params(diffusion_prior)\n",
    "except:\n",
    "    if local_rank == 0: print('Cannot count params for diffusion_prior (probably because it has Lazy layers)')\n",
    "\n",
    "if local_rank == 0: print('Creating SD image variation pipeline...')\n",
    "sd_cache_dir = '/fsx/home-paulscotti/.cache/huggingface/diffusers/models--lambdalabs--sd-image-variations-diffusers/snapshots/a2a13984e57db80adcc9e3f85d568dcccb9b29fc'\n",
    "if not os.path.isdir(sd_cache_dir): # download from huggingface if not already downloaded / cached\n",
    "    sd_pipe = BrainSD.from_pretrained(\n",
    "        \"lambdalabs/sd-image-variations-diffusers\", \n",
    "        revision=\"v2.0\",\n",
    "        safety_checker=None,\n",
    "        requires_safety_checker=False,\n",
    "        torch_dtype=torch.float16, # fp16 is fine if we're not training this\n",
    "    ).to(device)\n",
    "else:\n",
    "    sd_pipe = BrainSD.from_pretrained(\n",
    "        sd_cache_dir, \n",
    "        revision=\"v2.0\",\n",
    "        safety_checker=None,\n",
    "        requires_safety_checker=False,\n",
    "        torch_dtype=torch.float16, # fp16 is fine if we're not training this\n",
    "    ).to(device)\n",
    "\n",
    "# freeze everything, we're just using this for inference\n",
    "sd_pipe.unet.eval()\n",
    "sd_pipe.unet.requires_grad_(False)\n",
    "\n",
    "sd_pipe.vae.eval()\n",
    "sd_pipe.vae.requires_grad_(False)\n",
    "\n",
    "sd_pipe.image_encoder.eval()\n",
    "sd_pipe.image_encoder.requires_grad_(False)\n",
    "assert sd_pipe.image_encoder.training == False\n",
    "\n",
    "if local_rank == 0: print('Pulling NSD webdataset data...')\n",
    "if remote_data:\n",
    "    # pull data directly from huggingface\n",
    "    train_url, val_url = utils.get_huggingface_urls(data_commit)\n",
    "    meta_url = None\n",
    "else:\n",
    "    # local paths\n",
    "    train_url = \"/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset/train/train_subj01_{0..49}.tar\"\n",
    "    val_url = \"/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset/val/val_subj01_0.tar\"\n",
    "    meta_url = \"/fsx/proj-medarc/fmri/natural-scenes-dataset/webdataset/metadata_subj01.json\"\n",
    "\n",
    "# which to use for the voxels\n",
    "if voxel_dims == 1:\n",
    "    voxels_key = 'nsdgeneral.npy'\n",
    "elif voxel_dims == 3:\n",
    "    voxels_key = 'wholebrain_3d.npy'\n",
    "else:\n",
    "    raise Exception(f\"voxel_dims must be 1 or 3, not {voxel_dims}\")\n",
    "\n",
    "if local_rank == 0: print('Prepping train and validation dataloaders...')\n",
    "train_dl, val_dl, num_train, num_val = utils.get_dataloaders(\n",
    "    batch_size, \n",
    "    image_var, \n",
    "    num_devices=num_devices,\n",
    "    num_workers=num_workers,\n",
    "    train_url=train_url,\n",
    "    val_url=val_url,\n",
    "    meta_url=meta_url,\n",
    "    cache_dir=cache_dir,\n",
    "    n_cache_recs=n_cache_recs,\n",
    "    voxels_key=voxels_key,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(diffusion_prior.parameters(), lr=initial_lr)\n",
    "if lr_scheduler == 'fixed':\n",
    "    lr_scheduler = None\n",
    "elif lr_scheduler == 'cycle':\n",
    "    # <TODO> hard-coded values\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=max_lr, \n",
    "        total_steps=num_epochs*((num_train//batch_size)//num_devices), \n",
    "        final_div_factor=1000,\n",
    "        last_epoch=-1, pct_start=2/num_epochs\n",
    "    )\n",
    "    \n",
    "def save_ckpt(tag):\n",
    "    ckpt_path = os.path.join(outdir, f'ckpt-{tag}.pth')\n",
    "    print(f'saving {ckpt_path}')\n",
    "    if local_rank==0:\n",
    "        state_dict = diffusion_prior.state_dict()\n",
    "        if distributed: # if using DDP, convert DDP state_dict to non-DDP before saving\n",
    "            for key in list(state_dict.keys()):\n",
    "                if 'module.' in key:\n",
    "                    state_dict[key.replace('module.', '')] = state_dict[key]\n",
    "                    del state_dict[key]\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': diffusion_prior.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_losses': losses,\n",
    "            'val_losses': val_losses,\n",
    "            'lrs': lrs,\n",
    "            }, ckpt_path)\n",
    "\n",
    "        # if wandb_log:\n",
    "        #     wandb.save(ckpt_path)\n",
    "print(\"\\nDone with model preparations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5690151-2131-4918-b750-e869cbd1a8a8",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571d9b46-2a39-4b3b-982f-0e78e39d648b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using cudnn.deterministic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                              | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 15724])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                  | 0/60 [03:49<?, ?it/s, lr=0.000156, train_loss=5.1, train_sim=0.603, val_loss=4.49, val_sim=0.623]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving ../train_logs/models/prior-w-voxel2clip/test/ckpt-best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██▎                                                                                                                                     | 1/60 [07:38<3:49:29, 233.38s/it, lr=0.0003, train_loss=3.27, train_sim=0.756, val_loss=3.24, val_sim=0.827]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving ../train_logs/models/prior-w-voxel2clip/test/ckpt-best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|████▌                                                                                                                                   | 2/60 [07:42<3:43:25, 231.13s/it, lr=0.0003, train_loss=3.27, train_sim=0.756, val_loss=3.24, val_sim=0.827]"
     ]
    }
   ],
   "source": [
    "# need non-deterministic CuDNN for conv3D to work\n",
    "utils.seed_everything(local_rank, cudnn_deterministic=False)\n",
    "\n",
    "epoch = 0\n",
    "losses, val_losses, lrs = [], [], []\n",
    "best_val_loss = 1e9\n",
    "nce = utils.mixco_nce  # same as infonce if mixup not used\n",
    "\n",
    "# weight for prior's MSE loss term\n",
    "if alpha_schedule == 'constant':\n",
    "    alphas = np.ones(num_epochs) * 0.01\n",
    "elif alpha_schedule == 'linear':\n",
    "    alphas = np.linspace(0.01, 0.05, num_epochs, endpoint=True)\n",
    "else:\n",
    "    raise ValueError(f'unknown alpha_schedule: {alpha_schedule}')\n",
    "\n",
    "if wandb_log:\n",
    "    wandb.init(\n",
    "        project=wandb_project,\n",
    "        name=wandb_run_name,\n",
    "        config=config,\n",
    "        notes=wandb_notes,\n",
    "    )\n",
    "\n",
    "voxel0 = image0 = val_voxel0 = val_image0 = None\n",
    "\n",
    "progress_bar = tqdm(range(epoch,num_epochs), ncols=250, disable=(local_rank!=0))\n",
    "for epoch in progress_bar:\n",
    "    diffusion_prior.train()\n",
    "\n",
    "    sims = 0.\n",
    "    sims_base = 0.\n",
    "    val_sims = 0.\n",
    "    val_sims_base = 0.\n",
    "    fwd_percent_correct = 0.\n",
    "    bwd_percent_correct = 0.\n",
    "    val_fwd_percent_correct = 0.\n",
    "    val_bwd_percent_correct = 0.\n",
    "    loss_nce_sum = 0.\n",
    "    loss_prior_sum = 0.\n",
    "    val_loss_nce_sum = 0.\n",
    "    val_loss_prior_sum = 0.\n",
    "\n",
    "    alpha = alphas[epoch]\n",
    "\n",
    "    for train_i, (voxel, image, key) in enumerate(train_dl):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        image = image.to(device).float()\n",
    "        voxel = voxel.to(device).float()\n",
    "        if image0 is None and local_rank == 0:\n",
    "            image0 = image.clone()\n",
    "            voxel0 = voxel.clone()\n",
    "            key0 = key\n",
    "            print(voxel0.shape)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=use_mp):\n",
    "            with torch.cuda.amp.autocast(enabled=True):\n",
    "                clip_image = clip_extractor.embed_image(image, return_norm=False).float()\n",
    "            clip_image.to(voxel.dtype)\n",
    "            loss, pred, clip_voxels = diffusion_prior(image_embed=clip_image, voxel=voxel)\n",
    "            utils.check_loss(loss)\n",
    "\n",
    "            loss_nce = nce(\n",
    "                nn.functional.normalize(clip_voxels, dim=-1), \n",
    "                nn.functional.normalize(clip_image, dim=-1),\n",
    "                distributed=distributed\n",
    "            )\n",
    "            utils.check_loss(loss_nce)\n",
    "\n",
    "            loss_nce_sum += loss_nce.item()\n",
    "            loss_prior_sum += loss.item()\n",
    "\n",
    "            # MSE and NCE are weighted equally at the beginning,\n",
    "            # with alpha=0.01 we'll have something like .01*300 + .99*3 = 3 + 3\n",
    "            loss = alpha * loss + (1-alpha) * loss_nce\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            lrs.append(optimizer.param_groups[0]['lr'])\n",
    "            # similarity after prior diffusion\n",
    "            sims += F.cosine_similarity(clip_image, pred).mean().item()\n",
    "            # baseline similarity before prior diffusion\n",
    "            sims_base += F.cosine_similarity(clip_image, clip_voxels).mean().item()\n",
    "            # forward and backward top 1 accuracy\n",
    "            labels = torch.arange(len(clip_voxels)).to(device)\n",
    "            fwd_percent_correct += utils.topk(\n",
    "                utils.batchwise_cosine_similarity(clip_image, clip_voxels), labels, k=1\n",
    "            )\n",
    "            bwd_percent_correct += utils.topk(\n",
    "                utils.batchwise_cosine_similarity(clip_voxels, clip_image), labels, k=1\n",
    "            )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "    if local_rank==0: \n",
    "        diffusion_prior.eval()\n",
    "        for val_i, (voxel, image, key) in enumerate(val_dl): \n",
    "            for val_i, (voxel, image) in enumerate(val_dl): \n",
    "                with torch.no_grad():\n",
    "                    image = image.to(device).float()\n",
    "                    voxel = voxel.to(device).float()\n",
    "                    if val_image0 is None:\n",
    "                        val_image0 = image.clone()\n",
    "                        val_voxel0 = voxel.clone()\n",
    "                        val_key0 = key\n",
    "                    \n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        clip_image = clip_extractor.embed_image(image).float()\n",
    "                        loss, pred, clip_voxels = diffusion_prior.module(image_embed=clip_image, voxel=voxel) \\\n",
    "                            if distributed else diffusion_prior(image_embed=clip_image, voxel=voxel)\n",
    "\n",
    "                        loss_nce = nce(\n",
    "                            nn.functional.normalize(clip_voxels, dim=-1), \n",
    "                            nn.functional.normalize(clip_image, dim=-1),\n",
    "                        )\n",
    "                        val_loss_nce_sum += loss_nce.item()\n",
    "                        val_loss_prior_sum += loss.item()\n",
    "\n",
    "                        val_loss = alpha * loss + (1-alpha) * loss_nce\n",
    "\n",
    "                        val_losses.append(val_loss.item())\n",
    "                        val_sims += F.cosine_similarity(clip_image, pred).mean().item()\n",
    "                        val_sims_base += F.cosine_similarity(clip_image, clip_voxels).mean().item()\n",
    "                        labels = torch.arange(len(clip_voxels)).to(device)\n",
    "                        val_fwd_percent_correct += utils.topk(\n",
    "                            utils.batchwise_cosine_similarity(clip_image, clip_voxels), labels, k=1\n",
    "                        )\n",
    "                        val_bwd_percent_correct += utils.topk(\n",
    "                            utils.batchwise_cosine_similarity(clip_voxels, clip_image), labels, k=1\n",
    "                        )\n",
    "\n",
    "                logs = OrderedDict(\n",
    "                    train_loss=np.mean(losses[-(train_i+1):]),\n",
    "                    val_loss=np.mean(val_losses[-(val_i+1):]),\n",
    "                    lr=lrs[-1],\n",
    "                    train_sim=sims / (train_i + 1),\n",
    "                    val_sim=val_sims / (val_i + 1),\n",
    "                )\n",
    "                progress_bar.set_postfix(**logs)\n",
    "\n",
    "            if (not save_at_end and ckpt_saving) or (save_at_end and epoch == num_epochs - 1):\n",
    "                # save best model\n",
    "                val_loss = np.mean(val_losses[-(val_i+1):])\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    save_ckpt('best')\n",
    "                else:\n",
    "                    print(f'not best - val_loss: {val_loss:.3f}, best_val_loss: {best_val_loss:.3f}')\n",
    "\n",
    "                # Save model checkpoint every `ckpt_interval`` epochs or on the last epoch\n",
    "                if (ckpt_interval is not None and (epoch + 1) % ckpt_interval == 0) or epoch == num_epochs - 1:\n",
    "                    save_ckpt(f'epoch{epoch:03d}')\n",
    "\n",
    "            logs = {\n",
    "                \"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "                \"val/loss\": np.mean(val_losses[-(val_i+1):]),\n",
    "                \"train/lr\": lrs[-1],\n",
    "                \"train/num_steps\": len(losses),\n",
    "                \"train/cosine_sim\": sims / (train_i + 1),\n",
    "                \"val/cosine_sim\": val_sims / (val_i + 1),\n",
    "                \"train/cosine_sim_base\": sims_base / (train_i + 1),\n",
    "                \"val/cosine_sim_base\": val_sims_base / (val_i + 1),\n",
    "                \"train/fwd_pct_correct\": fwd_percent_correct / (train_i + 1),\n",
    "                \"train/bwd_pct_correct\": bwd_percent_correct / (train_i + 1),\n",
    "                \"val/val_fwd_pct_correct\": val_fwd_percent_correct / (val_i + 1),\n",
    "                \"val/val_bwd_pct_correct\": val_bwd_percent_correct / (val_i + 1),\n",
    "                \"train/loss_nce\": loss_nce_sum / (train_i + 1),\n",
    "                \"train/loss_prior\": loss_prior_sum / (train_i + 1),\n",
    "                \"val/loss_nce\": val_loss_nce_sum / (val_i + 1),\n",
    "                \"val/loss_prior\": val_loss_prior_sum / (val_i + 1),\n",
    "                \"train/alpha\": alpha,\n",
    "            }\n",
    "\n",
    "            # sample some images\n",
    "            if sd_pipe is not None:\n",
    "                if (ckpt_interval is not None and (epoch + 1) % ckpt_interval == 0) or epoch == num_epochs - 1:\n",
    "                    if (not save_at_end and n_samples_save > 0) or (save_at_end and epoch == num_epochs - 1):\n",
    "                        grids = utils.sample_images(\n",
    "                            clip_extractor, \n",
    "                            diffusion_prior.voxel2clip if not distributed else diffusion_prior.module.voxel2clip, \n",
    "                            sd_pipe, \n",
    "                            diffusion_prior if not distributed else diffusion_prior.module,\n",
    "                            voxel0[:n_samples_save], \n",
    "                            image0[:n_samples_save], \n",
    "                            seed=42,\n",
    "                        )\n",
    "                        for i, grid in enumerate(grids):\n",
    "                            grid.save(os.path.join(outdir, f'samples-train-{key0[i]}.png'))\n",
    "                        if wandb_log:\n",
    "                            logs['train/samples'] = [wandb.Image(grid, caption=key0[i]) for i, grid in enumerate(grids)]\n",
    "\n",
    "                        # validation\n",
    "                        print(\"Sampling validation images...\")\n",
    "                        grids = utils.sample_images(\n",
    "                            clip_extractor, \n",
    "                            diffusion_prior.voxel2clip if not distributed else diffusion_prior.module.voxel2clip, \n",
    "                            sd_pipe, \n",
    "                            diffusion_prior if not distributed else diffusion_prior.module,\n",
    "                            val_voxel0[:n_samples_save * 4], \n",
    "                            val_image0[:n_samples_save * 4],\n",
    "                            seed=42,\n",
    "                        )\n",
    "                        for i, grid in enumerate(grids):\n",
    "                            grid.save(os.path.join(outdir, f'samples-val-{val_key0[i]}.png'))\n",
    "                        if wandb_log:\n",
    "                            logs['val/samples'] = [wandb.Image(grid, caption=val_key0[i]) for i, grid in enumerate(grids)]\n",
    "\n",
    "            if wandb_log:\n",
    "                wandb.log(logs)\n",
    "    if distributed:\n",
    "        dist.barrier()\n",
    "\n",
    "if wandb_log:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38301700-37bc-4ef5-8e00-af9fb5eb8762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b773a56b-7d31-492a-98e0-ef8d34ceb5eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "62aae01ef0cf7b6af841ab1c8ce59175c4332e693ab3d00bc32ceffb78a35376"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
